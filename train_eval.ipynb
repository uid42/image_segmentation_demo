{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "from fastai.basic_train import LearnerCallback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "from fastai.callbacks.general_sched import *\n",
    "from fastai.callback import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "from fastai.core import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "from IPython.core import debugger as idb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "from exp import nb_resnet_unet\n",
    "from exp import nb_loss_metrics\n",
    "from exp import nb_optimizer\n",
    "from exp import nb_tensorboard_callback\n",
    "from exp import nb_scheduling_train\n",
    "from exp import nb_databunch\n",
    "from exp import nb_train_script_logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "from torch import tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "from torch.nn import Sequential, ModuleList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "from fastai.basic_train import Learner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "from fastai.torch_core import bn_types,bias_types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "from functools import partial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "import os,shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "from fastai.callbacks import CSVLogger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "from fastai.callbacks.tracker import SaveModelCallback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# export\n",
    "def get_learn(data, model_name = 'resnet18', allres = True\n",
    "              , loss_func_name = 'balance_bce', balance_ratio = 1\n",
    "              , tag = None):\n",
    "    assert model_name in ['resnet18', 'assert34'], 'invalid model name'\n",
    "    assert loss_func_name in ['balance_bce', 'dice_loss', 'combo_loss'], 'invalid loss func name'\n",
    "    #检查一下确保参数没写错\n",
    "    assert tag is not None\n",
    "    if tag is not None:\n",
    "        #tag = '__'.join(['resnet18', 'allres', 'dice_loss', 'dataset200', 'adam'])\n",
    "        tags = tag.split('__')\n",
    "        assert model_name == tags[0]\n",
    "        assert tags[1] in ['allres', 'vanila']\n",
    "        assert (allres and tags[1] == 'allres') or (not allres and tags[1] == 'vanila')\n",
    "        if loss_func_name == 'dice_loss':\n",
    "            assert  len(tags) == 5\n",
    "        else:\n",
    "            assert  len(tags) == 6\n",
    "            assert (tags[5] == 'balance_ratio_1' and balance_ratio == 1) \\\n",
    "                    or (tags[5] == 'balance_ratio_10' and balance_ratio == 10) \\\n",
    "                    or (tags[5] == 'balance_ratio_0.1' and balance_ratio == 0.1)\n",
    "            \n",
    "    # create model\n",
    "    model = None\n",
    "    if model_name == 'resnet18':\n",
    "        model = nb_resnet_unet.get_unet_res18(1, allres)\n",
    "        if allres:\n",
    "            model.load_state_dict(torch.load('./models/unet_res18_allres_init.pth'))\n",
    "        else:\n",
    "            model.load_state_dict(torch.load('./models/unet_res18_vanila_init.pth'))\n",
    "    elif model_name == 'resnet34':\n",
    "        model = nb_resnet_unet.get_unet_res34(1, allres)\n",
    "        if allres:\n",
    "            model.load_state_dict(torch.load('./models/unet_res34_allres_init.pth'))\n",
    "        else:\n",
    "            model.load_state_dict(torch.load('./models/unet_res34_vanila_init.pth'))\n",
    "    \n",
    "    # create learner\n",
    "    learn = Learner(data,model)\n",
    "    \n",
    "    # split model\n",
    "    learn.layer_groups = nb_train_script_logger.split_model(learn.model)\n",
    "    \n",
    "    # set multi-gpu\n",
    "    if data.device.type=='cuda':\n",
    "        learn.model = torch.nn.DataParallel(learn.model,device_ids=[0,1,2,3,4,5])\n",
    "        \n",
    "    # set loss func\n",
    "    if loss_func_name == 'combo_loss':\n",
    "        learn.loss_func = partial(nb_loss_metrics.combo_loss, balance_ratio=balance_ratio)\n",
    "    elif loss_func_name == 'dice_loss':\n",
    "        learn.loss_func = nb_loss_metrics.dice_loss\n",
    "    elif loss_func_name == 'balance_bce':\n",
    "        learn.loss_func = partial(nb_loss_metrics.balance_bce, balance_ratio=balance_ratio)\n",
    "    \n",
    "    # 添加metrics\n",
    "    learn.metrics += [nb_loss_metrics.dice_loss]\n",
    "    learn.metrics += [partial(nb_loss_metrics.balance_bce,balance_ratio=1)]\n",
    "    learn.metrics += [nb_loss_metrics.mask_iou]\n",
    "    \n",
    "    return learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#export\n",
    "model_names = ['resnet18', 'assert34']\n",
    "loss_func_names = ['dice_loss', 'balance_bce', 'combo_loss']\n",
    "balance_ratios = [1, 0.1, 10]\n",
    "dataset_dirs = ['data/dataset_20200715', 'data/dataset_20200715_2000', 'data/dataset_20200715_200_et']\n",
    "dataset_names = ['dataset200', 'dataset2000', 'dataset200_et']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#把结果备份一下\n",
    "def backup_train_logs(tag, dst_root = 'train_logs'):    \n",
    "    src_dirs = ['run_log', 'tb_log']\n",
    "    path = os.path.join(dst_root, tag)\n",
    "    os.makedirs(path, exist_ok = True)\n",
    "    for d in src_dirs:        \n",
    "        dst = os.path.join(dst_root, tag, d)\n",
    "        #3.8的python才有dirs_exist_ok。现在只能先删掉\n",
    "        shutil.rmtree(dst, ignore_errors = True)\n",
    "        shutil.copytree(d, dst)\n",
    "    ds = tag.split('__')[3]\n",
    "    assert ds in dataset_names, ds + '???'\n",
    "    csvpath = dataset_dirs[dataset_names.index(ds)] + '/image'\n",
    "    shutil.rmtree(os.path.join(path, 'csv_log'), ignore_errors = True)\n",
    "    #print(os.path.join(csvpath, 'csv_log'), path)\n",
    "    shutil.copytree(os.path.join(csvpath, 'csv_log'), os.path.join(path, 'csv_log'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "# 设置device\n",
    "device = torch.device('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "BS = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 200数据集\n",
    "ds = './data/dataset_20200715'\n",
    "data_200 = nb_databunch.get_databunch(ds, bs=BS, device=device, transforms = None) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2000数据集\n",
    "ds = './data/dataset_20200715_2000'\n",
    "data_2000 = nb_databunch.get_databunch(ds, bs=BS, device=device, transforms = None) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 200带elastic_transform数据集\n",
    "ds = './data/dataset_20200715_200_et'\n",
    "data_200et = nb_databunch.get_databunch_et(ds, bs=BS, device=device, transforms = None) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "optf_adam = partial(nb_optimizer.Adam, betas=(0.9,0.99))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 数据集比较"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### 200数据集，无transform,resnet18，allres,dice,adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "tag = '__'.join(['resnet18', 'allres', 'dice_loss', 'dataset200', 'adam'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "getlearn = partial(get_learn, data = data_200, model_name = 'resnet18'\n",
    "                   , loss_func_name = 'dice_loss', allres = True, tag = tag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Learner(data=ImageDataBunch;\n",
       "\n",
       "Train: LabelList (1600 items)\n",
       "x: SegmentationItemList\n",
       "Image (3, 512, 512),Image (3, 512, 512),Image (3, 512, 512),Image (3, 512, 512),Image (3, 512, 512)\n",
       "y: SegmentationLabelList\n",
       "ImageSegment (1, 512, 512),ImageSegment (1, 512, 512),ImageSegment (1, 512, 512),ImageSegment (1, 512, 512),ImageSegment (1, 512, 512)\n",
       "Path: data/dataset_20200715_2000/image;\n",
       "\n",
       "Valid: LabelList (400 items)\n",
       "x: SegmentationItemList\n",
       "Image (3, 512, 512),Image (3, 512, 512),Image (3, 512, 512),Image (3, 512, 512),Image (3, 512, 512)\n",
       "y: SegmentationLabelList\n",
       "ImageSegment (1, 512, 512),ImageSegment (1, 512, 512),ImageSegment (1, 512, 512),ImageSegment (1, 512, 512),ImageSegment (1, 512, 512)\n",
       "Path: data/dataset_20200715_2000/image;\n",
       "\n",
       "Test: None, model=DataParallel(\n",
       "  (module): Resnet_UNet(\n",
       "    (down_blocks): ModuleList(\n",
       "      (0): Sequential(\n",
       "        (0): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "        (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU(inplace=True)\n",
       "      )\n",
       "      (1): Sequential(\n",
       "        (0): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "        (1): Sequential(\n",
       "          (0): BasicBlock(\n",
       "            (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "            (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (relu): ReLU(inplace=True)\n",
       "            (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "            (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "          (1): BasicBlock(\n",
       "            (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "            (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (relu): ReLU(inplace=True)\n",
       "            (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "            (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (2): Sequential(\n",
       "        (0): BasicBlock(\n",
       "          (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (downsample): Sequential(\n",
       "            (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "            (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (1): BasicBlock(\n",
       "          (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (3): Sequential(\n",
       "        (0): BasicBlock(\n",
       "          (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (downsample): Sequential(\n",
       "            (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "            (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (1): BasicBlock(\n",
       "          (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (4): Sequential(\n",
       "        (0): BasicBlock(\n",
       "          (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (downsample): Sequential(\n",
       "            (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "            (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (1): BasicBlock(\n",
       "          (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (bridge): Bridge(\n",
       "      (conv1): Conv_Bn_ReLu(\n",
       "        (conv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (bn): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (conv2): Conv_Bn_ReLu(\n",
       "        (conv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (bn): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (side_layers): ModuleList(\n",
       "      (0): Conv_Bn_ReLu(\n",
       "        (conv): Conv2d(3, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (1): Conv_Bn_ReLu(\n",
       "        (conv): Conv2d(64, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (2): Conv_Bn_ReLu(\n",
       "        (conv): Conv2d(64, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (3): Conv_Bn_ReLu(\n",
       "        (conv): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (4): Conv_Bn_ReLu(\n",
       "        (conv): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (up_blocks): ModuleList(\n",
       "      (0): UpBlock(\n",
       "        (upsample): Sequential(\n",
       "          (0): ConvTranspose2d(512, 256, kernel_size=(2, 2), stride=(2, 2), bias=False)\n",
       "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (conv1): Conv_Bn_ReLu(\n",
       "          (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (conv2): Conv_Bn_ReLu(\n",
       "          (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): UpBlock(\n",
       "        (upsample): Sequential(\n",
       "          (0): ConvTranspose2d(256, 128, kernel_size=(2, 2), stride=(2, 2), bias=False)\n",
       "          (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (conv1): Conv_Bn_ReLu(\n",
       "          (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (conv2): Conv_Bn_ReLu(\n",
       "          (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (2): UpBlock(\n",
       "        (upsample): Sequential(\n",
       "          (0): ConvTranspose2d(128, 128, kernel_size=(2, 2), stride=(2, 2), bias=False)\n",
       "          (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (conv1): Conv_Bn_ReLu(\n",
       "          (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (conv2): Conv_Bn_ReLu(\n",
       "          (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (3): UpBlock(\n",
       "        (upsample): Sequential(\n",
       "          (0): ConvTranspose2d(128, 128, kernel_size=(2, 2), stride=(2, 2), bias=False)\n",
       "          (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (conv1): Conv_Bn_ReLu(\n",
       "          (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (conv2): Conv_Bn_ReLu(\n",
       "          (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (4): UpBlock(\n",
       "        (upsample): Sequential(\n",
       "          (0): ConvTranspose2d(128, 128, kernel_size=(2, 2), stride=(2, 2), bias=False)\n",
       "          (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (conv1): Conv_Bn_ReLu(\n",
       "          (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (conv2): Conv_Bn_ReLu(\n",
       "          (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (head): Conv2d(128, 1, kernel_size=(1, 1), stride=(1, 1))\n",
       "  )\n",
       "), opt_func=functools.partial(<class 'torch.optim.adam.Adam'>, betas=(0.9, 0.99)), loss_func=<function dice_loss at 0x7f90e485f730>, metrics=[<function dice_loss at 0x7f90e485f730>, functools.partial(<function balance_bce at 0x7f90e485f840>, balance_ratio=1), <function mask_iou at 0x7f90e485f950>], true_wd=True, bn_wd=True, wd=0.01, train_bn=True, path=PosixPath('data/dataset_20200715_2000/image'), model_dir='models', callback_fns=[functools.partial(<class 'fastai.basic_train.Recorder'>, add_time=True, silent=False)], callbacks=[], layer_groups=[ModuleList(\n",
       "  (0): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "  (1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "  (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "  (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "  (4): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "  (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "  (6): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "  (7): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "  (8): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "  (9): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "  (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "  (11): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "  (12): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "  (13): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "  (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "  (15): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "  (16): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "  (17): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "  (18): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "  (19): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "), ModuleList(\n",
       "  (0): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (5): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (6): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (7): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (8): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (9): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (10): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (11): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (12): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (13): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (14): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (15): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (16): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (17): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (18): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (19): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (20): Bridge(\n",
       "    (conv1): Conv_Bn_ReLu(\n",
       "      (conv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (bn): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (conv2): Conv_Bn_ReLu(\n",
       "      (conv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (bn): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (21): ModuleList(\n",
       "    (0): Conv_Bn_ReLu(\n",
       "      (conv): Conv2d(3, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (1): Conv_Bn_ReLu(\n",
       "      (conv): Conv2d(64, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (2): Conv_Bn_ReLu(\n",
       "      (conv): Conv2d(64, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (3): Conv_Bn_ReLu(\n",
       "      (conv): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (4): Conv_Bn_ReLu(\n",
       "      (conv): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (22): ModuleList(\n",
       "    (0): UpBlock(\n",
       "      (upsample): Sequential(\n",
       "        (0): ConvTranspose2d(512, 256, kernel_size=(2, 2), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (conv1): Conv_Bn_ReLu(\n",
       "        (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (conv2): Conv_Bn_ReLu(\n",
       "        (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): UpBlock(\n",
       "      (upsample): Sequential(\n",
       "        (0): ConvTranspose2d(256, 128, kernel_size=(2, 2), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (conv1): Conv_Bn_ReLu(\n",
       "        (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (conv2): Conv_Bn_ReLu(\n",
       "        (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (2): UpBlock(\n",
       "      (upsample): Sequential(\n",
       "        (0): ConvTranspose2d(128, 128, kernel_size=(2, 2), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (conv1): Conv_Bn_ReLu(\n",
       "        (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (conv2): Conv_Bn_ReLu(\n",
       "        (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (3): UpBlock(\n",
       "      (upsample): Sequential(\n",
       "        (0): ConvTranspose2d(128, 128, kernel_size=(2, 2), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (conv1): Conv_Bn_ReLu(\n",
       "        (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (conv2): Conv_Bn_ReLu(\n",
       "        (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (4): UpBlock(\n",
       "      (upsample): Sequential(\n",
       "        (0): ConvTranspose2d(128, 128, kernel_size=(2, 2), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (conv1): Conv_Bn_ReLu(\n",
       "        (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (conv2): Conv_Bn_ReLu(\n",
       "        (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (23): Conv2d(128, 1, kernel_size=(1, 1), stride=(1, 1))\n",
       ")], add_time=True, silent=False)"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "getlearn()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "                background: #F44336;\n",
       "            }\n",
       "        </style>\n",
       "      <progress value='9' class='' max='10', style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      90.00% [9/10 03:27<00:23]\n",
       "    </div>\n",
       "    \n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>dice_loss</th>\n",
       "      <th>balance_bce</th>\n",
       "      <th>mask_iou</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.741615</td>\n",
       "      <td>#na#</td>\n",
       "      <td>00:22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.741572</td>\n",
       "      <td>#na#</td>\n",
       "      <td>00:22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.731651</td>\n",
       "      <td>#na#</td>\n",
       "      <td>00:23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.699959</td>\n",
       "      <td>#na#</td>\n",
       "      <td>00:23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.659821</td>\n",
       "      <td>#na#</td>\n",
       "      <td>00:23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.622453</td>\n",
       "      <td>#na#</td>\n",
       "      <td>00:23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.588488</td>\n",
       "      <td>#na#</td>\n",
       "      <td>00:23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.562138</td>\n",
       "      <td>#na#</td>\n",
       "      <td>00:23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.549199</td>\n",
       "      <td>#na#</td>\n",
       "      <td>00:23</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>\n",
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "                background: #F44336;\n",
       "            }\n",
       "        </style>\n",
       "      <progress value='9' class='' max='10', style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      90.00% [9/10 00:20<00:02 0.5491]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LR Finder is complete, type {learner_name}.recorder.plot() to see the graph.\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "learn = getlearn()\n",
    "learn.opt_func = optf_adam\n",
    "learn.lr_find()\n",
    "learn.recorder.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ALLERT: You are using CumtomEpochLength, please make sure that your training dataloader is using random sampler, or this may cause problem.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "                background: #F44336;\n",
       "            }\n",
       "        </style>\n",
       "      <progress value='241' class='' max='500', style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      48.20% [241/500 1:28:54<1:35:33]\n",
       "    </div>\n",
       "    \n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>dice_loss</th>\n",
       "      <th>balance_bce</th>\n",
       "      <th>mask_iou</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.610610</td>\n",
       "      <td>0.755079</td>\n",
       "      <td>0.755079</td>\n",
       "      <td>0.668652</td>\n",
       "      <td>0.213382</td>\n",
       "      <td>00:22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.559654</td>\n",
       "      <td>0.695043</td>\n",
       "      <td>0.695043</td>\n",
       "      <td>0.564333</td>\n",
       "      <td>0.324456</td>\n",
       "      <td>00:22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.537154</td>\n",
       "      <td>0.603292</td>\n",
       "      <td>0.603292</td>\n",
       "      <td>0.600829</td>\n",
       "      <td>0.324799</td>\n",
       "      <td>00:22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.520687</td>\n",
       "      <td>0.533004</td>\n",
       "      <td>0.533004</td>\n",
       "      <td>0.804303</td>\n",
       "      <td>0.349211</td>\n",
       "      <td>00:22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.501595</td>\n",
       "      <td>0.510433</td>\n",
       "      <td>0.510433</td>\n",
       "      <td>0.967319</td>\n",
       "      <td>0.359203</td>\n",
       "      <td>00:22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.475195</td>\n",
       "      <td>0.396737</td>\n",
       "      <td>0.396737</td>\n",
       "      <td>0.674145</td>\n",
       "      <td>0.466871</td>\n",
       "      <td>00:22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.437727</td>\n",
       "      <td>0.281640</td>\n",
       "      <td>0.281640</td>\n",
       "      <td>0.715841</td>\n",
       "      <td>0.577688</td>\n",
       "      <td>00:22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.399568</td>\n",
       "      <td>0.292850</td>\n",
       "      <td>0.292850</td>\n",
       "      <td>1.086517</td>\n",
       "      <td>0.558671</td>\n",
       "      <td>00:22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.365667</td>\n",
       "      <td>0.230956</td>\n",
       "      <td>0.230956</td>\n",
       "      <td>0.586369</td>\n",
       "      <td>0.655903</td>\n",
       "      <td>00:22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.336315</td>\n",
       "      <td>0.190047</td>\n",
       "      <td>0.190047</td>\n",
       "      <td>0.652293</td>\n",
       "      <td>0.690253</td>\n",
       "      <td>00:22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.312729</td>\n",
       "      <td>0.955539</td>\n",
       "      <td>0.955539</td>\n",
       "      <td>3.235886</td>\n",
       "      <td>0.020971</td>\n",
       "      <td>00:22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.293559</td>\n",
       "      <td>0.213773</td>\n",
       "      <td>0.213773</td>\n",
       "      <td>0.694596</td>\n",
       "      <td>0.654261</td>\n",
       "      <td>00:22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.271126</td>\n",
       "      <td>0.167289</td>\n",
       "      <td>0.167289</td>\n",
       "      <td>0.676367</td>\n",
       "      <td>0.717747</td>\n",
       "      <td>00:22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.251608</td>\n",
       "      <td>0.241672</td>\n",
       "      <td>0.241672</td>\n",
       "      <td>0.613835</td>\n",
       "      <td>0.618692</td>\n",
       "      <td>00:22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.233957</td>\n",
       "      <td>0.164946</td>\n",
       "      <td>0.164946</td>\n",
       "      <td>0.407409</td>\n",
       "      <td>0.722423</td>\n",
       "      <td>00:22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.214702</td>\n",
       "      <td>0.133892</td>\n",
       "      <td>0.133892</td>\n",
       "      <td>0.496811</td>\n",
       "      <td>0.769268</td>\n",
       "      <td>00:22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.196999</td>\n",
       "      <td>0.600593</td>\n",
       "      <td>0.600593</td>\n",
       "      <td>2.648783</td>\n",
       "      <td>0.251052</td>\n",
       "      <td>00:21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.183052</td>\n",
       "      <td>0.130746</td>\n",
       "      <td>0.130746</td>\n",
       "      <td>0.403959</td>\n",
       "      <td>0.795683</td>\n",
       "      <td>00:22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.168400</td>\n",
       "      <td>0.102913</td>\n",
       "      <td>0.102913</td>\n",
       "      <td>0.459004</td>\n",
       "      <td>0.816133</td>\n",
       "      <td>00:21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.155071</td>\n",
       "      <td>0.105429</td>\n",
       "      <td>0.105429</td>\n",
       "      <td>0.452150</td>\n",
       "      <td>0.816673</td>\n",
       "      <td>00:22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.144072</td>\n",
       "      <td>0.338494</td>\n",
       "      <td>0.338494</td>\n",
       "      <td>1.908770</td>\n",
       "      <td>0.505940</td>\n",
       "      <td>00:22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>0.132928</td>\n",
       "      <td>0.096853</td>\n",
       "      <td>0.096853</td>\n",
       "      <td>0.336604</td>\n",
       "      <td>0.825832</td>\n",
       "      <td>00:22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>0.124210</td>\n",
       "      <td>0.097266</td>\n",
       "      <td>0.097266</td>\n",
       "      <td>0.328395</td>\n",
       "      <td>0.826162</td>\n",
       "      <td>00:22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>0.117380</td>\n",
       "      <td>0.086803</td>\n",
       "      <td>0.086803</td>\n",
       "      <td>0.476110</td>\n",
       "      <td>0.843166</td>\n",
       "      <td>00:22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>0.110990</td>\n",
       "      <td>0.070182</td>\n",
       "      <td>0.070182</td>\n",
       "      <td>0.276273</td>\n",
       "      <td>0.872498</td>\n",
       "      <td>00:22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>0.103382</td>\n",
       "      <td>0.133847</td>\n",
       "      <td>0.133847</td>\n",
       "      <td>0.258689</td>\n",
       "      <td>0.770380</td>\n",
       "      <td>00:22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>0.098791</td>\n",
       "      <td>0.123172</td>\n",
       "      <td>0.123172</td>\n",
       "      <td>0.331906</td>\n",
       "      <td>0.786279</td>\n",
       "      <td>00:21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>0.094365</td>\n",
       "      <td>0.152936</td>\n",
       "      <td>0.152936</td>\n",
       "      <td>1.002337</td>\n",
       "      <td>0.741024</td>\n",
       "      <td>00:22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28</td>\n",
       "      <td>0.091896</td>\n",
       "      <td>0.262351</td>\n",
       "      <td>0.262351</td>\n",
       "      <td>1.911202</td>\n",
       "      <td>0.593364</td>\n",
       "      <td>00:21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29</td>\n",
       "      <td>0.087313</td>\n",
       "      <td>0.102479</td>\n",
       "      <td>0.102479</td>\n",
       "      <td>0.620073</td>\n",
       "      <td>0.816457</td>\n",
       "      <td>00:22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>0.083281</td>\n",
       "      <td>0.074690</td>\n",
       "      <td>0.074690</td>\n",
       "      <td>0.361926</td>\n",
       "      <td>0.863041</td>\n",
       "      <td>00:22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31</td>\n",
       "      <td>0.079190</td>\n",
       "      <td>0.074643</td>\n",
       "      <td>0.074643</td>\n",
       "      <td>0.292637</td>\n",
       "      <td>0.863281</td>\n",
       "      <td>00:22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32</td>\n",
       "      <td>0.075156</td>\n",
       "      <td>0.072236</td>\n",
       "      <td>0.072236</td>\n",
       "      <td>0.312222</td>\n",
       "      <td>0.866563</td>\n",
       "      <td>00:21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33</td>\n",
       "      <td>0.070965</td>\n",
       "      <td>0.068679</td>\n",
       "      <td>0.068679</td>\n",
       "      <td>0.349022</td>\n",
       "      <td>0.875484</td>\n",
       "      <td>00:22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34</td>\n",
       "      <td>0.067410</td>\n",
       "      <td>0.062502</td>\n",
       "      <td>0.062502</td>\n",
       "      <td>0.261931</td>\n",
       "      <td>0.884110</td>\n",
       "      <td>00:22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35</td>\n",
       "      <td>0.065094</td>\n",
       "      <td>0.077847</td>\n",
       "      <td>0.077847</td>\n",
       "      <td>0.246647</td>\n",
       "      <td>0.856935</td>\n",
       "      <td>00:22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36</td>\n",
       "      <td>0.064436</td>\n",
       "      <td>0.069338</td>\n",
       "      <td>0.069338</td>\n",
       "      <td>0.283675</td>\n",
       "      <td>0.874181</td>\n",
       "      <td>00:22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>37</td>\n",
       "      <td>0.061318</td>\n",
       "      <td>0.064820</td>\n",
       "      <td>0.064820</td>\n",
       "      <td>0.284119</td>\n",
       "      <td>0.880800</td>\n",
       "      <td>00:21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>38</td>\n",
       "      <td>0.058021</td>\n",
       "      <td>0.084861</td>\n",
       "      <td>0.084861</td>\n",
       "      <td>0.428519</td>\n",
       "      <td>0.847398</td>\n",
       "      <td>00:22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>39</td>\n",
       "      <td>0.054768</td>\n",
       "      <td>0.064153</td>\n",
       "      <td>0.064153</td>\n",
       "      <td>0.339523</td>\n",
       "      <td>0.880493</td>\n",
       "      <td>00:22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>0.052757</td>\n",
       "      <td>0.070238</td>\n",
       "      <td>0.070238</td>\n",
       "      <td>0.296672</td>\n",
       "      <td>0.869677</td>\n",
       "      <td>00:22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>41</td>\n",
       "      <td>0.052762</td>\n",
       "      <td>0.270525</td>\n",
       "      <td>0.270525</td>\n",
       "      <td>1.678694</td>\n",
       "      <td>0.578177</td>\n",
       "      <td>00:22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>42</td>\n",
       "      <td>0.051586</td>\n",
       "      <td>0.058479</td>\n",
       "      <td>0.058479</td>\n",
       "      <td>0.267551</td>\n",
       "      <td>0.891094</td>\n",
       "      <td>00:22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>43</td>\n",
       "      <td>0.051883</td>\n",
       "      <td>0.145333</td>\n",
       "      <td>0.145333</td>\n",
       "      <td>1.062911</td>\n",
       "      <td>0.759310</td>\n",
       "      <td>00:22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>44</td>\n",
       "      <td>0.053915</td>\n",
       "      <td>0.091055</td>\n",
       "      <td>0.091055</td>\n",
       "      <td>0.483174</td>\n",
       "      <td>0.838437</td>\n",
       "      <td>00:22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>45</td>\n",
       "      <td>0.054913</td>\n",
       "      <td>0.062106</td>\n",
       "      <td>0.062106</td>\n",
       "      <td>0.249780</td>\n",
       "      <td>0.884925</td>\n",
       "      <td>00:22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>46</td>\n",
       "      <td>0.053509</td>\n",
       "      <td>0.061838</td>\n",
       "      <td>0.061838</td>\n",
       "      <td>0.418912</td>\n",
       "      <td>0.885323</td>\n",
       "      <td>00:22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>47</td>\n",
       "      <td>0.051416</td>\n",
       "      <td>0.067431</td>\n",
       "      <td>0.067431</td>\n",
       "      <td>0.374596</td>\n",
       "      <td>0.876844</td>\n",
       "      <td>00:22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>48</td>\n",
       "      <td>0.049930</td>\n",
       "      <td>0.062880</td>\n",
       "      <td>0.062880</td>\n",
       "      <td>0.220283</td>\n",
       "      <td>0.885956</td>\n",
       "      <td>00:21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>49</td>\n",
       "      <td>0.049237</td>\n",
       "      <td>0.061663</td>\n",
       "      <td>0.061663</td>\n",
       "      <td>0.378543</td>\n",
       "      <td>0.885265</td>\n",
       "      <td>00:22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.047513</td>\n",
       "      <td>0.071879</td>\n",
       "      <td>0.071879</td>\n",
       "      <td>0.486076</td>\n",
       "      <td>0.867151</td>\n",
       "      <td>00:22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>51</td>\n",
       "      <td>0.045563</td>\n",
       "      <td>0.063625</td>\n",
       "      <td>0.063625</td>\n",
       "      <td>0.356330</td>\n",
       "      <td>0.881424</td>\n",
       "      <td>00:22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>52</td>\n",
       "      <td>0.043921</td>\n",
       "      <td>0.059639</td>\n",
       "      <td>0.059639</td>\n",
       "      <td>0.329214</td>\n",
       "      <td>0.888040</td>\n",
       "      <td>00:21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>53</td>\n",
       "      <td>0.041860</td>\n",
       "      <td>0.061974</td>\n",
       "      <td>0.061974</td>\n",
       "      <td>0.359632</td>\n",
       "      <td>0.884229</td>\n",
       "      <td>00:22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>54</td>\n",
       "      <td>0.040070</td>\n",
       "      <td>0.050909</td>\n",
       "      <td>0.050909</td>\n",
       "      <td>0.250354</td>\n",
       "      <td>0.903707</td>\n",
       "      <td>00:22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>55</td>\n",
       "      <td>0.038381</td>\n",
       "      <td>0.052448</td>\n",
       "      <td>0.052448</td>\n",
       "      <td>0.221169</td>\n",
       "      <td>0.900924</td>\n",
       "      <td>00:21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>56</td>\n",
       "      <td>0.036692</td>\n",
       "      <td>0.051880</td>\n",
       "      <td>0.051880</td>\n",
       "      <td>0.321192</td>\n",
       "      <td>0.902330</td>\n",
       "      <td>00:21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>57</td>\n",
       "      <td>0.035488</td>\n",
       "      <td>0.055905</td>\n",
       "      <td>0.055905</td>\n",
       "      <td>0.271640</td>\n",
       "      <td>0.895287</td>\n",
       "      <td>00:22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>58</td>\n",
       "      <td>0.034700</td>\n",
       "      <td>0.054179</td>\n",
       "      <td>0.054179</td>\n",
       "      <td>0.284870</td>\n",
       "      <td>0.898066</td>\n",
       "      <td>00:22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>59</td>\n",
       "      <td>0.034180</td>\n",
       "      <td>0.056877</td>\n",
       "      <td>0.056877</td>\n",
       "      <td>0.361091</td>\n",
       "      <td>0.894768</td>\n",
       "      <td>00:22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>0.034190</td>\n",
       "      <td>0.066400</td>\n",
       "      <td>0.066400</td>\n",
       "      <td>0.269136</td>\n",
       "      <td>0.879641</td>\n",
       "      <td>00:22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>61</td>\n",
       "      <td>0.035066</td>\n",
       "      <td>0.077812</td>\n",
       "      <td>0.077812</td>\n",
       "      <td>0.439506</td>\n",
       "      <td>0.856085</td>\n",
       "      <td>00:21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>62</td>\n",
       "      <td>0.035286</td>\n",
       "      <td>0.063034</td>\n",
       "      <td>0.063034</td>\n",
       "      <td>0.363842</td>\n",
       "      <td>0.883750</td>\n",
       "      <td>00:21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>63</td>\n",
       "      <td>0.034530</td>\n",
       "      <td>0.057150</td>\n",
       "      <td>0.057150</td>\n",
       "      <td>0.429877</td>\n",
       "      <td>0.892485</td>\n",
       "      <td>00:21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>64</td>\n",
       "      <td>0.033708</td>\n",
       "      <td>0.060005</td>\n",
       "      <td>0.060005</td>\n",
       "      <td>0.444656</td>\n",
       "      <td>0.887508</td>\n",
       "      <td>00:22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>65</td>\n",
       "      <td>0.032863</td>\n",
       "      <td>0.048644</td>\n",
       "      <td>0.048644</td>\n",
       "      <td>0.253907</td>\n",
       "      <td>0.907858</td>\n",
       "      <td>00:22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>66</td>\n",
       "      <td>0.031551</td>\n",
       "      <td>0.047091</td>\n",
       "      <td>0.047091</td>\n",
       "      <td>0.248008</td>\n",
       "      <td>0.910637</td>\n",
       "      <td>00:21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>67</td>\n",
       "      <td>0.031121</td>\n",
       "      <td>0.046898</td>\n",
       "      <td>0.046898</td>\n",
       "      <td>0.272551</td>\n",
       "      <td>0.911036</td>\n",
       "      <td>00:21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>68</td>\n",
       "      <td>0.030443</td>\n",
       "      <td>0.050675</td>\n",
       "      <td>0.050675</td>\n",
       "      <td>0.270609</td>\n",
       "      <td>0.903879</td>\n",
       "      <td>00:21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>69</td>\n",
       "      <td>0.030217</td>\n",
       "      <td>0.073583</td>\n",
       "      <td>0.073583</td>\n",
       "      <td>0.324879</td>\n",
       "      <td>0.863399</td>\n",
       "      <td>00:22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>0.029851</td>\n",
       "      <td>0.050287</td>\n",
       "      <td>0.050287</td>\n",
       "      <td>0.352589</td>\n",
       "      <td>0.904761</td>\n",
       "      <td>00:22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>71</td>\n",
       "      <td>0.029296</td>\n",
       "      <td>0.072518</td>\n",
       "      <td>0.072518</td>\n",
       "      <td>0.602726</td>\n",
       "      <td>0.866574</td>\n",
       "      <td>00:21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>72</td>\n",
       "      <td>0.029200</td>\n",
       "      <td>0.052088</td>\n",
       "      <td>0.052088</td>\n",
       "      <td>0.256221</td>\n",
       "      <td>0.902086</td>\n",
       "      <td>00:21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>73</td>\n",
       "      <td>0.029235</td>\n",
       "      <td>0.077642</td>\n",
       "      <td>0.077642</td>\n",
       "      <td>0.763010</td>\n",
       "      <td>0.857066</td>\n",
       "      <td>00:21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>74</td>\n",
       "      <td>0.029837</td>\n",
       "      <td>0.080343</td>\n",
       "      <td>0.080343</td>\n",
       "      <td>0.544708</td>\n",
       "      <td>0.852472</td>\n",
       "      <td>00:22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>75</td>\n",
       "      <td>0.029959</td>\n",
       "      <td>0.059152</td>\n",
       "      <td>0.059152</td>\n",
       "      <td>0.375873</td>\n",
       "      <td>0.888732</td>\n",
       "      <td>00:22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>76</td>\n",
       "      <td>0.030072</td>\n",
       "      <td>0.058419</td>\n",
       "      <td>0.058419</td>\n",
       "      <td>0.429149</td>\n",
       "      <td>0.890404</td>\n",
       "      <td>00:22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>77</td>\n",
       "      <td>0.030497</td>\n",
       "      <td>0.062235</td>\n",
       "      <td>0.062235</td>\n",
       "      <td>0.257248</td>\n",
       "      <td>0.884597</td>\n",
       "      <td>00:21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>78</td>\n",
       "      <td>0.030073</td>\n",
       "      <td>0.056010</td>\n",
       "      <td>0.056010</td>\n",
       "      <td>0.326165</td>\n",
       "      <td>0.894900</td>\n",
       "      <td>00:22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>79</td>\n",
       "      <td>0.029428</td>\n",
       "      <td>0.052157</td>\n",
       "      <td>0.052157</td>\n",
       "      <td>0.260877</td>\n",
       "      <td>0.901435</td>\n",
       "      <td>00:22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>0.028629</td>\n",
       "      <td>0.046572</td>\n",
       "      <td>0.046572</td>\n",
       "      <td>0.217444</td>\n",
       "      <td>0.911787</td>\n",
       "      <td>00:22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>81</td>\n",
       "      <td>0.028328</td>\n",
       "      <td>0.042238</td>\n",
       "      <td>0.042238</td>\n",
       "      <td>0.267033</td>\n",
       "      <td>0.919621</td>\n",
       "      <td>00:22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>82</td>\n",
       "      <td>0.027411</td>\n",
       "      <td>0.041828</td>\n",
       "      <td>0.041828</td>\n",
       "      <td>0.271485</td>\n",
       "      <td>0.920431</td>\n",
       "      <td>00:22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>83</td>\n",
       "      <td>0.026547</td>\n",
       "      <td>0.042259</td>\n",
       "      <td>0.042259</td>\n",
       "      <td>0.263342</td>\n",
       "      <td>0.919664</td>\n",
       "      <td>00:22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>84</td>\n",
       "      <td>0.026191</td>\n",
       "      <td>0.042744</td>\n",
       "      <td>0.042744</td>\n",
       "      <td>0.257767</td>\n",
       "      <td>0.918413</td>\n",
       "      <td>00:22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>85</td>\n",
       "      <td>0.025460</td>\n",
       "      <td>0.041395</td>\n",
       "      <td>0.041395</td>\n",
       "      <td>0.252945</td>\n",
       "      <td>0.921018</td>\n",
       "      <td>00:21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>86</td>\n",
       "      <td>0.024853</td>\n",
       "      <td>0.041250</td>\n",
       "      <td>0.041250</td>\n",
       "      <td>0.267541</td>\n",
       "      <td>0.921390</td>\n",
       "      <td>00:21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>87</td>\n",
       "      <td>0.024117</td>\n",
       "      <td>0.041186</td>\n",
       "      <td>0.041186</td>\n",
       "      <td>0.259703</td>\n",
       "      <td>0.921555</td>\n",
       "      <td>00:22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>88</td>\n",
       "      <td>0.023561</td>\n",
       "      <td>0.041050</td>\n",
       "      <td>0.041050</td>\n",
       "      <td>0.272192</td>\n",
       "      <td>0.921626</td>\n",
       "      <td>00:22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>89</td>\n",
       "      <td>0.023213</td>\n",
       "      <td>0.041109</td>\n",
       "      <td>0.041109</td>\n",
       "      <td>0.280444</td>\n",
       "      <td>0.921542</td>\n",
       "      <td>00:22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>0.022785</td>\n",
       "      <td>0.040706</td>\n",
       "      <td>0.040706</td>\n",
       "      <td>0.246226</td>\n",
       "      <td>0.922188</td>\n",
       "      <td>00:22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>91</td>\n",
       "      <td>0.022884</td>\n",
       "      <td>0.041355</td>\n",
       "      <td>0.041355</td>\n",
       "      <td>0.275352</td>\n",
       "      <td>0.921025</td>\n",
       "      <td>00:22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>92</td>\n",
       "      <td>0.022458</td>\n",
       "      <td>0.041222</td>\n",
       "      <td>0.041222</td>\n",
       "      <td>0.277162</td>\n",
       "      <td>0.921343</td>\n",
       "      <td>00:21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>93</td>\n",
       "      <td>0.022028</td>\n",
       "      <td>0.041458</td>\n",
       "      <td>0.041458</td>\n",
       "      <td>0.297238</td>\n",
       "      <td>0.920825</td>\n",
       "      <td>00:22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>94</td>\n",
       "      <td>0.021785</td>\n",
       "      <td>0.041356</td>\n",
       "      <td>0.041356</td>\n",
       "      <td>0.273819</td>\n",
       "      <td>0.920947</td>\n",
       "      <td>00:22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>95</td>\n",
       "      <td>0.021583</td>\n",
       "      <td>0.041433</td>\n",
       "      <td>0.041433</td>\n",
       "      <td>0.292221</td>\n",
       "      <td>0.920889</td>\n",
       "      <td>00:22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>96</td>\n",
       "      <td>0.021241</td>\n",
       "      <td>0.041332</td>\n",
       "      <td>0.041332</td>\n",
       "      <td>0.282746</td>\n",
       "      <td>0.921063</td>\n",
       "      <td>00:21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>97</td>\n",
       "      <td>0.021422</td>\n",
       "      <td>0.040215</td>\n",
       "      <td>0.040215</td>\n",
       "      <td>0.258082</td>\n",
       "      <td>0.923118</td>\n",
       "      <td>00:22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>98</td>\n",
       "      <td>0.021471</td>\n",
       "      <td>0.042155</td>\n",
       "      <td>0.042155</td>\n",
       "      <td>0.287220</td>\n",
       "      <td>0.919645</td>\n",
       "      <td>00:21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>99</td>\n",
       "      <td>0.021186</td>\n",
       "      <td>0.042142</td>\n",
       "      <td>0.042142</td>\n",
       "      <td>0.273121</td>\n",
       "      <td>0.919583</td>\n",
       "      <td>00:22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.020948</td>\n",
       "      <td>0.041336</td>\n",
       "      <td>0.041336</td>\n",
       "      <td>0.286896</td>\n",
       "      <td>0.921121</td>\n",
       "      <td>00:22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>101</td>\n",
       "      <td>0.020847</td>\n",
       "      <td>0.041907</td>\n",
       "      <td>0.041907</td>\n",
       "      <td>0.297314</td>\n",
       "      <td>0.920099</td>\n",
       "      <td>00:21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>102</td>\n",
       "      <td>0.020552</td>\n",
       "      <td>0.041421</td>\n",
       "      <td>0.041421</td>\n",
       "      <td>0.286065</td>\n",
       "      <td>0.920851</td>\n",
       "      <td>00:22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>103</td>\n",
       "      <td>0.020518</td>\n",
       "      <td>0.041771</td>\n",
       "      <td>0.041771</td>\n",
       "      <td>0.285611</td>\n",
       "      <td>0.920192</td>\n",
       "      <td>00:22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>104</td>\n",
       "      <td>0.020296</td>\n",
       "      <td>0.041588</td>\n",
       "      <td>0.041588</td>\n",
       "      <td>0.297979</td>\n",
       "      <td>0.920531</td>\n",
       "      <td>00:22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>105</td>\n",
       "      <td>0.020231</td>\n",
       "      <td>0.041896</td>\n",
       "      <td>0.041896</td>\n",
       "      <td>0.280884</td>\n",
       "      <td>0.919926</td>\n",
       "      <td>00:22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>106</td>\n",
       "      <td>0.020086</td>\n",
       "      <td>0.041357</td>\n",
       "      <td>0.041357</td>\n",
       "      <td>0.305779</td>\n",
       "      <td>0.921016</td>\n",
       "      <td>00:22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>107</td>\n",
       "      <td>0.019867</td>\n",
       "      <td>0.042399</td>\n",
       "      <td>0.042399</td>\n",
       "      <td>0.312602</td>\n",
       "      <td>0.919035</td>\n",
       "      <td>00:22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>108</td>\n",
       "      <td>0.019671</td>\n",
       "      <td>0.041877</td>\n",
       "      <td>0.041877</td>\n",
       "      <td>0.284553</td>\n",
       "      <td>0.919959</td>\n",
       "      <td>00:21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>109</td>\n",
       "      <td>0.019597</td>\n",
       "      <td>0.042055</td>\n",
       "      <td>0.042055</td>\n",
       "      <td>0.285670</td>\n",
       "      <td>0.919717</td>\n",
       "      <td>00:22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>110</td>\n",
       "      <td>0.019423</td>\n",
       "      <td>0.041698</td>\n",
       "      <td>0.041698</td>\n",
       "      <td>0.294880</td>\n",
       "      <td>0.920481</td>\n",
       "      <td>00:21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>111</td>\n",
       "      <td>0.019198</td>\n",
       "      <td>0.041600</td>\n",
       "      <td>0.041600</td>\n",
       "      <td>0.287454</td>\n",
       "      <td>0.920573</td>\n",
       "      <td>00:22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>112</td>\n",
       "      <td>0.019037</td>\n",
       "      <td>0.041445</td>\n",
       "      <td>0.041445</td>\n",
       "      <td>0.282289</td>\n",
       "      <td>0.920828</td>\n",
       "      <td>00:21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>113</td>\n",
       "      <td>0.018887</td>\n",
       "      <td>0.041373</td>\n",
       "      <td>0.041373</td>\n",
       "      <td>0.276651</td>\n",
       "      <td>0.920875</td>\n",
       "      <td>00:21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>114</td>\n",
       "      <td>0.018725</td>\n",
       "      <td>0.042114</td>\n",
       "      <td>0.042114</td>\n",
       "      <td>0.311661</td>\n",
       "      <td>0.919576</td>\n",
       "      <td>00:22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>115</td>\n",
       "      <td>0.018705</td>\n",
       "      <td>0.041409</td>\n",
       "      <td>0.041409</td>\n",
       "      <td>0.268681</td>\n",
       "      <td>0.920918</td>\n",
       "      <td>00:22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>116</td>\n",
       "      <td>0.018580</td>\n",
       "      <td>0.041445</td>\n",
       "      <td>0.041445</td>\n",
       "      <td>0.279801</td>\n",
       "      <td>0.920662</td>\n",
       "      <td>00:21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>117</td>\n",
       "      <td>0.018562</td>\n",
       "      <td>0.042509</td>\n",
       "      <td>0.042509</td>\n",
       "      <td>0.303988</td>\n",
       "      <td>0.918836</td>\n",
       "      <td>00:22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>118</td>\n",
       "      <td>0.018387</td>\n",
       "      <td>0.041907</td>\n",
       "      <td>0.041907</td>\n",
       "      <td>0.285080</td>\n",
       "      <td>0.919892</td>\n",
       "      <td>00:22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>119</td>\n",
       "      <td>0.018205</td>\n",
       "      <td>0.041251</td>\n",
       "      <td>0.041251</td>\n",
       "      <td>0.289604</td>\n",
       "      <td>0.921072</td>\n",
       "      <td>00:22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120</td>\n",
       "      <td>0.018205</td>\n",
       "      <td>0.041206</td>\n",
       "      <td>0.041206</td>\n",
       "      <td>0.302909</td>\n",
       "      <td>0.921141</td>\n",
       "      <td>00:21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>121</td>\n",
       "      <td>0.018099</td>\n",
       "      <td>0.041452</td>\n",
       "      <td>0.041452</td>\n",
       "      <td>0.304687</td>\n",
       "      <td>0.920747</td>\n",
       "      <td>00:22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>122</td>\n",
       "      <td>0.018102</td>\n",
       "      <td>0.041658</td>\n",
       "      <td>0.041658</td>\n",
       "      <td>0.262560</td>\n",
       "      <td>0.920262</td>\n",
       "      <td>00:21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>123</td>\n",
       "      <td>0.018102</td>\n",
       "      <td>0.041465</td>\n",
       "      <td>0.041465</td>\n",
       "      <td>0.287949</td>\n",
       "      <td>0.920843</td>\n",
       "      <td>00:22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>124</td>\n",
       "      <td>0.017942</td>\n",
       "      <td>0.041072</td>\n",
       "      <td>0.041072</td>\n",
       "      <td>0.295023</td>\n",
       "      <td>0.921424</td>\n",
       "      <td>00:22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>125</td>\n",
       "      <td>0.017893</td>\n",
       "      <td>0.040860</td>\n",
       "      <td>0.040860</td>\n",
       "      <td>0.292160</td>\n",
       "      <td>0.921826</td>\n",
       "      <td>00:22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>126</td>\n",
       "      <td>0.017768</td>\n",
       "      <td>0.041112</td>\n",
       "      <td>0.041112</td>\n",
       "      <td>0.268891</td>\n",
       "      <td>0.921390</td>\n",
       "      <td>00:21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>127</td>\n",
       "      <td>0.017868</td>\n",
       "      <td>0.041302</td>\n",
       "      <td>0.041302</td>\n",
       "      <td>0.308968</td>\n",
       "      <td>0.920902</td>\n",
       "      <td>00:22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>128</td>\n",
       "      <td>0.017746</td>\n",
       "      <td>0.040946</td>\n",
       "      <td>0.040946</td>\n",
       "      <td>0.299072</td>\n",
       "      <td>0.921586</td>\n",
       "      <td>00:22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>129</td>\n",
       "      <td>0.017585</td>\n",
       "      <td>0.041103</td>\n",
       "      <td>0.041103</td>\n",
       "      <td>0.269115</td>\n",
       "      <td>0.921388</td>\n",
       "      <td>00:22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>130</td>\n",
       "      <td>0.017509</td>\n",
       "      <td>0.041347</td>\n",
       "      <td>0.041347</td>\n",
       "      <td>0.272920</td>\n",
       "      <td>0.920966</td>\n",
       "      <td>00:22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>131</td>\n",
       "      <td>0.017382</td>\n",
       "      <td>0.040484</td>\n",
       "      <td>0.040484</td>\n",
       "      <td>0.283578</td>\n",
       "      <td>0.922521</td>\n",
       "      <td>00:22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>132</td>\n",
       "      <td>0.017270</td>\n",
       "      <td>0.040723</td>\n",
       "      <td>0.040723</td>\n",
       "      <td>0.309549</td>\n",
       "      <td>0.922031</td>\n",
       "      <td>00:21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>133</td>\n",
       "      <td>0.017247</td>\n",
       "      <td>0.041068</td>\n",
       "      <td>0.041068</td>\n",
       "      <td>0.303043</td>\n",
       "      <td>0.921439</td>\n",
       "      <td>00:22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>134</td>\n",
       "      <td>0.017090</td>\n",
       "      <td>0.040706</td>\n",
       "      <td>0.040706</td>\n",
       "      <td>0.272331</td>\n",
       "      <td>0.922098</td>\n",
       "      <td>00:22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>135</td>\n",
       "      <td>0.016992</td>\n",
       "      <td>0.041262</td>\n",
       "      <td>0.041262</td>\n",
       "      <td>0.285071</td>\n",
       "      <td>0.921105</td>\n",
       "      <td>00:22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>136</td>\n",
       "      <td>0.016985</td>\n",
       "      <td>0.040995</td>\n",
       "      <td>0.040995</td>\n",
       "      <td>0.286988</td>\n",
       "      <td>0.921516</td>\n",
       "      <td>00:21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>137</td>\n",
       "      <td>0.016931</td>\n",
       "      <td>0.041185</td>\n",
       "      <td>0.041185</td>\n",
       "      <td>0.312280</td>\n",
       "      <td>0.921285</td>\n",
       "      <td>00:21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>138</td>\n",
       "      <td>0.016834</td>\n",
       "      <td>0.041166</td>\n",
       "      <td>0.041166</td>\n",
       "      <td>0.302672</td>\n",
       "      <td>0.921294</td>\n",
       "      <td>00:21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>139</td>\n",
       "      <td>0.016747</td>\n",
       "      <td>0.041519</td>\n",
       "      <td>0.041519</td>\n",
       "      <td>0.292635</td>\n",
       "      <td>0.920733</td>\n",
       "      <td>00:22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>140</td>\n",
       "      <td>0.016700</td>\n",
       "      <td>0.041228</td>\n",
       "      <td>0.041228</td>\n",
       "      <td>0.312790</td>\n",
       "      <td>0.921087</td>\n",
       "      <td>00:22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>141</td>\n",
       "      <td>0.016888</td>\n",
       "      <td>0.041682</td>\n",
       "      <td>0.041682</td>\n",
       "      <td>0.273359</td>\n",
       "      <td>0.920274</td>\n",
       "      <td>00:22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>142</td>\n",
       "      <td>0.016803</td>\n",
       "      <td>0.041503</td>\n",
       "      <td>0.041503</td>\n",
       "      <td>0.313256</td>\n",
       "      <td>0.920646</td>\n",
       "      <td>00:21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>143</td>\n",
       "      <td>0.016724</td>\n",
       "      <td>0.041594</td>\n",
       "      <td>0.041594</td>\n",
       "      <td>0.304760</td>\n",
       "      <td>0.920401</td>\n",
       "      <td>00:21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>144</td>\n",
       "      <td>0.016578</td>\n",
       "      <td>0.041406</td>\n",
       "      <td>0.041406</td>\n",
       "      <td>0.310153</td>\n",
       "      <td>0.920776</td>\n",
       "      <td>00:22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>145</td>\n",
       "      <td>0.016411</td>\n",
       "      <td>0.041366</td>\n",
       "      <td>0.041366</td>\n",
       "      <td>0.327765</td>\n",
       "      <td>0.920752</td>\n",
       "      <td>00:22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>146</td>\n",
       "      <td>0.016376</td>\n",
       "      <td>0.041533</td>\n",
       "      <td>0.041533</td>\n",
       "      <td>0.317051</td>\n",
       "      <td>0.920486</td>\n",
       "      <td>00:21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>147</td>\n",
       "      <td>0.016172</td>\n",
       "      <td>0.042526</td>\n",
       "      <td>0.042526</td>\n",
       "      <td>0.335817</td>\n",
       "      <td>0.918752</td>\n",
       "      <td>00:22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>148</td>\n",
       "      <td>0.016037</td>\n",
       "      <td>0.041354</td>\n",
       "      <td>0.041354</td>\n",
       "      <td>0.312493</td>\n",
       "      <td>0.920892</td>\n",
       "      <td>00:22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>149</td>\n",
       "      <td>0.016093</td>\n",
       "      <td>0.041638</td>\n",
       "      <td>0.041638</td>\n",
       "      <td>0.325714</td>\n",
       "      <td>0.920396</td>\n",
       "      <td>00:22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>0.016025</td>\n",
       "      <td>0.041669</td>\n",
       "      <td>0.041669</td>\n",
       "      <td>0.318977</td>\n",
       "      <td>0.920244</td>\n",
       "      <td>00:22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>151</td>\n",
       "      <td>0.015910</td>\n",
       "      <td>0.041717</td>\n",
       "      <td>0.041717</td>\n",
       "      <td>0.321800</td>\n",
       "      <td>0.920161</td>\n",
       "      <td>00:21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>152</td>\n",
       "      <td>0.015993</td>\n",
       "      <td>0.041835</td>\n",
       "      <td>0.041835</td>\n",
       "      <td>0.327049</td>\n",
       "      <td>0.919975</td>\n",
       "      <td>00:21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>153</td>\n",
       "      <td>0.015922</td>\n",
       "      <td>0.042277</td>\n",
       "      <td>0.042277</td>\n",
       "      <td>0.324828</td>\n",
       "      <td>0.919156</td>\n",
       "      <td>00:22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>154</td>\n",
       "      <td>0.015806</td>\n",
       "      <td>0.041983</td>\n",
       "      <td>0.041983</td>\n",
       "      <td>0.334495</td>\n",
       "      <td>0.919657</td>\n",
       "      <td>00:22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>155</td>\n",
       "      <td>0.015757</td>\n",
       "      <td>0.042093</td>\n",
       "      <td>0.042093</td>\n",
       "      <td>0.331242</td>\n",
       "      <td>0.919513</td>\n",
       "      <td>00:22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>156</td>\n",
       "      <td>0.015629</td>\n",
       "      <td>0.040760</td>\n",
       "      <td>0.040760</td>\n",
       "      <td>0.322943</td>\n",
       "      <td>0.921957</td>\n",
       "      <td>00:22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>157</td>\n",
       "      <td>0.015620</td>\n",
       "      <td>0.041606</td>\n",
       "      <td>0.041606</td>\n",
       "      <td>0.312760</td>\n",
       "      <td>0.920334</td>\n",
       "      <td>00:21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>158</td>\n",
       "      <td>0.015508</td>\n",
       "      <td>0.041557</td>\n",
       "      <td>0.041557</td>\n",
       "      <td>0.332950</td>\n",
       "      <td>0.920399</td>\n",
       "      <td>00:21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>159</td>\n",
       "      <td>0.015403</td>\n",
       "      <td>0.041496</td>\n",
       "      <td>0.041496</td>\n",
       "      <td>0.310691</td>\n",
       "      <td>0.920528</td>\n",
       "      <td>00:22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>160</td>\n",
       "      <td>0.015336</td>\n",
       "      <td>0.041495</td>\n",
       "      <td>0.041495</td>\n",
       "      <td>0.339502</td>\n",
       "      <td>0.920567</td>\n",
       "      <td>00:21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>161</td>\n",
       "      <td>0.015562</td>\n",
       "      <td>0.042157</td>\n",
       "      <td>0.042157</td>\n",
       "      <td>0.346137</td>\n",
       "      <td>0.919420</td>\n",
       "      <td>00:22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>162</td>\n",
       "      <td>0.015388</td>\n",
       "      <td>0.042531</td>\n",
       "      <td>0.042531</td>\n",
       "      <td>0.361046</td>\n",
       "      <td>0.918694</td>\n",
       "      <td>00:21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>163</td>\n",
       "      <td>0.015412</td>\n",
       "      <td>0.041908</td>\n",
       "      <td>0.041908</td>\n",
       "      <td>0.314560</td>\n",
       "      <td>0.919893</td>\n",
       "      <td>00:22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>164</td>\n",
       "      <td>0.015319</td>\n",
       "      <td>0.041387</td>\n",
       "      <td>0.041387</td>\n",
       "      <td>0.323885</td>\n",
       "      <td>0.920789</td>\n",
       "      <td>00:22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>165</td>\n",
       "      <td>0.015255</td>\n",
       "      <td>0.041837</td>\n",
       "      <td>0.041837</td>\n",
       "      <td>0.337351</td>\n",
       "      <td>0.919934</td>\n",
       "      <td>00:22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>166</td>\n",
       "      <td>0.015249</td>\n",
       "      <td>0.041669</td>\n",
       "      <td>0.041669</td>\n",
       "      <td>0.327014</td>\n",
       "      <td>0.920307</td>\n",
       "      <td>00:21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>167</td>\n",
       "      <td>0.015271</td>\n",
       "      <td>0.041545</td>\n",
       "      <td>0.041545</td>\n",
       "      <td>0.310895</td>\n",
       "      <td>0.920588</td>\n",
       "      <td>00:22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>168</td>\n",
       "      <td>0.015207</td>\n",
       "      <td>0.041404</td>\n",
       "      <td>0.041404</td>\n",
       "      <td>0.298103</td>\n",
       "      <td>0.920741</td>\n",
       "      <td>00:22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>169</td>\n",
       "      <td>0.015245</td>\n",
       "      <td>0.041477</td>\n",
       "      <td>0.041477</td>\n",
       "      <td>0.314153</td>\n",
       "      <td>0.920586</td>\n",
       "      <td>00:22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>170</td>\n",
       "      <td>0.015198</td>\n",
       "      <td>0.041635</td>\n",
       "      <td>0.041635</td>\n",
       "      <td>0.336906</td>\n",
       "      <td>0.920348</td>\n",
       "      <td>00:22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>171</td>\n",
       "      <td>0.015238</td>\n",
       "      <td>0.041519</td>\n",
       "      <td>0.041519</td>\n",
       "      <td>0.321128</td>\n",
       "      <td>0.920606</td>\n",
       "      <td>00:21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>172</td>\n",
       "      <td>0.015141</td>\n",
       "      <td>0.041122</td>\n",
       "      <td>0.041122</td>\n",
       "      <td>0.345020</td>\n",
       "      <td>0.921335</td>\n",
       "      <td>00:21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>173</td>\n",
       "      <td>0.014988</td>\n",
       "      <td>0.040567</td>\n",
       "      <td>0.040567</td>\n",
       "      <td>0.328333</td>\n",
       "      <td>0.922286</td>\n",
       "      <td>00:22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>174</td>\n",
       "      <td>0.014833</td>\n",
       "      <td>0.041397</td>\n",
       "      <td>0.041397</td>\n",
       "      <td>0.327940</td>\n",
       "      <td>0.920763</td>\n",
       "      <td>00:22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>175</td>\n",
       "      <td>0.014760</td>\n",
       "      <td>0.040443</td>\n",
       "      <td>0.040443</td>\n",
       "      <td>0.308204</td>\n",
       "      <td>0.922539</td>\n",
       "      <td>00:22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>176</td>\n",
       "      <td>0.014681</td>\n",
       "      <td>0.041108</td>\n",
       "      <td>0.041108</td>\n",
       "      <td>0.331784</td>\n",
       "      <td>0.921293</td>\n",
       "      <td>00:21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>177</td>\n",
       "      <td>0.014602</td>\n",
       "      <td>0.040817</td>\n",
       "      <td>0.040817</td>\n",
       "      <td>0.338135</td>\n",
       "      <td>0.921932</td>\n",
       "      <td>00:21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>178</td>\n",
       "      <td>0.014559</td>\n",
       "      <td>0.041153</td>\n",
       "      <td>0.041153</td>\n",
       "      <td>0.346447</td>\n",
       "      <td>0.921231</td>\n",
       "      <td>00:21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>179</td>\n",
       "      <td>0.014454</td>\n",
       "      <td>0.041617</td>\n",
       "      <td>0.041617</td>\n",
       "      <td>0.341544</td>\n",
       "      <td>0.920483</td>\n",
       "      <td>00:22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>180</td>\n",
       "      <td>0.014437</td>\n",
       "      <td>0.041336</td>\n",
       "      <td>0.041336</td>\n",
       "      <td>0.359627</td>\n",
       "      <td>0.920922</td>\n",
       "      <td>00:21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>181</td>\n",
       "      <td>0.014371</td>\n",
       "      <td>0.041400</td>\n",
       "      <td>0.041400</td>\n",
       "      <td>0.329346</td>\n",
       "      <td>0.920782</td>\n",
       "      <td>00:22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>182</td>\n",
       "      <td>0.014304</td>\n",
       "      <td>0.041076</td>\n",
       "      <td>0.041076</td>\n",
       "      <td>0.329261</td>\n",
       "      <td>0.921335</td>\n",
       "      <td>00:21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>183</td>\n",
       "      <td>0.014294</td>\n",
       "      <td>0.041180</td>\n",
       "      <td>0.041180</td>\n",
       "      <td>0.323617</td>\n",
       "      <td>0.921220</td>\n",
       "      <td>00:22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>184</td>\n",
       "      <td>0.014237</td>\n",
       "      <td>0.040818</td>\n",
       "      <td>0.040818</td>\n",
       "      <td>0.343529</td>\n",
       "      <td>0.921803</td>\n",
       "      <td>00:22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>185</td>\n",
       "      <td>0.014208</td>\n",
       "      <td>0.041027</td>\n",
       "      <td>0.041027</td>\n",
       "      <td>0.326496</td>\n",
       "      <td>0.921487</td>\n",
       "      <td>00:22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>186</td>\n",
       "      <td>0.014151</td>\n",
       "      <td>0.041156</td>\n",
       "      <td>0.041156</td>\n",
       "      <td>0.336165</td>\n",
       "      <td>0.921189</td>\n",
       "      <td>00:21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>187</td>\n",
       "      <td>0.014074</td>\n",
       "      <td>0.041221</td>\n",
       "      <td>0.041221</td>\n",
       "      <td>0.349939</td>\n",
       "      <td>0.921063</td>\n",
       "      <td>00:22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>188</td>\n",
       "      <td>0.014062</td>\n",
       "      <td>0.041542</td>\n",
       "      <td>0.041542</td>\n",
       "      <td>0.358565</td>\n",
       "      <td>0.920477</td>\n",
       "      <td>00:21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>189</td>\n",
       "      <td>0.014043</td>\n",
       "      <td>0.041072</td>\n",
       "      <td>0.041072</td>\n",
       "      <td>0.339146</td>\n",
       "      <td>0.921328</td>\n",
       "      <td>00:22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>190</td>\n",
       "      <td>0.014104</td>\n",
       "      <td>0.041167</td>\n",
       "      <td>0.041167</td>\n",
       "      <td>0.308492</td>\n",
       "      <td>0.921246</td>\n",
       "      <td>00:21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>191</td>\n",
       "      <td>0.014045</td>\n",
       "      <td>0.040841</td>\n",
       "      <td>0.040841</td>\n",
       "      <td>0.356795</td>\n",
       "      <td>0.921716</td>\n",
       "      <td>00:21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>192</td>\n",
       "      <td>0.014029</td>\n",
       "      <td>0.041050</td>\n",
       "      <td>0.041050</td>\n",
       "      <td>0.335872</td>\n",
       "      <td>0.921397</td>\n",
       "      <td>00:21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>193</td>\n",
       "      <td>0.013995</td>\n",
       "      <td>0.041484</td>\n",
       "      <td>0.041484</td>\n",
       "      <td>0.381374</td>\n",
       "      <td>0.920596</td>\n",
       "      <td>00:22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>194</td>\n",
       "      <td>0.013971</td>\n",
       "      <td>0.041497</td>\n",
       "      <td>0.041497</td>\n",
       "      <td>0.350933</td>\n",
       "      <td>0.920593</td>\n",
       "      <td>00:22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>195</td>\n",
       "      <td>0.013947</td>\n",
       "      <td>0.041862</td>\n",
       "      <td>0.041862</td>\n",
       "      <td>0.371294</td>\n",
       "      <td>0.919969</td>\n",
       "      <td>00:22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>196</td>\n",
       "      <td>0.013842</td>\n",
       "      <td>0.041416</td>\n",
       "      <td>0.041416</td>\n",
       "      <td>0.354529</td>\n",
       "      <td>0.920730</td>\n",
       "      <td>00:22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>197</td>\n",
       "      <td>0.013826</td>\n",
       "      <td>0.041236</td>\n",
       "      <td>0.041236</td>\n",
       "      <td>0.370408</td>\n",
       "      <td>0.921018</td>\n",
       "      <td>00:22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>198</td>\n",
       "      <td>0.013928</td>\n",
       "      <td>0.041032</td>\n",
       "      <td>0.041032</td>\n",
       "      <td>0.370999</td>\n",
       "      <td>0.921400</td>\n",
       "      <td>00:22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>199</td>\n",
       "      <td>0.013865</td>\n",
       "      <td>0.041313</td>\n",
       "      <td>0.041313</td>\n",
       "      <td>0.354399</td>\n",
       "      <td>0.920939</td>\n",
       "      <td>00:22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.013943</td>\n",
       "      <td>0.043748</td>\n",
       "      <td>0.043748</td>\n",
       "      <td>0.368002</td>\n",
       "      <td>0.916543</td>\n",
       "      <td>00:22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>201</td>\n",
       "      <td>0.013980</td>\n",
       "      <td>0.043787</td>\n",
       "      <td>0.043787</td>\n",
       "      <td>0.405744</td>\n",
       "      <td>0.916450</td>\n",
       "      <td>00:22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>202</td>\n",
       "      <td>0.013935</td>\n",
       "      <td>0.041886</td>\n",
       "      <td>0.041886</td>\n",
       "      <td>0.383716</td>\n",
       "      <td>0.919844</td>\n",
       "      <td>00:22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>203</td>\n",
       "      <td>0.013936</td>\n",
       "      <td>0.040793</td>\n",
       "      <td>0.040793</td>\n",
       "      <td>0.364321</td>\n",
       "      <td>0.921962</td>\n",
       "      <td>00:22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>204</td>\n",
       "      <td>0.013804</td>\n",
       "      <td>0.040878</td>\n",
       "      <td>0.040878</td>\n",
       "      <td>0.360813</td>\n",
       "      <td>0.921740</td>\n",
       "      <td>00:22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>205</td>\n",
       "      <td>0.013802</td>\n",
       "      <td>0.040597</td>\n",
       "      <td>0.040597</td>\n",
       "      <td>0.344264</td>\n",
       "      <td>0.922349</td>\n",
       "      <td>00:22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>206</td>\n",
       "      <td>0.013738</td>\n",
       "      <td>0.040514</td>\n",
       "      <td>0.040514</td>\n",
       "      <td>0.361239</td>\n",
       "      <td>0.922429</td>\n",
       "      <td>00:21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>207</td>\n",
       "      <td>0.013720</td>\n",
       "      <td>0.040989</td>\n",
       "      <td>0.040989</td>\n",
       "      <td>0.360320</td>\n",
       "      <td>0.921537</td>\n",
       "      <td>00:22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>208</td>\n",
       "      <td>0.013754</td>\n",
       "      <td>0.040423</td>\n",
       "      <td>0.040423</td>\n",
       "      <td>0.361343</td>\n",
       "      <td>0.922604</td>\n",
       "      <td>00:22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>209</td>\n",
       "      <td>0.013695</td>\n",
       "      <td>0.040323</td>\n",
       "      <td>0.040323</td>\n",
       "      <td>0.340544</td>\n",
       "      <td>0.922706</td>\n",
       "      <td>00:22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>210</td>\n",
       "      <td>0.013578</td>\n",
       "      <td>0.040208</td>\n",
       "      <td>0.040208</td>\n",
       "      <td>0.348666</td>\n",
       "      <td>0.922948</td>\n",
       "      <td>00:22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>211</td>\n",
       "      <td>0.013668</td>\n",
       "      <td>0.040204</td>\n",
       "      <td>0.040204</td>\n",
       "      <td>0.348085</td>\n",
       "      <td>0.922970</td>\n",
       "      <td>00:22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>212</td>\n",
       "      <td>0.013598</td>\n",
       "      <td>0.040296</td>\n",
       "      <td>0.040296</td>\n",
       "      <td>0.362584</td>\n",
       "      <td>0.922781</td>\n",
       "      <td>00:21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>213</td>\n",
       "      <td>0.013562</td>\n",
       "      <td>0.040292</td>\n",
       "      <td>0.040292</td>\n",
       "      <td>0.321232</td>\n",
       "      <td>0.922795</td>\n",
       "      <td>00:22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>214</td>\n",
       "      <td>0.013604</td>\n",
       "      <td>0.040381</td>\n",
       "      <td>0.040381</td>\n",
       "      <td>0.339742</td>\n",
       "      <td>0.922620</td>\n",
       "      <td>00:22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>215</td>\n",
       "      <td>0.013444</td>\n",
       "      <td>0.040429</td>\n",
       "      <td>0.040429</td>\n",
       "      <td>0.363546</td>\n",
       "      <td>0.922552</td>\n",
       "      <td>00:22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>216</td>\n",
       "      <td>0.013386</td>\n",
       "      <td>0.040244</td>\n",
       "      <td>0.040244</td>\n",
       "      <td>0.355934</td>\n",
       "      <td>0.922962</td>\n",
       "      <td>00:21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>217</td>\n",
       "      <td>0.013408</td>\n",
       "      <td>0.040205</td>\n",
       "      <td>0.040205</td>\n",
       "      <td>0.350827</td>\n",
       "      <td>0.923024</td>\n",
       "      <td>00:21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>218</td>\n",
       "      <td>0.013316</td>\n",
       "      <td>0.040518</td>\n",
       "      <td>0.040518</td>\n",
       "      <td>0.336288</td>\n",
       "      <td>0.922355</td>\n",
       "      <td>00:21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>219</td>\n",
       "      <td>0.013286</td>\n",
       "      <td>0.040607</td>\n",
       "      <td>0.040607</td>\n",
       "      <td>0.344440</td>\n",
       "      <td>0.922169</td>\n",
       "      <td>00:22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>220</td>\n",
       "      <td>0.013193</td>\n",
       "      <td>0.040500</td>\n",
       "      <td>0.040500</td>\n",
       "      <td>0.346701</td>\n",
       "      <td>0.922383</td>\n",
       "      <td>00:22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>221</td>\n",
       "      <td>0.013222</td>\n",
       "      <td>0.040469</td>\n",
       "      <td>0.040469</td>\n",
       "      <td>0.358268</td>\n",
       "      <td>0.922533</td>\n",
       "      <td>00:22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>222</td>\n",
       "      <td>0.013175</td>\n",
       "      <td>0.040225</td>\n",
       "      <td>0.040225</td>\n",
       "      <td>0.357566</td>\n",
       "      <td>0.922958</td>\n",
       "      <td>00:21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>223</td>\n",
       "      <td>0.013358</td>\n",
       "      <td>0.040597</td>\n",
       "      <td>0.040597</td>\n",
       "      <td>0.365880</td>\n",
       "      <td>0.922357</td>\n",
       "      <td>00:22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>224</td>\n",
       "      <td>0.013298</td>\n",
       "      <td>0.040492</td>\n",
       "      <td>0.040492</td>\n",
       "      <td>0.363777</td>\n",
       "      <td>0.922480</td>\n",
       "      <td>00:22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>225</td>\n",
       "      <td>0.013207</td>\n",
       "      <td>0.040486</td>\n",
       "      <td>0.040486</td>\n",
       "      <td>0.339124</td>\n",
       "      <td>0.922403</td>\n",
       "      <td>00:22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>226</td>\n",
       "      <td>0.013142</td>\n",
       "      <td>0.040397</td>\n",
       "      <td>0.040397</td>\n",
       "      <td>0.341433</td>\n",
       "      <td>0.922589</td>\n",
       "      <td>00:21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>227</td>\n",
       "      <td>0.013079</td>\n",
       "      <td>0.040235</td>\n",
       "      <td>0.040235</td>\n",
       "      <td>0.344639</td>\n",
       "      <td>0.922913</td>\n",
       "      <td>00:22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>228</td>\n",
       "      <td>0.013117</td>\n",
       "      <td>0.040188</td>\n",
       "      <td>0.040188</td>\n",
       "      <td>0.350575</td>\n",
       "      <td>0.922999</td>\n",
       "      <td>00:22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>229</td>\n",
       "      <td>0.013070</td>\n",
       "      <td>0.040264</td>\n",
       "      <td>0.040264</td>\n",
       "      <td>0.351580</td>\n",
       "      <td>0.922877</td>\n",
       "      <td>00:22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>230</td>\n",
       "      <td>0.013075</td>\n",
       "      <td>0.040339</td>\n",
       "      <td>0.040339</td>\n",
       "      <td>0.358766</td>\n",
       "      <td>0.922745</td>\n",
       "      <td>00:22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>231</td>\n",
       "      <td>0.013008</td>\n",
       "      <td>0.040386</td>\n",
       "      <td>0.040386</td>\n",
       "      <td>0.341280</td>\n",
       "      <td>0.922613</td>\n",
       "      <td>00:22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>232</td>\n",
       "      <td>0.013068</td>\n",
       "      <td>0.040460</td>\n",
       "      <td>0.040460</td>\n",
       "      <td>0.365613</td>\n",
       "      <td>0.922517</td>\n",
       "      <td>00:22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>233</td>\n",
       "      <td>0.013030</td>\n",
       "      <td>0.040590</td>\n",
       "      <td>0.040590</td>\n",
       "      <td>0.366855</td>\n",
       "      <td>0.922250</td>\n",
       "      <td>00:22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>234</td>\n",
       "      <td>0.012951</td>\n",
       "      <td>0.040558</td>\n",
       "      <td>0.040558</td>\n",
       "      <td>0.356288</td>\n",
       "      <td>0.922314</td>\n",
       "      <td>00:22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>235</td>\n",
       "      <td>0.013029</td>\n",
       "      <td>0.040592</td>\n",
       "      <td>0.040592</td>\n",
       "      <td>0.356975</td>\n",
       "      <td>0.922282</td>\n",
       "      <td>00:22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>236</td>\n",
       "      <td>0.013069</td>\n",
       "      <td>0.040755</td>\n",
       "      <td>0.040755</td>\n",
       "      <td>0.359724</td>\n",
       "      <td>0.921948</td>\n",
       "      <td>00:21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>237</td>\n",
       "      <td>0.013049</td>\n",
       "      <td>0.040672</td>\n",
       "      <td>0.040672</td>\n",
       "      <td>0.352912</td>\n",
       "      <td>0.922066</td>\n",
       "      <td>00:21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>238</td>\n",
       "      <td>0.013040</td>\n",
       "      <td>0.040770</td>\n",
       "      <td>0.040770</td>\n",
       "      <td>0.358992</td>\n",
       "      <td>0.921910</td>\n",
       "      <td>00:21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>239</td>\n",
       "      <td>0.012989</td>\n",
       "      <td>0.040646</td>\n",
       "      <td>0.040646</td>\n",
       "      <td>0.363330</td>\n",
       "      <td>0.922131</td>\n",
       "      <td>00:22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>240</td>\n",
       "      <td>0.012935</td>\n",
       "      <td>0.040574</td>\n",
       "      <td>0.040574</td>\n",
       "      <td>0.380086</td>\n",
       "      <td>0.922264</td>\n",
       "      <td>00:22</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>\n",
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "                background: #F44336;\n",
       "            }\n",
       "        </style>\n",
       "      <progress value='3' class='' max='3', style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      100.00% [3/3 00:02<00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Better model found at epoch 0 with valid_loss value: 0.755079448223114.\n",
      "Better model found at epoch 1 with valid_loss value: 0.695042610168457.\n",
      "Better model found at epoch 2 with valid_loss value: 0.6032920479774475.\n",
      "Better model found at epoch 3 with valid_loss value: 0.5330035090446472.\n",
      "Better model found at epoch 4 with valid_loss value: 0.5104333162307739.\n",
      "Better model found at epoch 5 with valid_loss value: 0.3967370092868805.\n",
      "Better model found at epoch 6 with valid_loss value: 0.2816401720046997.\n",
      "Better model found at epoch 8 with valid_loss value: 0.230955570936203.\n",
      "Better model found at epoch 9 with valid_loss value: 0.19004663825035095.\n",
      "Better model found at epoch 12 with valid_loss value: 0.16728903353214264.\n",
      "Better model found at epoch 14 with valid_loss value: 0.1649456024169922.\n",
      "Better model found at epoch 15 with valid_loss value: 0.1338915377855301.\n",
      "Better model found at epoch 17 with valid_loss value: 0.13074588775634766.\n",
      "Better model found at epoch 18 with valid_loss value: 0.10291296243667603.\n",
      "Better model found at epoch 21 with valid_loss value: 0.09685271978378296.\n",
      "Better model found at epoch 23 with valid_loss value: 0.08680301904678345.\n",
      "Better model found at epoch 24 with valid_loss value: 0.07018208503723145.\n",
      "Better model found at epoch 33 with valid_loss value: 0.06867896020412445.\n",
      "Better model found at epoch 34 with valid_loss value: 0.06250214576721191.\n",
      "Better model found at epoch 42 with valid_loss value: 0.05847909301519394.\n",
      "Better model found at epoch 54 with valid_loss value: 0.050908587872982025.\n",
      "Better model found at epoch 65 with valid_loss value: 0.04864424467086792.\n",
      "Better model found at epoch 66 with valid_loss value: 0.04709111526608467.\n",
      "Better model found at epoch 67 with valid_loss value: 0.04689768701791763.\n",
      "on end of epoch#77: start annealing from 0.001 to 0.0001\n",
      "Better model found at epoch 80 with valid_loss value: 0.046572040766477585.\n",
      "Better model found at epoch 81 with valid_loss value: 0.042237889021635056.\n",
      "Better model found at epoch 82 with valid_loss value: 0.04182792827486992.\n",
      "Better model found at epoch 85 with valid_loss value: 0.04139496013522148.\n",
      "Better model found at epoch 86 with valid_loss value: 0.04125000163912773.\n",
      "Better model found at epoch 87 with valid_loss value: 0.04118574783205986.\n",
      "Better model found at epoch 88 with valid_loss value: 0.041050173342227936.\n",
      "Better model found at epoch 90 with valid_loss value: 0.040705643594264984.\n",
      "Better model found at epoch 97 with valid_loss value: 0.04021521657705307.\n",
      "on end of epoch#202: start annealing from 0.0001 to 1e-05\n",
      "Better model found at epoch 210 with valid_loss value: 0.040208280086517334.\n",
      "Better model found at epoch 211 with valid_loss value: 0.040203630924224854.\n",
      "Better model found at epoch 228 with valid_loss value: 0.04018794372677803.\n",
      "on end of epoch#239: start annealing from 1e-05 to 1.0000000000000002e-06\n"
     ]
    }
   ],
   "source": [
    "lrs = [1e-3]\n",
    "checkpoints = [None]\n",
    "opts = [optf_adam]\n",
    "\n",
    "nb_train_script_logger.multi_train(get_learn=getlearn, \n",
    "            epoch_len=1e9, epochs=500,\n",
    "            opts=opts, lrs=lrs, checkpoints=checkpoints,\n",
    "            tb_log_root='./tb_log/',\n",
    "            autoSave=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "backup_train_logs(tag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "epochs:240,train_loss:0.012935,mask_iou:0.922264"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### 2000数据集，无transform,resnet18，allres,dice,adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "tag = '__'.join(['resnet18', 'allres', 'dice_loss', 'dataset2000','adam'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "getlearn = partial(get_learn, data = data_2000, model_name = 'resnet18'\n",
    "                   , loss_func_name = 'dice_loss', allres = True, tag = tag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Learner(data=ImageDataBunch;\n",
       "\n",
       "Train: LabelList (1600 items)\n",
       "x: SegmentationItemList\n",
       "Image (3, 512, 512),Image (3, 512, 512),Image (3, 512, 512),Image (3, 512, 512),Image (3, 512, 512)\n",
       "y: SegmentationLabelList\n",
       "ImageSegment (1, 512, 512),ImageSegment (1, 512, 512),ImageSegment (1, 512, 512),ImageSegment (1, 512, 512),ImageSegment (1, 512, 512)\n",
       "Path: data/dataset_20200715_2000/image;\n",
       "\n",
       "Valid: LabelList (400 items)\n",
       "x: SegmentationItemList\n",
       "Image (3, 512, 512),Image (3, 512, 512),Image (3, 512, 512),Image (3, 512, 512),Image (3, 512, 512)\n",
       "y: SegmentationLabelList\n",
       "ImageSegment (1, 512, 512),ImageSegment (1, 512, 512),ImageSegment (1, 512, 512),ImageSegment (1, 512, 512),ImageSegment (1, 512, 512)\n",
       "Path: data/dataset_20200715_2000/image;\n",
       "\n",
       "Test: None, model=DataParallel(\n",
       "  (module): Resnet_UNet(\n",
       "    (down_blocks): ModuleList(\n",
       "      (0): Sequential(\n",
       "        (0): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "        (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU(inplace=True)\n",
       "      )\n",
       "      (1): Sequential(\n",
       "        (0): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "        (1): Sequential(\n",
       "          (0): BasicBlock(\n",
       "            (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "            (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (relu): ReLU(inplace=True)\n",
       "            (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "            (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "          (1): BasicBlock(\n",
       "            (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "            (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (relu): ReLU(inplace=True)\n",
       "            (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "            (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (2): Sequential(\n",
       "        (0): BasicBlock(\n",
       "          (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (downsample): Sequential(\n",
       "            (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "            (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (1): BasicBlock(\n",
       "          (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (3): Sequential(\n",
       "        (0): BasicBlock(\n",
       "          (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (downsample): Sequential(\n",
       "            (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "            (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (1): BasicBlock(\n",
       "          (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (4): Sequential(\n",
       "        (0): BasicBlock(\n",
       "          (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (downsample): Sequential(\n",
       "            (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "            (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (1): BasicBlock(\n",
       "          (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (bridge): Bridge(\n",
       "      (conv1): Conv_Bn_ReLu(\n",
       "        (conv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (bn): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (conv2): Conv_Bn_ReLu(\n",
       "        (conv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (bn): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (side_layers): ModuleList(\n",
       "      (0): Conv_Bn_ReLu(\n",
       "        (conv): Conv2d(3, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (1): Conv_Bn_ReLu(\n",
       "        (conv): Conv2d(64, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (2): Conv_Bn_ReLu(\n",
       "        (conv): Conv2d(64, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (3): Conv_Bn_ReLu(\n",
       "        (conv): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (4): Conv_Bn_ReLu(\n",
       "        (conv): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (up_blocks): ModuleList(\n",
       "      (0): UpBlock(\n",
       "        (upsample): Sequential(\n",
       "          (0): ConvTranspose2d(512, 256, kernel_size=(2, 2), stride=(2, 2), bias=False)\n",
       "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (conv1): Conv_Bn_ReLu(\n",
       "          (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (conv2): Conv_Bn_ReLu(\n",
       "          (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): UpBlock(\n",
       "        (upsample): Sequential(\n",
       "          (0): ConvTranspose2d(256, 128, kernel_size=(2, 2), stride=(2, 2), bias=False)\n",
       "          (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (conv1): Conv_Bn_ReLu(\n",
       "          (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (conv2): Conv_Bn_ReLu(\n",
       "          (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (2): UpBlock(\n",
       "        (upsample): Sequential(\n",
       "          (0): ConvTranspose2d(128, 128, kernel_size=(2, 2), stride=(2, 2), bias=False)\n",
       "          (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (conv1): Conv_Bn_ReLu(\n",
       "          (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (conv2): Conv_Bn_ReLu(\n",
       "          (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (3): UpBlock(\n",
       "        (upsample): Sequential(\n",
       "          (0): ConvTranspose2d(128, 128, kernel_size=(2, 2), stride=(2, 2), bias=False)\n",
       "          (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (conv1): Conv_Bn_ReLu(\n",
       "          (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (conv2): Conv_Bn_ReLu(\n",
       "          (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (4): UpBlock(\n",
       "        (upsample): Sequential(\n",
       "          (0): ConvTranspose2d(128, 128, kernel_size=(2, 2), stride=(2, 2), bias=False)\n",
       "          (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (conv1): Conv_Bn_ReLu(\n",
       "          (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (conv2): Conv_Bn_ReLu(\n",
       "          (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (head): Conv2d(128, 1, kernel_size=(1, 1), stride=(1, 1))\n",
       "  )\n",
       "), opt_func=functools.partial(<class 'torch.optim.adam.Adam'>, betas=(0.9, 0.99)), loss_func=<function dice_loss at 0x7f90e485f730>, metrics=[<function dice_loss at 0x7f90e485f730>, functools.partial(<function balance_bce at 0x7f90e485f840>, balance_ratio=1), <function mask_iou at 0x7f90e485f950>], true_wd=True, bn_wd=True, wd=0.01, train_bn=True, path=PosixPath('data/dataset_20200715_2000/image'), model_dir='models', callback_fns=[functools.partial(<class 'fastai.basic_train.Recorder'>, add_time=True, silent=False)], callbacks=[], layer_groups=[ModuleList(\n",
       "  (0): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "  (1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "  (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "  (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "  (4): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "  (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "  (6): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "  (7): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "  (8): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "  (9): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "  (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "  (11): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "  (12): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "  (13): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "  (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "  (15): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "  (16): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "  (17): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "  (18): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "  (19): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "), ModuleList(\n",
       "  (0): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (5): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (6): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (7): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (8): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (9): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (10): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (11): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (12): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (13): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (14): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (15): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (16): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (17): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (18): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (19): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (20): Bridge(\n",
       "    (conv1): Conv_Bn_ReLu(\n",
       "      (conv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (bn): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (conv2): Conv_Bn_ReLu(\n",
       "      (conv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (bn): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (21): ModuleList(\n",
       "    (0): Conv_Bn_ReLu(\n",
       "      (conv): Conv2d(3, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (1): Conv_Bn_ReLu(\n",
       "      (conv): Conv2d(64, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (2): Conv_Bn_ReLu(\n",
       "      (conv): Conv2d(64, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (3): Conv_Bn_ReLu(\n",
       "      (conv): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (4): Conv_Bn_ReLu(\n",
       "      (conv): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (22): ModuleList(\n",
       "    (0): UpBlock(\n",
       "      (upsample): Sequential(\n",
       "        (0): ConvTranspose2d(512, 256, kernel_size=(2, 2), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (conv1): Conv_Bn_ReLu(\n",
       "        (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (conv2): Conv_Bn_ReLu(\n",
       "        (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): UpBlock(\n",
       "      (upsample): Sequential(\n",
       "        (0): ConvTranspose2d(256, 128, kernel_size=(2, 2), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (conv1): Conv_Bn_ReLu(\n",
       "        (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (conv2): Conv_Bn_ReLu(\n",
       "        (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (2): UpBlock(\n",
       "      (upsample): Sequential(\n",
       "        (0): ConvTranspose2d(128, 128, kernel_size=(2, 2), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (conv1): Conv_Bn_ReLu(\n",
       "        (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (conv2): Conv_Bn_ReLu(\n",
       "        (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (3): UpBlock(\n",
       "      (upsample): Sequential(\n",
       "        (0): ConvTranspose2d(128, 128, kernel_size=(2, 2), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (conv1): Conv_Bn_ReLu(\n",
       "        (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (conv2): Conv_Bn_ReLu(\n",
       "        (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (4): UpBlock(\n",
       "      (upsample): Sequential(\n",
       "        (0): ConvTranspose2d(128, 128, kernel_size=(2, 2), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (conv1): Conv_Bn_ReLu(\n",
       "        (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (conv2): Conv_Bn_ReLu(\n",
       "        (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (23): Conv2d(128, 1, kernel_size=(1, 1), stride=(1, 1))\n",
       ")], add_time=True, silent=False)"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "getlearn()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "                background: #F44336;\n",
       "            }\n",
       "        </style>\n",
       "      <progress value='0' class='' max='1', style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      0.00% [0/1 00:00<00:00]\n",
       "    </div>\n",
       "    \n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>dice_loss</th>\n",
       "      <th>balance_bce</th>\n",
       "      <th>mask_iou</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>\n",
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "                background: #F44336;\n",
       "            }\n",
       "        </style>\n",
       "      <progress value='99' class='' max='100', style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      99.00% [99/100 03:47<00:02 0.5506]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LR Finder is complete, type {learner_name}.recorder.plot() to see the graph.\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYgAAAEGCAYAAAB/+QKOAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjMsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+AADFEAAAgAElEQVR4nO3deXhU5d3/8fc3k5UAgUCAEJYAgrJvAUREsaIF912wWor7gmu1P31sH63Wp5utWhSVulZFRFRcK1IrboAQdggCYU9YEsIayJ7790dGDTAJIWRyMsnndV1zOXOfZT6JQ75zzn3OfZtzDhERkcOFeR1ARETqJhUIEREJSAVCREQCUoEQEZGAVCBERCSgcK8D1JSWLVu65ORkr2OIiISUhQsX7nTOJQRaVm8KRHJyMqmpqV7HEBEJKWa2qaJlOsUkIiIBqUCIiEhAKhAiIhKQCoSIiASkAiEiIgGpQIiISEAqECIiElC9uQ+iugqLS1mzYz85BwrJyS1g14FCYqPCuXxgO8J9h9bPg4XF/Hv5ds7rm0hUuM+jxCIitaPBF4i9eUWcN/GbI9r/vWI7E8f2Jy4mAoCM3Qe54V8LWbVtH7kFxYw7JbmWk4qI1K4GXyDiYyN5/pqBtGwcSXxsFPGxkXyyfBu/m7GCiyd9y4vjBrEzt4CbX1tIYUkpbeOieXdxpgqEiNR7Db5A+MKMn/dsc0jb2MEd6NQyllteX8gFT39DflEJ7Zo34oVxKfx3VRaPfbKKddm5dElo7FFqEZHgUyd1BU7u3IIPJpxKxxaNGN41gRm3DqNLQmMu7NeWMIP3FmV6HVFEJKga/BFEZdrHN+Kj24cf0taqaTSndk3gvcWZ3HNWN8LCzKN0IiLBpSOIarikfxKZe/KYv3GX11FERIJGBaIazu7ZmthIn04ziUi9pgJRDY0iwxnVK5FPlm8jv6gEgBWZe/nlS/N547sKh1YXEQkp6oOopksHJPHOogw+WLKV9OxcXvxmAwZ8tSabmAgflwxo53VEEZHjogJRTSd3bkFiXDS/eWcZAGMHt+fus7px19Ql3Dd9GU2jIxjZo7XHKUVEqk+nmKopLMy46bTO9G0Xx1s3nswfL+lDqybRTP5lCj3bNuW2KYv4bn2O1zFFRKrNnHNeZ6gRKSkprq7MSb3rQCGXPzeH7XvzGXdKMtcM7UhiXIzXsUREjmBmC51zKYGW6QgiCOJjI3n9+iEMO6Elz365jlP//AUTpizi++37vI4mIlJlQS0QZjbKzFabWbqZ3R9g+RNmtsT/WGNme8otKym37INg5gyGxLgYJv8yha/uO4NrhyXz5Zpsxkyex459+V5HExGpkqCdYjIzH7AGOAvIABYAY51zaRWsfzvQ3zl3rf91rnOuyoMd1aVTTIGsy87l3H98zeBOLXh1/CDMdAe2iHjPq1NMg4F059x651whMBW4sJL1xwJvBjGPp7okNObBc7rz1Zps/jVX90qISN0XzAKRBGwp9zrD33YEM+sIdAL+W6452sxSzWyemV1UwXY3+tdJzc7OrqncQXP1yR0ZcWIC//fJKtKz9nsdR0SkUsEsEIHOoVR0PmsMMN05V1KurYP/sOcq4Ekz63LEzpyb7JxLcc6lJCQkHH/iIDMz/nJpHxpF+rhz6hIKi0u9jiQiUqFgFogMoH251+2ArRWsO4bDTi8557b6/7semA30r/mIta9V02j+eEkfVm7dx5jJc3UkISJ1VjALxAKgq5l1MrNIyorAEVcjmdmJQHNgbrm25mYW5X/eEhgGBOzcDkWjerXhqTH9WL/zAOc89Q3PfJFOUYmOJkSkbglagXDOFQMTgJnAKmCac26lmT1iZheUW3UsMNUdejlVdyDVzJYCXwB/qujqp1B1Yb8kZt19Omf1aM1fZ67m4knfkr2/wOtYIiI/0p3UdcC/l2/jnmlLSWoew5QbhtCqSbTXkUSkgdCd1HXc6N6JvDx+EFv35OlmOhGpM1Qg6oiTO7fglfGD2bE3nzGT57Eic++Pc00E4pxjc85BPly6lfXZubWYVEQaCp1iqmNSN+7iVy8vILegGDNoGxdDxxaNaBodQaNIHzGRPvYcLGLBxl1k+fsswgwu6NuW28/sSpeEKt98LiJS6SkmFYg6KHNPHqkbd7Fh5wE27jzApl0HOVBQzMHCEvIKS4iJ9JHSsTkpyfH0Sorj3yu28a85mygoLuHcPm25IqUdQzu3INx36AFibkExG7IPsCGnbL8HC0sY2qUFQzrFEx3h8+inFREvqUA0ADm5BUz+ej1vzNtMbkExLRtHcm7vRBKbxbAicy8rt+5jw84Dh2wTHmYUlzqiwsMY0rkFt5zehaFdWnj0E4iIF1QgGpD8ohJmr87i/SVb+fz7LAqLS0lqFkPvpDh6JTXlhFaNSW4ZS8f4WADmbcjhqzXZzFyxnb15Rbw/YRgntGri8U8hIrVFBaKBOlBQTGFxKc1jI4+67ra9eZw/8RuaxkTw/m3DaBIdUQsJRcRrusy1gYqNCq9ScYCy+SuevmoAm3IO8utpSyktrR9fHESk+lQg5Ecnd27Bg+d057O0HUyane51HBHxWLjXAaRuGT8smWUZe/jbrDUsz9zLlYPac1rXhCOuiBKR+k8FQg5hZvzxkj60jotmemoGM1fuoHXTKCaccQLXDE32Op6I1CJ9LZQjxET6eGB0d+Y+cCbPXT2A9s0b8dAHK4+4TFZE6jcVCKlQZHgYo3olMunqAYT7wpj81XqvI4lILVKBkKNq1SSaywe2452FGWRpIEGRBkMFQqrkxtM6U1xayovfbvA6iojUEhUIqZKOLWI5t09b3pi3mb15RV7HEZFaoAIhVXbz6Z3JLSjm9XmbfmwrKC5R57VIPaXLXKXKeraN4/RuCbz87QaGdIrng6Vb+WDpVvYcLOK353bn+uGdvY4oIjVIBUKOyS0jujBm8jwue24uUeFhnN2zDXmFJfzh41XkFZZw+5ldvY4oIjVEBUKOyZBO8Tww+iSaxkRwbp9EmkZHUFxSym+mL+Nvs9aQV1TCfT8/ETPzOqqIHCcVCDkmZsZNp3c5pC3cF8bjl/clJtLHpNnryN5fwIPndqdZo6oNFCgidZMKhNSIsDDjDxf1onmjSCbNTmfWqh3ce/aJjB3cAV+YjiZEQpGuYpIaY2bc+/MT+fiO4ZzUpgm/nbGC8yZ+Q+aePK+jiUg1qEBIjeue2JQ3bziZSb8YwLrsXJ7/cp3XkUSkGlQgJCjMjHN6JzK6VxtmLM4kv6jE60gicoxUICSorkxpz778Ymau3O51FBE5RioQElQnd25B+/gY3lqwxesoInKMVCAkqMLCjCsGtmfOuhw25xz0Oo6IHAMVCAm6y1LaEWbw9kIdRYiEEhUICbrEuBhO65bA9IUZlJQ6r+OISBWpQEituDKlPdv25vPV2myvo4hIFalASK04s3trWsRGMk2d1SIhQwVCakVkeBgX90/iP6t2sDO3wOs4IlIFKhBSa8YMbk9RieOdhRleRxGRKlCBkFpzQqsmDEpuztQFW3BOndUidZ0KhNSqsYM7sGHnAeat3+V1FBE5ChUIqVXn9E6kaXQ4Uxds9jqKiByFCoTUqugIHxf3T+LfK7az+0Ch13FEpBIqEFLrxg7pQGFxKe8uzvQ6iohUIqgFwsxGmdlqM0s3s/sDLH/CzJb4H2vMbE+5ZePMbK3/MS6YOaV2ndSmKf3aN2Pq/M3qrBapw4JWIMzMBzwDjAZ6AGPNrEf5dZxzdzvn+jnn+gETgXf928YDDwFDgMHAQ2bWPFhZpfZdNbgDa7NyWbhpt9dRRKQCwTyCGAykO+fWO+cKganAhZWsPxZ40//858As59wu59xuYBYwKohZpZad1zeRJlHhPPJRmm6cE6mjglkgkoDy4ypk+NuOYGYdgU7Af49lWzO70cxSzSw1O1tj/ISSRpHh/P3KfqzZsZ+LJ31LetZ+ryOJyGGCWSAsQFtFJ5zHANOdcz/MS1mlbZ1zk51zKc65lISEhGrGFK+c1aM1b904lLzCUi6eNIc56Tu9jiQi5QSzQGQA7cu9bgdsrWDdMfx0eulYt5UQ1rd9M2bcdgqJcdFc89J8rnx+LpNmp7Micy+lGhpcxFMWrKtIzCwcWAOcCWQCC4CrnHMrD1vvRGAm0Mn5w/g7qRcCA/yrLQIGOucqvP02JSXFpaam1vjPIbVjX34Rz81exxers1m1bR8AJ7VpwsvjB5EYF+NxOpH6y8wWOudSAi0L2hGEc64YmEDZH/9VwDTn3Eoze8TMLii36lhgqitXqfyF4FHKisoC4JHKioOEvqbREfxm1En8+87hzH/wTP58aW8yd+dx2bNzWZ+d63U8kQYpaEcQtU1HEPXPisy9jHtpPmbw6rWD6dk2zutIIvWOJ0cQIserV1Ic024eSqQvjDHPz2NZxp6jbyQiNUYFQuq0LgmNmX7LKTSK8vHIh2m681qkFqlASJ3XtlkME844gdRNu5mzLsfrOCINhgqEhIQrBrWnTdNonvrPWh1FiNQSFQgJCVHhPm4Z0YX5G3dpsiGRWqICISHjykHtadUkiqc+X+N1FJEGQQVCQkZ0hI+bT+/CvPW7+G69+iJEgk0FQkLKVUM6kNAkiifVFyESdCoQElKiI3zcOqILc9fncMXzc1m5da/XkUTqLRUICTnjhibzp0t6sy77AOdP/IbfzljOnoOa31qkpqlASMgJCzPGDO7AF78ewS+HJvPm/C2c89TXPw7yJyI1QwVCQlZcowgevqAnM24dRqmDy56dw3+/3+F1LJF6QwVCQl7vdnHMuG0YnRJiuf7VVF76ZoM6sEVqgAqE1Att4qKZdtNQRnZvzSMfpfG791dQXFLqdSyRkKYCIfVGo8hwnrt6IDed1pnX521m/CsL2Jdf5HUskZClAiH1SliY8cA53fnzpb2Zuy6HSyfNYXPOQa9jiYQkFQipl64c1IHXrhtC1v4CLnl2Dlv35HkdSSTkqEBIvTW0SwvevnkoeYXF3PrGIgqKS7yOJBJSVCCkXuvWugmPX96XJVv28OhHaV7HEQkpKhBS743unchNp5d1XE9fmOF1HJGQoQIhDcJ9Z5/I0M4tePC95azI1PhNIlVRpQJhZl3MLMr/fISZ3WFmzYIbTaTmhPvCmHhVf+JjI7n2lQW6skmkCqp6BPEOUGJmJwAvAp2AKUFLJRIELRtH8eq1gyksKeWal74ja3++15FE6rSqFohS51wxcDHwpHPubiAxeLFEgqNb6ya8/KtBZO8vYNxLC9ibpxvpRCpS1QJRZGZjgXHAR/62iOBEEgmu/h2a8/w1A0nP2s91rywgv0iXv4oEUtUCMR4YCjzmnNtgZp2A14MXSyS4hndN4Ikr+5G6aTcvfL3e6zgidVKVCoRzLs05d4dz7k0zaw40cc79KcjZRILqvD5t+XnP1kyavY6sfeqPEDlcVa9imm1mTc0sHlgKvGxmfw9uNJHge2B0d4pKSvnbZ2u8jiJS51T1FFOcc24fcAnwsnNuIDAyeLFEakdyy1jGDU1m2sItmt9a5DBVLRDhZpYIXMFPndQi9cLtZ3alWUwEj328ShMNiZRT1QLxCDATWOecW2BmnYG1wYslUnviYiK4a2Q35qzL4fNVWV7HEakzqtpJ/bZzro9z7hb/6/XOuUuDG02k9lw1pANdEmJ55KM0DhQUex1HpE6oaid1OzN7z8yyzGyHmb1jZu2CHU6ktkT4wvjjJX3YsvugRn0V8avqKaaXgQ+AtkAS8KG/TaTeGNwpnltO78LUBVv4dMV2r+OIeK6qBSLBOfeyc67Y/3gFSAhiLhFP3DWyG72T4njg3WXs0L0R0sBVtUDsNLOrzcznf1wN5AQzmIgXIsPDeHJMP/KKSrj37aVszjnI+0syefiDldwzbQnb9mrqUmk4rCqX9ZlZB+BpyobbcMAc4A7n3Obgxqu6lJQUl5qa6nUMqSden7eJ385Y8ePrmAgfAI2jw/nnL1Po116j3Uv9YGYLnXMpgZaFV2UH/kJwwWE7vQt48vjjidQ9vxjSgaKSUiJ8YQzo0JxurRuzLvsA1726gCufn8vjl/fl/L5tvY4pElRVOoIIuKHZZudchxrOU206gpDakJNbwM2vL2TBxt3cMqILd4/sRmS4JmaU0FXZEcTxfLKtCm88ysxWm1m6md1fwTpXmFmama00synl2kvMbIn/8cFx5BSpMS0aR/H69UMYM6g9z85ex8WTvmXNjv1exxIJiuMpEJUeepiZD3gGGA30AMaaWY/D1ukKPAAMc871BO4qtzjPOdfP/zjk9JaIl6LCffzp0j48d/VAtu/N57yJ3/DPr9ZTWqphOqR+qbQPwsz2E7gQGBBzlH0PBtKdc+v9+5oKXAiUvwvpBuAZ59xuAOecxjmQkDGqVxtSkpvzwLvLeeyTVYSFGded2snrWCI1ptIjCOdcE+dc0wCPJs65o3VwJwFbyr3O8LeV1w3oZmbfmtk8MxtVblm0maX62y8K9AZmdqN/ndTs7OyjxBGpeS0bRzH5moEM79qSf3y+lr0HNYWp1B/B7F0L1Edx+NFIONAVGAGMBV4wsx+uH+zg7zi5CnjSzLocsTPnJjvnUpxzKQkJum9PvGFmPDC6O/vyi5g0O93rOCI1JpgFIgNoX+51O2BrgHXed84VOec2AKspKxg457b6/7semA30D2JWkePSo21TLunfjpfnbCRj90Gv44jUiGAWiAVAVzPrZGaRwBjKxnMqbwZwBoCZtaTslNN6M2tuZlHl2odxaN+FSJ1z78+7YcDjM1d7HUWkRgStQDjnioEJlM0jsQqY5pxbaWaPmNkPVyXNBHLMLA34ArjPOZcDdAdSzWypv/1PzjkVCKnTEuNiuO7UTsxYspUVmZqdTkJftW+Uq2t0o5zUBfvyixjx19l0SYjlX9cOISbS53UkkUoF60Y5ETlM0+gI/uec7qRu2s1Fz3xLelau15FEqk0FQqSGXTawHa+MH0x2bgEXPP0N7y3O8DqSSLWoQIgEwendEvjkjuH0Sorj7reW8pdPv/c6ksgxU4EQCZI2cdFMuX4IYwe3Z9LsdUz5rs6Mji9SJVUa7ltEqifcF8ajF/Zi2958fvf+CtrHxzC8q27qlNCgIwiRIAv3hTFxbH+6tmrMra8v0uivEjJUIERqQZPoCF781SCiI32Mf3kBX67Jpr5cYi71lwqESC1JahbDS+MGUVxayriX5jP6qa+ZvjCDwuJSr6OJBKQCIVKLereL4+vf/IzHL+8LwL1vL+XSZ+eoSEidpAIhUssiw8O4bGA7/n3ncB6/vC/LM/fy3JfrvI4lcgQVCBGPmBmXDWzH+X3bMvG/a1mrzmupY1QgRDz20Pk9iI0K5/+9s4wSTVsqdYgKhIjHWjaO4qHze7Bo8x5em7vR6zgiP1KBEKkDLuqXxIgTE/jLzNWsz9YAf1I3qECI1AFmxh8u6oUvzDjnH18z8fO15BeVeB1LGjgVCJE6ol3zRnx612n87KRW/G3WGs5+4is+X7XD61jSgKlAiNQhSc1imPSLgbxx/RCiwsO47tVU3vhuk9expIFSgRCpg4ad0JKP7xjOz05qxW9nrOD9JZleR5IGSAVCpI6KDA9j0i8GMKRTPPdMW8pnK7d7HUkaGBUIkTosOsLHC+MG0SspjglTFvPC1+v5dMV25m/YxYadBzTgnwSV5oMQqeMaR4Xz6vhB/OKF7/jDx6sOWdY7KY6bTu/M6F6J+MLMo4RSX1l9+QaSkpLiUlNTvY4hEjQlpY6s/fnk5Bay+2Ah67MP8OqcjazfeYAO8Y2488yuXDqwndcxJcSY2ULnXEqgZTqCEAkRvjAjMS6GxLgYAIZ3TeDqkzsyK207k2av49dvLyUm0sc5vRM9Tir1hfogREKYL8wY1SuR6TefQr/2zbjv7aWs053YUkNUIETqgR+ueIqK8HHL6ws5WFjsdSSpB1QgROqJts1ieGpMP9Zm5fLgeyt0hZMcNxUIkXpkeNcE7hnZjfcWZ/LaPN2BLcdHBUKknrntjBM486RW/P7DNOas2+l1HAlhKhAi9UxYmPHkmH50bhnLrW8sYlPOAa8jSYhSgRCph5pER/DCuLJL269/NZX9+UUeJ5JQpAIhUk91bBHLpKsGsH7nAe6cuoTiklKvI0mIUYEQqcdOOaElD1/Qk/9+n8VNr+nyVzk2KhAi9dw1J3fk0Qt78sXqLK58fh5Z+/O9jiQhQgVCpAG4Zmgyk69JIT0rl0smzSE9a7/XkSQEqECINBAje7TmrZtOJr+ohIufmaP5JeSoVCBEGpA+7Zox47ZhdEqI5cbXFvLnT79X57VUSKO5ijQw7Zo3YtpNQ/n9h2k8O3sdS7fsYXCneDbvOkjGrjwOFhUzsntrLujbls4Jjb2OKx7SfBAiDdi01C38bsYKCktKadM0mvbxjXDOkbppN86VTUh02xldGNVLQ4jXV5XNB6ECIdLAHSwsxhdmRIX7fmzbvjefj5ZtZVrqFtZm5fLYRb25akgHD1NKsFRWIILaB2Fmo8xstZmlm9n9FaxzhZmlmdlKM5tSrn2cma31P8YFM6dIQ9YoMvyQ4gDQJi6a64d35oMJpzKiWwL/895yXvxmg0cJxStB64MwMx/wDHAWkAEsMLMPnHNp5dbpCjwADHPO7TazVv72eOAhIAVwwEL/truDlVdEjhQd4eP5a1K4c+piHv0ojfyiEm474wSvY0ktCeYRxGAg3Tm33jlXCEwFLjxsnRuAZ374w++cy/K3/xyY5Zzb5V82CxgVxKwiUoHI8DAmju3PRf3a8teZq7ns2bJLZEtL68fpaalYMAtEErCl3OsMf1t53YBuZvatmc0zs1HHsC1mdqOZpZpZanZ2dg1GF5Hywn1h/O2Kfvz+gp5s35fPja8tZOTfv+T9JZleR5MgCmaBsABth3/lCAe6AiOAscALZtasitvinJvsnEtxzqUkJCQcZ1wRqYwvzBh3SjKz7x3BxLH9iYn0cefUJcxJ15wT9VUwC0QG0L7c63bA1gDrvO+cK3LObQBWU1YwqrKtiHgg3BfG+X3bMv3mU+jcMpb7pi9jn4YTr5eCWSAWAF3NrJOZRQJjgA8OW2cGcAaAmbWk7JTTemAmcLaZNTez5sDZ/jYRqSNiIn08fkVftu3N49EP046+gYScoBUI51wxMIGyP+yrgGnOuZVm9oiZXeBfbSaQY2ZpwBfAfc65HOfcLuBRyorMAuARf5uI1CEDOjTnlhFdeHthBv9J2+F1HKlhulFORI5LQXEJFz79LTtzC/ns7tOIj430OpIcA89ulBOR+i8q3Mffr+jH3rxC7n5LM9fVJyoQInLcerRtyu8v6MWXa7J55KM0qntmwjnHb6Yv5Y43F7Mzt6CGU8qx0miuIlIjrhrSgQ07c/nn1xvo1DKW8cM6HfM+PkvbwbTUDAC+Sd/JYxf1YnRvDRToFR1BiEiNuX90d87u0ZpHP0rj81XH1mmdX1TCox+lcWLrJnxyx3DaNovmljcWccebi9mvy2g9oQIhIjXGF2Y8OaYfPdvGcfubi9my62CVt33+y/Vk7M7j4Qt60qNtU967dRh3j+zGx8u3cfWL89l7UEWitqlAiEiNahQZzvPXDKSk1PGPz9dWaZstuw4yaXY65/ZJZGiXFgBE+MK4c2RXnv3FAFZt3cfYf84jR/0StUoFQkRqXNtmMVx9ckfeXZzJhp0Hjrr+Yx+vIsyMB8/pfsSys3u2YfIvB7IuO5cxk+eRtS8/GJElABUIEQmKm0/vQoTPmHiUo4hv03fy6crt3HZGF9o2iwm4zogTW/Hy+EFk7snjsufm8v32fcGILIdRgRCRoEhoEsW4ocnMWJJJelZuwHWcc/x15mqSmsVw/fDOle7vlC4teeP6IeQXlXDxM3OOGEl22948Pl+1Q8OQ1yAVCBEJmhtP60x0hK/CvojZq7NZsmUPt//sBKIjfAHXKa9/h+Z8dMep9E6K486pS3j4g5W88d0mrnx+Lqf86b9c92oqT3+RXtM/RoOlAiEiQdOicRTjTknmw2VbWbNj/yHLnHP8fdYa2sfHcOnAdlXeZ6sm0bxxwxDGD0vmlTkbefC9FezMLeCuM7txbp9EnvjPGr5YnXX0HclR6UY5EQmqG4d35l9zNvLoR2m8MC7lx/mvZ6XtYHnmXv56WR8ifMf2XTXCF8ZD5/fk/L5tiQoPo0diU8yMvMIS1mcf4K6pS/hwwql0aNHomPMWFJfw8bJtnNWjNU2iI455+/pERxAiElTNYyO5f/RJfL12J+NeKrufobTU8cR/1pLcohEX9z9issgqG9ChOT3bxmFWNsdYTKSP564egHOOm19fSF5hyTHtr7C4lNveWMQ905byq5cXkFtQXO1s9YEKhIgE3TVDk3nyyn4s2rSHS579lhe/2cCqbfu4c2RXwo/x6OFoOraI5akx/Vm1fR93vbWYA1X8I19UUsqEKYv4z6osrkxpz5Itexj/8vwqb18fqUCISK24qH8S/7puMDtzC3nsk1V0Tojlgr7VP3qozBknteK35/ZgVtoOzpv4DSsy91a6fnFJKXdOXcxnaTv4/QU9+fNlfXhqTD8Wbd7D+FcWcLCwmKKSUjbsPMCXa7LZvrdh3Iuh+SBEpFalZ+Xy2xnLuXXECZzWLbhzyc9bn8NdU5eQc6CA+0d359phyT+ejvqBc45fT1vKu4sz+d15Pbju1J8GGXx/SSZ3v7WEuJgI9ucXU+y/hNYXZozu1Ybxw5IZ0KH5EfsMJZXNB6ECISL12u4DhfzmnWXMStvB+GHJPHR+z0OWv/HdJh58bwV3j+zGnSO7HrH9zJXb+XjZNtrHx5DcIpakZjF8sTqLqQu2sD+/mL7tm/H02P60jz/2DvG6QAVCRBo05xy//zCNV+Zs5NELe3LN0GQAvt++jwuf/pbBneJ5dfxgwsKqfiRwoKCYdxdl8Phna2gcFc7UG08OySKhGeVEpEEzM353Xg/OPKkVD3+YxuzVWRwsLGbClMU0jYng71f0O6biABAbFc41Q5N54/oh5BYUc+Xzc9mUc/Rxp0KJCoSINAi+MOOpsf3p1roJE6Ys5vYpi1mXncsTV/QjoUlUtffbK4WzbtMAAAqdSURBVCmOKTcMIa+ohDGT57GxCoMThgoVCBFpMBpHhfPiuBQaRfr4/Pssbh3RhVO7tjzu/fZsG8eUG06moLiUX7zwXb2ZLlUFQkQalLbNYvjXdYO556xu3D2yW43tt3tiU14dP5icAwXc8vpCCotLa2zfXlGBEJEG56Q2TbnjzJq/Sa93uzgev7wvCzbu5rczlhPqFwFpLCYRkRp0Xp+2rNm+n3/8N50T2zQ95L6KUKMCISJSw+4a2Y01O3J57OM0mkaHc3lKe68jVYtOMYmI1LCwMOPvV/ZlSKcW3Dd9Gfe9vZSDhaE3ppMKhIhIEDSKDOf164dwx5ldmb4ogwuf/vaIOTHqOp1iEhEJEl+Ycc9Z3RicHM9dby3m7Ce+olvrxgxKjmdQcjxnnNSKuJijzzmxdU8e0RE+4mMjayH1TzTUhohILcjan8/bqRnM37CLhZt2k1tQTPv4GF67dgjJLWMDbrN4824mzV7HrLQdACS3aET/Ds0Z3Cmeywa2O+aJlgLRWEwiInVISalj3vocJkxZhC/MeGX8YHolxQFl40Z9tXYnz3+5jjnrcoiLieCXQzsSGxXO4s27WbR5D9n7C+jTLo6nxvSnUwXFpapUIERE6qD0rFx++eJ37M8v5h9X9Sdzdx6vzNlIelYurZpEccPwzowd0oHGUT/1Bjjn+HTFdu5/dzlFJaU8fH5PLk9pV+0hx1UgRETqqG1787jmxfmkZ+UC0DspjmtPTebc3m2JDK/4FNK2vXn8etpS5qzL4dzeiUwc2/+YBxyEyguEOqlFRDyUGBfD9JuH8tI3GzitWwIDO1ZtAqLEuBhev24I//x6Pfvzi6tVHI5GRxAiIg2Y5oMQEZFjpgIhIiIBqUCIiEhAKhAiIhKQCoSIiASkAiEiIgGpQIiISEAqECIiElC9uVHOzLKBPcDeAIvjDmuv7PUPzwO1tQR2HmO0w9+rqsurk7n88+PJXFmuypYfra0uZg7Urs/H0TWUz0coZg7UXtnrrs65uIB7d87VmwcwuSrtlb3+4XkFbak1lSkYmQPlr07m6uY+WltdzKzPhz4f9S3z8Xw+Dn/Ut1NMH1axvbLXH1bSVpOZjra8OpnLPz+ezFXZPtDyo7XVxcyB2vX5OLqG8vkIxcyB2qv6+ThEvTnFVBvMLNVVMGZJXaXMtScUcytz7QjFzKBO6mM12esA1aDMtScUcytz7QjFzDqCEBGRwHQEISIiAalAiIhIQA2yQJjZS2aWZWYrqrHtQDNbbmbpZvYPKzf1k5ndbmarzWylmf2lZlMHJ7eZPWxmmWa2xP84p65nLrf8XjNzZtay5hIH7ff8qJkt8/+OPzOztjWZOYi5/2pm3/uzv2dmzUIg8+X+f4OlZlZjHcPHk7WC/Y0zs7X+x7hy7ZV+7mtVda7NDfUHcBowAFhRjW3nA0MBA/4NjPa3nwH8B4jyv24VIrkfBu4Npd+1f1l7YCawCWhZ1zMDTcutcwfwXCj8roGzgXD/8z8Dfw6BzN2BE4HZQIrXWf05kg9riwfW+//b3P+8eWU/lxePBnkE4Zz7CthVvs3MupjZp2a20My+NrOTDt/OzBIp+4c+15X9n/wXcJF/8S3An5xzBf73yAqR3EEVxMxPAL8Bavwqi2Bkds7tK7dqbAjl/sw5V+xfdR7QLgQyr3LOra7JnMeTtQI/B2Y553Y553YDs4BRXv5bDaRBFogKTAZud84NBO4FJgVYJwnIKPc6w98G0A0YbmbfmdmXZjYoqGl/cry5ASb4TyG8ZGbNgxf1R8eV2cwuADKdc0uDHbSc4/49m9ljZrYF+AXwv0HMWl5NfD5+cC1l32iDrSYzB1tVsgaSBGwp9/qH/HXl5wIg3Ks3rkvMrDFwCvB2udN9UYFWDdD2wzfBcMoOFU8GBgHTzKyz/1tAUNRQ7meBR/2vHwX+RtkfgqA43sxm1gh4kLJTH7Wihn7POOceBB40sweACcBDNRz10DA1lNu/rweBYuCNmsx4RJAazBxslWU1s/HAnf62E4BPzKwQ2OCcu5iK83v+c5WnAlEmDNjjnOtXvtHMfMBC/8sPKPtjWv4Qux2w1f88A3jXXxDmm1kpZQN0Zdfl3M65HeW2+yfwURDzwvFn7gJ0Apb6/1G2AxaZ2WDn3PY6mvlwU4CPCXKBoIZy+ztQzwPODOYXHr+a/l0HU8CsAM65l4GXAcxsNvAr59zGcqtkACPKvW5HWV9FBt7/XD/xqvPD6weQTLnOJmAOcLn/uQF9K9huAWVHCT90IJ3jb78ZeMT/vBtlh48WArkTy61zNzC1rmc+bJ2N1HAndZB+z13LrXM7MD1EPtejgDQgIRh5g/n5oIY7qaublYo7qTdQdtahuf95fFU/97X18ORNvX4AbwLbgCLKKvZ1lH0r/RRY6v8H8b8VbJsCrADWAU/z093okcDr/mWLgJ+FSO7XgOXAMsq+mSXW9cyHrbORmr+KKRi/53f87csoGxwtKUQ+H+mUfdlZ4n/U6NVXQcp8sX9fBcAOYKaXWQlQIPzt1/p/v+nA+GP53NfWQ0NtiIhIQLqKSUREAlKBEBGRgFQgREQkIBUIEREJSAVCREQCUoGQes3Mcmv5/V4wsx41tK8SKxv9dYWZfXi0kVTNrJmZ3VoT7y0CmlFO6jkzy3XONa7B/YW7nwavC6ry2c3sVWCNc+6xStZPBj5yzvWqjXxS/+kIQhocM0sws3fMbIH/MczfPtjM5pjZYv9/T/S3/8rM3jazD4HPzGyEmc02s+lWNlfCGz+M2e9vT/E/z/UP0LfUzOaZWWt/exf/6wVm9kgVj3Lm8tNghY3N7HMzW2Rl8wZc6F/nT0AX/1HHX/3r3ud/n2Vm9vsa/DVKA6ACIQ3RU8ATzrlBwKXAC/7274HTnHP9KRtt9f/KbTMUGOec+5n/dX/gLqAH0BkYFuB9YoF5zrm+wFfADeXe/yn/+x91nB3/OERnUnanO0A+cLFzbgBl85D8zV+g7gfWOef6OefuM7Ozga7AYKAfMNDMTjva+4n8QIP1SUM0EuhRbgTOpmbWBIgDXjWzrpSNoBlRbptZzrnycwHMd85lAJjZEsrG6PnmsPcp5KfBDxcCZ/mfD+WnMf6nAI9XkDOm3L4XUjZnAJSN0fN//j/2pZQdWbQOsP3Z/sdi/+vGlBWMryp4P5FDqEBIQxQGDHXO5ZVvNLOJwBfOuYv95/Nnl1t84LB9FJR7XkLgf0tF7qdOvorWqUyec66fmcVRVmhuA/5B2XwSCcBA51yRmW0EogNsb8AfnXPPH+P7igA6xSQN02eUzccAgJn9MFxzHJDpf/6rIL7/PMpObQGMOdrKzrm9lE1Teq+ZRVCWM8tfHM4AOvpX3Q80KbfpTOBa/7wFmFmSmbWqoZ9BGgAVCKnvGplZRrnHPZT9sU3xd9ymUTZUO8BfgD+a2beAL4iZ7gLuMbP5QCKw92gbOOcWUzZi6BjKJu1JMbNUyo4mvvevkwN8678s9q/Ouc8oO4U118yWA9M5tICIVEqXuYrUMv+seHnOOWdmY4CxzrkLj7adSG1TH4RI7RsIPO2/8mgPQZziVeR46AhCREQCUh+EiIgEpAIhIiIBqUCIiEhAKhAiIhKQCoSIiAT0/wE1pIPR7KDGkAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "learn = getlearn()\n",
    "learn.opt_func = optf_adam\n",
    "learn.lr_find()\n",
    "learn.recorder.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ALLERT: You are using CumtomEpochLength, please make sure that your training dataloader is using random sampler, or this may cause problem.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "                background: #F44336;\n",
       "            }\n",
       "        </style>\n",
       "      <progress value='257' class='' max='500', style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      51.40% [257/500 17:12:16<16:16:02]\n",
       "    </div>\n",
       "    \n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>dice_loss</th>\n",
       "      <th>balance_bce</th>\n",
       "      <th>mask_iou</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.343741</td>\n",
       "      <td>0.275967</td>\n",
       "      <td>0.275967</td>\n",
       "      <td>0.832594</td>\n",
       "      <td>0.578787</td>\n",
       "      <td>04:02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.190607</td>\n",
       "      <td>0.110784</td>\n",
       "      <td>0.110784</td>\n",
       "      <td>0.578189</td>\n",
       "      <td>0.803420</td>\n",
       "      <td>04:02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.118330</td>\n",
       "      <td>0.097209</td>\n",
       "      <td>0.097209</td>\n",
       "      <td>0.600622</td>\n",
       "      <td>0.824674</td>\n",
       "      <td>04:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.089307</td>\n",
       "      <td>0.072525</td>\n",
       "      <td>0.072525</td>\n",
       "      <td>0.420103</td>\n",
       "      <td>0.866379</td>\n",
       "      <td>04:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.075614</td>\n",
       "      <td>0.068155</td>\n",
       "      <td>0.068155</td>\n",
       "      <td>0.291228</td>\n",
       "      <td>0.873257</td>\n",
       "      <td>04:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.068708</td>\n",
       "      <td>0.084531</td>\n",
       "      <td>0.084531</td>\n",
       "      <td>0.352046</td>\n",
       "      <td>0.845168</td>\n",
       "      <td>04:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.063752</td>\n",
       "      <td>0.056669</td>\n",
       "      <td>0.056669</td>\n",
       "      <td>0.503608</td>\n",
       "      <td>0.894085</td>\n",
       "      <td>03:59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.056909</td>\n",
       "      <td>0.052714</td>\n",
       "      <td>0.052714</td>\n",
       "      <td>0.460271</td>\n",
       "      <td>0.900575</td>\n",
       "      <td>04:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.052214</td>\n",
       "      <td>0.048201</td>\n",
       "      <td>0.048201</td>\n",
       "      <td>0.344029</td>\n",
       "      <td>0.908381</td>\n",
       "      <td>04:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.052278</td>\n",
       "      <td>0.073633</td>\n",
       "      <td>0.073633</td>\n",
       "      <td>0.760601</td>\n",
       "      <td>0.863644</td>\n",
       "      <td>04:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.047404</td>\n",
       "      <td>0.043686</td>\n",
       "      <td>0.043686</td>\n",
       "      <td>0.287762</td>\n",
       "      <td>0.916625</td>\n",
       "      <td>03:59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.040895</td>\n",
       "      <td>0.038279</td>\n",
       "      <td>0.038279</td>\n",
       "      <td>0.255462</td>\n",
       "      <td>0.926513</td>\n",
       "      <td>04:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.037862</td>\n",
       "      <td>0.036420</td>\n",
       "      <td>0.036420</td>\n",
       "      <td>0.262052</td>\n",
       "      <td>0.930103</td>\n",
       "      <td>04:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.045585</td>\n",
       "      <td>0.068358</td>\n",
       "      <td>0.068358</td>\n",
       "      <td>0.573172</td>\n",
       "      <td>0.873393</td>\n",
       "      <td>04:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.038577</td>\n",
       "      <td>0.039030</td>\n",
       "      <td>0.039030</td>\n",
       "      <td>0.255920</td>\n",
       "      <td>0.925188</td>\n",
       "      <td>04:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.033878</td>\n",
       "      <td>0.031564</td>\n",
       "      <td>0.031564</td>\n",
       "      <td>0.231204</td>\n",
       "      <td>0.938951</td>\n",
       "      <td>04:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.031116</td>\n",
       "      <td>0.040311</td>\n",
       "      <td>0.040311</td>\n",
       "      <td>0.389788</td>\n",
       "      <td>0.922623</td>\n",
       "      <td>04:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.036826</td>\n",
       "      <td>0.035153</td>\n",
       "      <td>0.035153</td>\n",
       "      <td>0.288007</td>\n",
       "      <td>0.932450</td>\n",
       "      <td>03:59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.034259</td>\n",
       "      <td>0.031743</td>\n",
       "      <td>0.031743</td>\n",
       "      <td>0.283934</td>\n",
       "      <td>0.938639</td>\n",
       "      <td>04:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.030508</td>\n",
       "      <td>0.030112</td>\n",
       "      <td>0.030112</td>\n",
       "      <td>0.297948</td>\n",
       "      <td>0.941747</td>\n",
       "      <td>04:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.028243</td>\n",
       "      <td>0.034286</td>\n",
       "      <td>0.034286</td>\n",
       "      <td>0.365875</td>\n",
       "      <td>0.933844</td>\n",
       "      <td>04:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>0.026988</td>\n",
       "      <td>0.030966</td>\n",
       "      <td>0.030966</td>\n",
       "      <td>0.315530</td>\n",
       "      <td>0.940096</td>\n",
       "      <td>04:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>0.027478</td>\n",
       "      <td>0.031011</td>\n",
       "      <td>0.031011</td>\n",
       "      <td>0.316249</td>\n",
       "      <td>0.940039</td>\n",
       "      <td>04:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>0.024298</td>\n",
       "      <td>0.028208</td>\n",
       "      <td>0.028208</td>\n",
       "      <td>0.306887</td>\n",
       "      <td>0.945283</td>\n",
       "      <td>04:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>0.024679</td>\n",
       "      <td>0.023979</td>\n",
       "      <td>0.023979</td>\n",
       "      <td>0.202549</td>\n",
       "      <td>0.953260</td>\n",
       "      <td>03:59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>0.024224</td>\n",
       "      <td>0.037464</td>\n",
       "      <td>0.037464</td>\n",
       "      <td>0.490222</td>\n",
       "      <td>0.927946</td>\n",
       "      <td>04:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>0.031468</td>\n",
       "      <td>0.050626</td>\n",
       "      <td>0.050626</td>\n",
       "      <td>0.597856</td>\n",
       "      <td>0.904259</td>\n",
       "      <td>04:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>0.033378</td>\n",
       "      <td>0.026834</td>\n",
       "      <td>0.026834</td>\n",
       "      <td>0.298083</td>\n",
       "      <td>0.947881</td>\n",
       "      <td>04:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28</td>\n",
       "      <td>0.023853</td>\n",
       "      <td>0.022845</td>\n",
       "      <td>0.022845</td>\n",
       "      <td>0.211959</td>\n",
       "      <td>0.955412</td>\n",
       "      <td>04:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29</td>\n",
       "      <td>0.020188</td>\n",
       "      <td>0.022132</td>\n",
       "      <td>0.022132</td>\n",
       "      <td>0.205804</td>\n",
       "      <td>0.956786</td>\n",
       "      <td>04:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>0.020869</td>\n",
       "      <td>0.022525</td>\n",
       "      <td>0.022525</td>\n",
       "      <td>0.264367</td>\n",
       "      <td>0.956032</td>\n",
       "      <td>04:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31</td>\n",
       "      <td>0.021268</td>\n",
       "      <td>0.030059</td>\n",
       "      <td>0.030059</td>\n",
       "      <td>0.276948</td>\n",
       "      <td>0.941749</td>\n",
       "      <td>04:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32</td>\n",
       "      <td>0.025120</td>\n",
       "      <td>0.028627</td>\n",
       "      <td>0.028627</td>\n",
       "      <td>0.293158</td>\n",
       "      <td>0.944467</td>\n",
       "      <td>04:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33</td>\n",
       "      <td>0.021288</td>\n",
       "      <td>0.022988</td>\n",
       "      <td>0.022988</td>\n",
       "      <td>0.330145</td>\n",
       "      <td>0.955149</td>\n",
       "      <td>04:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34</td>\n",
       "      <td>0.021222</td>\n",
       "      <td>0.022349</td>\n",
       "      <td>0.022349</td>\n",
       "      <td>0.242171</td>\n",
       "      <td>0.956355</td>\n",
       "      <td>04:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35</td>\n",
       "      <td>0.018099</td>\n",
       "      <td>0.019942</td>\n",
       "      <td>0.019942</td>\n",
       "      <td>0.226031</td>\n",
       "      <td>0.960959</td>\n",
       "      <td>04:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36</td>\n",
       "      <td>0.016873</td>\n",
       "      <td>0.019419</td>\n",
       "      <td>0.019419</td>\n",
       "      <td>0.222906</td>\n",
       "      <td>0.961962</td>\n",
       "      <td>04:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>37</td>\n",
       "      <td>0.016547</td>\n",
       "      <td>0.019161</td>\n",
       "      <td>0.019161</td>\n",
       "      <td>0.221215</td>\n",
       "      <td>0.962463</td>\n",
       "      <td>03:59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>38</td>\n",
       "      <td>0.015883</td>\n",
       "      <td>0.018877</td>\n",
       "      <td>0.018877</td>\n",
       "      <td>0.226543</td>\n",
       "      <td>0.963009</td>\n",
       "      <td>04:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>39</td>\n",
       "      <td>0.016090</td>\n",
       "      <td>0.018508</td>\n",
       "      <td>0.018508</td>\n",
       "      <td>0.224970</td>\n",
       "      <td>0.963726</td>\n",
       "      <td>04:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>0.015783</td>\n",
       "      <td>0.018381</td>\n",
       "      <td>0.018381</td>\n",
       "      <td>0.201086</td>\n",
       "      <td>0.963976</td>\n",
       "      <td>04:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>41</td>\n",
       "      <td>0.015386</td>\n",
       "      <td>0.018180</td>\n",
       "      <td>0.018180</td>\n",
       "      <td>0.207051</td>\n",
       "      <td>0.964353</td>\n",
       "      <td>03:59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>42</td>\n",
       "      <td>0.015250</td>\n",
       "      <td>0.017931</td>\n",
       "      <td>0.017931</td>\n",
       "      <td>0.200237</td>\n",
       "      <td>0.964839</td>\n",
       "      <td>04:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>43</td>\n",
       "      <td>0.014726</td>\n",
       "      <td>0.017710</td>\n",
       "      <td>0.017710</td>\n",
       "      <td>0.195754</td>\n",
       "      <td>0.965264</td>\n",
       "      <td>04:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>44</td>\n",
       "      <td>0.014522</td>\n",
       "      <td>0.017635</td>\n",
       "      <td>0.017635</td>\n",
       "      <td>0.196846</td>\n",
       "      <td>0.965414</td>\n",
       "      <td>04:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>45</td>\n",
       "      <td>0.014579</td>\n",
       "      <td>0.017601</td>\n",
       "      <td>0.017601</td>\n",
       "      <td>0.203963</td>\n",
       "      <td>0.965468</td>\n",
       "      <td>03:59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>46</td>\n",
       "      <td>0.014204</td>\n",
       "      <td>0.017314</td>\n",
       "      <td>0.017314</td>\n",
       "      <td>0.190929</td>\n",
       "      <td>0.966035</td>\n",
       "      <td>04:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>47</td>\n",
       "      <td>0.014076</td>\n",
       "      <td>0.017351</td>\n",
       "      <td>0.017351</td>\n",
       "      <td>0.198777</td>\n",
       "      <td>0.965955</td>\n",
       "      <td>04:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>48</td>\n",
       "      <td>0.013908</td>\n",
       "      <td>0.017084</td>\n",
       "      <td>0.017084</td>\n",
       "      <td>0.197471</td>\n",
       "      <td>0.966470</td>\n",
       "      <td>03:59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>49</td>\n",
       "      <td>0.013700</td>\n",
       "      <td>0.016942</td>\n",
       "      <td>0.016942</td>\n",
       "      <td>0.207537</td>\n",
       "      <td>0.966748</td>\n",
       "      <td>03:59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.013658</td>\n",
       "      <td>0.016747</td>\n",
       "      <td>0.016747</td>\n",
       "      <td>0.206124</td>\n",
       "      <td>0.967112</td>\n",
       "      <td>04:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>51</td>\n",
       "      <td>0.013550</td>\n",
       "      <td>0.016607</td>\n",
       "      <td>0.016607</td>\n",
       "      <td>0.190130</td>\n",
       "      <td>0.967395</td>\n",
       "      <td>04:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>52</td>\n",
       "      <td>0.013159</td>\n",
       "      <td>0.016538</td>\n",
       "      <td>0.016538</td>\n",
       "      <td>0.194390</td>\n",
       "      <td>0.967521</td>\n",
       "      <td>04:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>53</td>\n",
       "      <td>0.013198</td>\n",
       "      <td>0.016454</td>\n",
       "      <td>0.016454</td>\n",
       "      <td>0.198728</td>\n",
       "      <td>0.967692</td>\n",
       "      <td>04:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>54</td>\n",
       "      <td>0.013059</td>\n",
       "      <td>0.016494</td>\n",
       "      <td>0.016494</td>\n",
       "      <td>0.191776</td>\n",
       "      <td>0.967617</td>\n",
       "      <td>04:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>55</td>\n",
       "      <td>0.013106</td>\n",
       "      <td>0.016424</td>\n",
       "      <td>0.016424</td>\n",
       "      <td>0.197881</td>\n",
       "      <td>0.967749</td>\n",
       "      <td>04:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>56</td>\n",
       "      <td>0.012899</td>\n",
       "      <td>0.016355</td>\n",
       "      <td>0.016355</td>\n",
       "      <td>0.199533</td>\n",
       "      <td>0.967880</td>\n",
       "      <td>04:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>57</td>\n",
       "      <td>0.012964</td>\n",
       "      <td>0.016268</td>\n",
       "      <td>0.016268</td>\n",
       "      <td>0.213696</td>\n",
       "      <td>0.968044</td>\n",
       "      <td>04:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>58</td>\n",
       "      <td>0.012757</td>\n",
       "      <td>0.015928</td>\n",
       "      <td>0.015928</td>\n",
       "      <td>0.193286</td>\n",
       "      <td>0.968700</td>\n",
       "      <td>04:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>59</td>\n",
       "      <td>0.012373</td>\n",
       "      <td>0.015806</td>\n",
       "      <td>0.015806</td>\n",
       "      <td>0.200037</td>\n",
       "      <td>0.968949</td>\n",
       "      <td>03:59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>0.012221</td>\n",
       "      <td>0.015679</td>\n",
       "      <td>0.015679</td>\n",
       "      <td>0.194915</td>\n",
       "      <td>0.969198</td>\n",
       "      <td>04:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>61</td>\n",
       "      <td>0.012201</td>\n",
       "      <td>0.015495</td>\n",
       "      <td>0.015495</td>\n",
       "      <td>0.203907</td>\n",
       "      <td>0.969539</td>\n",
       "      <td>04:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>62</td>\n",
       "      <td>0.012373</td>\n",
       "      <td>0.015659</td>\n",
       "      <td>0.015659</td>\n",
       "      <td>0.202834</td>\n",
       "      <td>0.969227</td>\n",
       "      <td>04:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>63</td>\n",
       "      <td>0.012165</td>\n",
       "      <td>0.015601</td>\n",
       "      <td>0.015601</td>\n",
       "      <td>0.175091</td>\n",
       "      <td>0.969338</td>\n",
       "      <td>04:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>64</td>\n",
       "      <td>0.011817</td>\n",
       "      <td>0.015378</td>\n",
       "      <td>0.015378</td>\n",
       "      <td>0.171260</td>\n",
       "      <td>0.969791</td>\n",
       "      <td>04:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>65</td>\n",
       "      <td>0.011557</td>\n",
       "      <td>0.015177</td>\n",
       "      <td>0.015177</td>\n",
       "      <td>0.202828</td>\n",
       "      <td>0.970159</td>\n",
       "      <td>04:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>66</td>\n",
       "      <td>0.011527</td>\n",
       "      <td>0.015151</td>\n",
       "      <td>0.015151</td>\n",
       "      <td>0.203382</td>\n",
       "      <td>0.970214</td>\n",
       "      <td>04:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>67</td>\n",
       "      <td>0.011628</td>\n",
       "      <td>0.015070</td>\n",
       "      <td>0.015070</td>\n",
       "      <td>0.191643</td>\n",
       "      <td>0.970382</td>\n",
       "      <td>04:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>68</td>\n",
       "      <td>0.011604</td>\n",
       "      <td>0.015085</td>\n",
       "      <td>0.015085</td>\n",
       "      <td>0.180919</td>\n",
       "      <td>0.970349</td>\n",
       "      <td>04:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>69</td>\n",
       "      <td>0.011288</td>\n",
       "      <td>0.015016</td>\n",
       "      <td>0.015016</td>\n",
       "      <td>0.182743</td>\n",
       "      <td>0.970476</td>\n",
       "      <td>04:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>0.011227</td>\n",
       "      <td>0.014688</td>\n",
       "      <td>0.014688</td>\n",
       "      <td>0.206756</td>\n",
       "      <td>0.971107</td>\n",
       "      <td>04:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>71</td>\n",
       "      <td>0.011193</td>\n",
       "      <td>0.014586</td>\n",
       "      <td>0.014586</td>\n",
       "      <td>0.193131</td>\n",
       "      <td>0.971312</td>\n",
       "      <td>04:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>72</td>\n",
       "      <td>0.010909</td>\n",
       "      <td>0.014706</td>\n",
       "      <td>0.014706</td>\n",
       "      <td>0.188376</td>\n",
       "      <td>0.971076</td>\n",
       "      <td>03:59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>73</td>\n",
       "      <td>0.010895</td>\n",
       "      <td>0.014630</td>\n",
       "      <td>0.014630</td>\n",
       "      <td>0.186453</td>\n",
       "      <td>0.971220</td>\n",
       "      <td>04:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>74</td>\n",
       "      <td>0.010595</td>\n",
       "      <td>0.014412</td>\n",
       "      <td>0.014412</td>\n",
       "      <td>0.198193</td>\n",
       "      <td>0.971650</td>\n",
       "      <td>04:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>75</td>\n",
       "      <td>0.010785</td>\n",
       "      <td>0.014354</td>\n",
       "      <td>0.014354</td>\n",
       "      <td>0.200447</td>\n",
       "      <td>0.971751</td>\n",
       "      <td>04:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>76</td>\n",
       "      <td>0.010571</td>\n",
       "      <td>0.014220</td>\n",
       "      <td>0.014220</td>\n",
       "      <td>0.194572</td>\n",
       "      <td>0.972019</td>\n",
       "      <td>04:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>77</td>\n",
       "      <td>0.010554</td>\n",
       "      <td>0.014202</td>\n",
       "      <td>0.014202</td>\n",
       "      <td>0.190050</td>\n",
       "      <td>0.972057</td>\n",
       "      <td>04:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>78</td>\n",
       "      <td>0.010556</td>\n",
       "      <td>0.014135</td>\n",
       "      <td>0.014135</td>\n",
       "      <td>0.183214</td>\n",
       "      <td>0.972169</td>\n",
       "      <td>04:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>79</td>\n",
       "      <td>0.010135</td>\n",
       "      <td>0.014062</td>\n",
       "      <td>0.014062</td>\n",
       "      <td>0.178843</td>\n",
       "      <td>0.972329</td>\n",
       "      <td>04:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>0.010114</td>\n",
       "      <td>0.013975</td>\n",
       "      <td>0.013975</td>\n",
       "      <td>0.171451</td>\n",
       "      <td>0.972492</td>\n",
       "      <td>04:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>81</td>\n",
       "      <td>0.010018</td>\n",
       "      <td>0.013744</td>\n",
       "      <td>0.013744</td>\n",
       "      <td>0.179849</td>\n",
       "      <td>0.972952</td>\n",
       "      <td>03:59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>82</td>\n",
       "      <td>0.009744</td>\n",
       "      <td>0.013658</td>\n",
       "      <td>0.013658</td>\n",
       "      <td>0.196911</td>\n",
       "      <td>0.973107</td>\n",
       "      <td>04:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>83</td>\n",
       "      <td>0.009608</td>\n",
       "      <td>0.013496</td>\n",
       "      <td>0.013496</td>\n",
       "      <td>0.184167</td>\n",
       "      <td>0.973418</td>\n",
       "      <td>04:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>84</td>\n",
       "      <td>0.009698</td>\n",
       "      <td>0.013534</td>\n",
       "      <td>0.013534</td>\n",
       "      <td>0.188695</td>\n",
       "      <td>0.973347</td>\n",
       "      <td>04:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>85</td>\n",
       "      <td>0.009581</td>\n",
       "      <td>0.013459</td>\n",
       "      <td>0.013459</td>\n",
       "      <td>0.183762</td>\n",
       "      <td>0.973481</td>\n",
       "      <td>03:59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>86</td>\n",
       "      <td>0.009403</td>\n",
       "      <td>0.013288</td>\n",
       "      <td>0.013288</td>\n",
       "      <td>0.187014</td>\n",
       "      <td>0.973827</td>\n",
       "      <td>04:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>87</td>\n",
       "      <td>0.009436</td>\n",
       "      <td>0.013346</td>\n",
       "      <td>0.013346</td>\n",
       "      <td>0.183260</td>\n",
       "      <td>0.973713</td>\n",
       "      <td>04:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>88</td>\n",
       "      <td>0.009263</td>\n",
       "      <td>0.013120</td>\n",
       "      <td>0.013120</td>\n",
       "      <td>0.189079</td>\n",
       "      <td>0.974151</td>\n",
       "      <td>04:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>89</td>\n",
       "      <td>0.009341</td>\n",
       "      <td>0.013217</td>\n",
       "      <td>0.013217</td>\n",
       "      <td>0.180502</td>\n",
       "      <td>0.973971</td>\n",
       "      <td>04:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>0.009121</td>\n",
       "      <td>0.013163</td>\n",
       "      <td>0.013163</td>\n",
       "      <td>0.171209</td>\n",
       "      <td>0.974070</td>\n",
       "      <td>04:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>91</td>\n",
       "      <td>0.008781</td>\n",
       "      <td>0.012957</td>\n",
       "      <td>0.012957</td>\n",
       "      <td>0.178005</td>\n",
       "      <td>0.974473</td>\n",
       "      <td>04:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>92</td>\n",
       "      <td>0.008842</td>\n",
       "      <td>0.012963</td>\n",
       "      <td>0.012963</td>\n",
       "      <td>0.167308</td>\n",
       "      <td>0.974463</td>\n",
       "      <td>04:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>93</td>\n",
       "      <td>0.008540</td>\n",
       "      <td>0.013117</td>\n",
       "      <td>0.013117</td>\n",
       "      <td>0.153893</td>\n",
       "      <td>0.974175</td>\n",
       "      <td>04:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>94</td>\n",
       "      <td>0.008601</td>\n",
       "      <td>0.012805</td>\n",
       "      <td>0.012805</td>\n",
       "      <td>0.158961</td>\n",
       "      <td>0.974774</td>\n",
       "      <td>04:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>95</td>\n",
       "      <td>0.008508</td>\n",
       "      <td>0.012730</td>\n",
       "      <td>0.012730</td>\n",
       "      <td>0.186107</td>\n",
       "      <td>0.974913</td>\n",
       "      <td>04:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>96</td>\n",
       "      <td>0.008400</td>\n",
       "      <td>0.012689</td>\n",
       "      <td>0.012689</td>\n",
       "      <td>0.168395</td>\n",
       "      <td>0.974999</td>\n",
       "      <td>04:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>97</td>\n",
       "      <td>0.008481</td>\n",
       "      <td>0.012603</td>\n",
       "      <td>0.012603</td>\n",
       "      <td>0.166740</td>\n",
       "      <td>0.975150</td>\n",
       "      <td>04:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>98</td>\n",
       "      <td>0.008303</td>\n",
       "      <td>0.012630</td>\n",
       "      <td>0.012630</td>\n",
       "      <td>0.172744</td>\n",
       "      <td>0.975105</td>\n",
       "      <td>04:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>99</td>\n",
       "      <td>0.008161</td>\n",
       "      <td>0.012464</td>\n",
       "      <td>0.012464</td>\n",
       "      <td>0.173961</td>\n",
       "      <td>0.975435</td>\n",
       "      <td>04:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.008075</td>\n",
       "      <td>0.012597</td>\n",
       "      <td>0.012597</td>\n",
       "      <td>0.155947</td>\n",
       "      <td>0.975177</td>\n",
       "      <td>04:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>101</td>\n",
       "      <td>0.007934</td>\n",
       "      <td>0.012225</td>\n",
       "      <td>0.012225</td>\n",
       "      <td>0.171793</td>\n",
       "      <td>0.975895</td>\n",
       "      <td>04:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>102</td>\n",
       "      <td>0.007796</td>\n",
       "      <td>0.012250</td>\n",
       "      <td>0.012250</td>\n",
       "      <td>0.157961</td>\n",
       "      <td>0.975851</td>\n",
       "      <td>04:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>103</td>\n",
       "      <td>0.008052</td>\n",
       "      <td>0.012549</td>\n",
       "      <td>0.012549</td>\n",
       "      <td>0.151141</td>\n",
       "      <td>0.975267</td>\n",
       "      <td>04:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>104</td>\n",
       "      <td>0.007623</td>\n",
       "      <td>0.011927</td>\n",
       "      <td>0.011927</td>\n",
       "      <td>0.172627</td>\n",
       "      <td>0.976472</td>\n",
       "      <td>04:02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>105</td>\n",
       "      <td>0.007549</td>\n",
       "      <td>0.011960</td>\n",
       "      <td>0.011960</td>\n",
       "      <td>0.165381</td>\n",
       "      <td>0.976415</td>\n",
       "      <td>04:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>106</td>\n",
       "      <td>0.007742</td>\n",
       "      <td>0.012158</td>\n",
       "      <td>0.012158</td>\n",
       "      <td>0.149484</td>\n",
       "      <td>0.976037</td>\n",
       "      <td>04:02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>107</td>\n",
       "      <td>0.007554</td>\n",
       "      <td>0.011960</td>\n",
       "      <td>0.011960</td>\n",
       "      <td>0.163388</td>\n",
       "      <td>0.976414</td>\n",
       "      <td>04:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>108</td>\n",
       "      <td>0.007474</td>\n",
       "      <td>0.011811</td>\n",
       "      <td>0.011811</td>\n",
       "      <td>0.173993</td>\n",
       "      <td>0.976703</td>\n",
       "      <td>04:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>109</td>\n",
       "      <td>0.007268</td>\n",
       "      <td>0.011662</td>\n",
       "      <td>0.011662</td>\n",
       "      <td>0.173883</td>\n",
       "      <td>0.977000</td>\n",
       "      <td>04:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>110</td>\n",
       "      <td>0.007212</td>\n",
       "      <td>0.011692</td>\n",
       "      <td>0.011692</td>\n",
       "      <td>0.163585</td>\n",
       "      <td>0.976937</td>\n",
       "      <td>04:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>111</td>\n",
       "      <td>0.007204</td>\n",
       "      <td>0.011609</td>\n",
       "      <td>0.011609</td>\n",
       "      <td>0.170101</td>\n",
       "      <td>0.977104</td>\n",
       "      <td>04:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>112</td>\n",
       "      <td>0.007019</td>\n",
       "      <td>0.011680</td>\n",
       "      <td>0.011680</td>\n",
       "      <td>0.162199</td>\n",
       "      <td>0.976961</td>\n",
       "      <td>04:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>113</td>\n",
       "      <td>0.007098</td>\n",
       "      <td>0.011428</td>\n",
       "      <td>0.011428</td>\n",
       "      <td>0.171833</td>\n",
       "      <td>0.977443</td>\n",
       "      <td>04:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>114</td>\n",
       "      <td>0.007067</td>\n",
       "      <td>0.011686</td>\n",
       "      <td>0.011686</td>\n",
       "      <td>0.139736</td>\n",
       "      <td>0.976950</td>\n",
       "      <td>04:02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>115</td>\n",
       "      <td>0.006869</td>\n",
       "      <td>0.011230</td>\n",
       "      <td>0.011230</td>\n",
       "      <td>0.173729</td>\n",
       "      <td>0.977835</td>\n",
       "      <td>04:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>116</td>\n",
       "      <td>0.006723</td>\n",
       "      <td>0.011552</td>\n",
       "      <td>0.011552</td>\n",
       "      <td>0.170503</td>\n",
       "      <td>0.977201</td>\n",
       "      <td>04:02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>117</td>\n",
       "      <td>0.006605</td>\n",
       "      <td>0.011596</td>\n",
       "      <td>0.011596</td>\n",
       "      <td>0.135207</td>\n",
       "      <td>0.977127</td>\n",
       "      <td>04:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>118</td>\n",
       "      <td>0.006761</td>\n",
       "      <td>0.011439</td>\n",
       "      <td>0.011439</td>\n",
       "      <td>0.146200</td>\n",
       "      <td>0.977433</td>\n",
       "      <td>04:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>119</td>\n",
       "      <td>0.006583</td>\n",
       "      <td>0.011428</td>\n",
       "      <td>0.011428</td>\n",
       "      <td>0.147819</td>\n",
       "      <td>0.977456</td>\n",
       "      <td>04:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120</td>\n",
       "      <td>0.006628</td>\n",
       "      <td>0.011445</td>\n",
       "      <td>0.011445</td>\n",
       "      <td>0.145483</td>\n",
       "      <td>0.977417</td>\n",
       "      <td>04:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>121</td>\n",
       "      <td>0.006394</td>\n",
       "      <td>0.010959</td>\n",
       "      <td>0.010959</td>\n",
       "      <td>0.164972</td>\n",
       "      <td>0.978367</td>\n",
       "      <td>04:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>122</td>\n",
       "      <td>0.006402</td>\n",
       "      <td>0.010966</td>\n",
       "      <td>0.010966</td>\n",
       "      <td>0.151696</td>\n",
       "      <td>0.978356</td>\n",
       "      <td>04:02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>123</td>\n",
       "      <td>0.006374</td>\n",
       "      <td>0.011006</td>\n",
       "      <td>0.011006</td>\n",
       "      <td>0.162977</td>\n",
       "      <td>0.978272</td>\n",
       "      <td>04:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>124</td>\n",
       "      <td>0.006333</td>\n",
       "      <td>0.010867</td>\n",
       "      <td>0.010867</td>\n",
       "      <td>0.168883</td>\n",
       "      <td>0.978543</td>\n",
       "      <td>04:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>125</td>\n",
       "      <td>0.006395</td>\n",
       "      <td>0.011088</td>\n",
       "      <td>0.011088</td>\n",
       "      <td>0.141991</td>\n",
       "      <td>0.978130</td>\n",
       "      <td>04:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>126</td>\n",
       "      <td>0.006157</td>\n",
       "      <td>0.010847</td>\n",
       "      <td>0.010847</td>\n",
       "      <td>0.152839</td>\n",
       "      <td>0.978584</td>\n",
       "      <td>04:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>127</td>\n",
       "      <td>0.006166</td>\n",
       "      <td>0.010708</td>\n",
       "      <td>0.010708</td>\n",
       "      <td>0.155883</td>\n",
       "      <td>0.978855</td>\n",
       "      <td>04:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>128</td>\n",
       "      <td>0.006017</td>\n",
       "      <td>0.010933</td>\n",
       "      <td>0.010933</td>\n",
       "      <td>0.137766</td>\n",
       "      <td>0.978417</td>\n",
       "      <td>04:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>129</td>\n",
       "      <td>0.005933</td>\n",
       "      <td>0.010791</td>\n",
       "      <td>0.010791</td>\n",
       "      <td>0.162594</td>\n",
       "      <td>0.978694</td>\n",
       "      <td>04:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>130</td>\n",
       "      <td>0.005889</td>\n",
       "      <td>0.010515</td>\n",
       "      <td>0.010515</td>\n",
       "      <td>0.163551</td>\n",
       "      <td>0.979234</td>\n",
       "      <td>04:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>131</td>\n",
       "      <td>0.005815</td>\n",
       "      <td>0.010569</td>\n",
       "      <td>0.010569</td>\n",
       "      <td>0.156877</td>\n",
       "      <td>0.979136</td>\n",
       "      <td>04:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>132</td>\n",
       "      <td>0.005722</td>\n",
       "      <td>0.010573</td>\n",
       "      <td>0.010573</td>\n",
       "      <td>0.152389</td>\n",
       "      <td>0.979113</td>\n",
       "      <td>04:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>133</td>\n",
       "      <td>0.005771</td>\n",
       "      <td>0.010583</td>\n",
       "      <td>0.010583</td>\n",
       "      <td>0.156738</td>\n",
       "      <td>0.979107</td>\n",
       "      <td>04:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>134</td>\n",
       "      <td>0.005679</td>\n",
       "      <td>0.010322</td>\n",
       "      <td>0.010322</td>\n",
       "      <td>0.153326</td>\n",
       "      <td>0.979610</td>\n",
       "      <td>04:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>135</td>\n",
       "      <td>0.005563</td>\n",
       "      <td>0.010273</td>\n",
       "      <td>0.010273</td>\n",
       "      <td>0.157099</td>\n",
       "      <td>0.979708</td>\n",
       "      <td>04:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>136</td>\n",
       "      <td>0.005597</td>\n",
       "      <td>0.010353</td>\n",
       "      <td>0.010353</td>\n",
       "      <td>0.172324</td>\n",
       "      <td>0.979548</td>\n",
       "      <td>04:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>137</td>\n",
       "      <td>0.005482</td>\n",
       "      <td>0.010288</td>\n",
       "      <td>0.010288</td>\n",
       "      <td>0.167243</td>\n",
       "      <td>0.979676</td>\n",
       "      <td>04:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>138</td>\n",
       "      <td>0.005530</td>\n",
       "      <td>0.010552</td>\n",
       "      <td>0.010552</td>\n",
       "      <td>0.139089</td>\n",
       "      <td>0.979166</td>\n",
       "      <td>04:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>139</td>\n",
       "      <td>0.005469</td>\n",
       "      <td>0.010140</td>\n",
       "      <td>0.010140</td>\n",
       "      <td>0.155139</td>\n",
       "      <td>0.979963</td>\n",
       "      <td>04:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>140</td>\n",
       "      <td>0.005337</td>\n",
       "      <td>0.010201</td>\n",
       "      <td>0.010201</td>\n",
       "      <td>0.160957</td>\n",
       "      <td>0.979851</td>\n",
       "      <td>04:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>141</td>\n",
       "      <td>0.005225</td>\n",
       "      <td>0.010082</td>\n",
       "      <td>0.010082</td>\n",
       "      <td>0.157207</td>\n",
       "      <td>0.980081</td>\n",
       "      <td>04:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>142</td>\n",
       "      <td>0.005221</td>\n",
       "      <td>0.010353</td>\n",
       "      <td>0.010353</td>\n",
       "      <td>0.146719</td>\n",
       "      <td>0.979555</td>\n",
       "      <td>04:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>143</td>\n",
       "      <td>0.005243</td>\n",
       "      <td>0.010119</td>\n",
       "      <td>0.010119</td>\n",
       "      <td>0.150036</td>\n",
       "      <td>0.980010</td>\n",
       "      <td>04:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>144</td>\n",
       "      <td>0.005181</td>\n",
       "      <td>0.010165</td>\n",
       "      <td>0.010165</td>\n",
       "      <td>0.133607</td>\n",
       "      <td>0.979934</td>\n",
       "      <td>04:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>145</td>\n",
       "      <td>0.005144</td>\n",
       "      <td>0.009961</td>\n",
       "      <td>0.009961</td>\n",
       "      <td>0.147587</td>\n",
       "      <td>0.980321</td>\n",
       "      <td>04:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>146</td>\n",
       "      <td>0.005005</td>\n",
       "      <td>0.010139</td>\n",
       "      <td>0.010139</td>\n",
       "      <td>0.147098</td>\n",
       "      <td>0.979977</td>\n",
       "      <td>04:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>147</td>\n",
       "      <td>0.005012</td>\n",
       "      <td>0.010037</td>\n",
       "      <td>0.010037</td>\n",
       "      <td>0.156029</td>\n",
       "      <td>0.980165</td>\n",
       "      <td>04:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>148</td>\n",
       "      <td>0.005049</td>\n",
       "      <td>0.009971</td>\n",
       "      <td>0.009971</td>\n",
       "      <td>0.143080</td>\n",
       "      <td>0.980304</td>\n",
       "      <td>04:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>149</td>\n",
       "      <td>0.004924</td>\n",
       "      <td>0.009889</td>\n",
       "      <td>0.009889</td>\n",
       "      <td>0.159246</td>\n",
       "      <td>0.980457</td>\n",
       "      <td>04:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>0.004891</td>\n",
       "      <td>0.009798</td>\n",
       "      <td>0.009798</td>\n",
       "      <td>0.156877</td>\n",
       "      <td>0.980637</td>\n",
       "      <td>04:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>151</td>\n",
       "      <td>0.004850</td>\n",
       "      <td>0.009962</td>\n",
       "      <td>0.009962</td>\n",
       "      <td>0.150261</td>\n",
       "      <td>0.980327</td>\n",
       "      <td>04:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>152</td>\n",
       "      <td>0.004684</td>\n",
       "      <td>0.009830</td>\n",
       "      <td>0.009830</td>\n",
       "      <td>0.151151</td>\n",
       "      <td>0.980581</td>\n",
       "      <td>04:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>153</td>\n",
       "      <td>0.004631</td>\n",
       "      <td>0.009846</td>\n",
       "      <td>0.009846</td>\n",
       "      <td>0.143962</td>\n",
       "      <td>0.980552</td>\n",
       "      <td>04:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>154</td>\n",
       "      <td>0.004729</td>\n",
       "      <td>0.009777</td>\n",
       "      <td>0.009777</td>\n",
       "      <td>0.156343</td>\n",
       "      <td>0.980682</td>\n",
       "      <td>04:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>155</td>\n",
       "      <td>0.004814</td>\n",
       "      <td>0.009988</td>\n",
       "      <td>0.009988</td>\n",
       "      <td>0.143376</td>\n",
       "      <td>0.980269</td>\n",
       "      <td>04:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>156</td>\n",
       "      <td>0.004614</td>\n",
       "      <td>0.009900</td>\n",
       "      <td>0.009900</td>\n",
       "      <td>0.144057</td>\n",
       "      <td>0.980438</td>\n",
       "      <td>04:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>157</td>\n",
       "      <td>0.004509</td>\n",
       "      <td>0.009534</td>\n",
       "      <td>0.009534</td>\n",
       "      <td>0.155341</td>\n",
       "      <td>0.981161</td>\n",
       "      <td>04:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>158</td>\n",
       "      <td>0.004584</td>\n",
       "      <td>0.009477</td>\n",
       "      <td>0.009477</td>\n",
       "      <td>0.168156</td>\n",
       "      <td>0.981277</td>\n",
       "      <td>04:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>159</td>\n",
       "      <td>0.004412</td>\n",
       "      <td>0.009507</td>\n",
       "      <td>0.009507</td>\n",
       "      <td>0.155321</td>\n",
       "      <td>0.981210</td>\n",
       "      <td>04:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>160</td>\n",
       "      <td>0.004400</td>\n",
       "      <td>0.009807</td>\n",
       "      <td>0.009807</td>\n",
       "      <td>0.135675</td>\n",
       "      <td>0.980621</td>\n",
       "      <td>04:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>161</td>\n",
       "      <td>0.004470</td>\n",
       "      <td>0.009488</td>\n",
       "      <td>0.009488</td>\n",
       "      <td>0.147445</td>\n",
       "      <td>0.981243</td>\n",
       "      <td>04:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>162</td>\n",
       "      <td>0.004420</td>\n",
       "      <td>0.009816</td>\n",
       "      <td>0.009816</td>\n",
       "      <td>0.139629</td>\n",
       "      <td>0.980600</td>\n",
       "      <td>04:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>163</td>\n",
       "      <td>0.004393</td>\n",
       "      <td>0.009524</td>\n",
       "      <td>0.009524</td>\n",
       "      <td>0.151771</td>\n",
       "      <td>0.981171</td>\n",
       "      <td>04:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>164</td>\n",
       "      <td>0.004241</td>\n",
       "      <td>0.009353</td>\n",
       "      <td>0.009353</td>\n",
       "      <td>0.139065</td>\n",
       "      <td>0.981522</td>\n",
       "      <td>04:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>165</td>\n",
       "      <td>0.004321</td>\n",
       "      <td>0.009482</td>\n",
       "      <td>0.009482</td>\n",
       "      <td>0.155263</td>\n",
       "      <td>0.981254</td>\n",
       "      <td>04:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>166</td>\n",
       "      <td>0.004186</td>\n",
       "      <td>0.009765</td>\n",
       "      <td>0.009765</td>\n",
       "      <td>0.132894</td>\n",
       "      <td>0.980702</td>\n",
       "      <td>04:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>167</td>\n",
       "      <td>0.004248</td>\n",
       "      <td>0.009434</td>\n",
       "      <td>0.009434</td>\n",
       "      <td>0.144793</td>\n",
       "      <td>0.981350</td>\n",
       "      <td>04:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>168</td>\n",
       "      <td>0.004245</td>\n",
       "      <td>0.009271</td>\n",
       "      <td>0.009271</td>\n",
       "      <td>0.140845</td>\n",
       "      <td>0.981681</td>\n",
       "      <td>03:59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>169</td>\n",
       "      <td>0.004032</td>\n",
       "      <td>0.009429</td>\n",
       "      <td>0.009429</td>\n",
       "      <td>0.144572</td>\n",
       "      <td>0.981358</td>\n",
       "      <td>04:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>170</td>\n",
       "      <td>0.004051</td>\n",
       "      <td>0.009271</td>\n",
       "      <td>0.009271</td>\n",
       "      <td>0.139441</td>\n",
       "      <td>0.981671</td>\n",
       "      <td>04:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>171</td>\n",
       "      <td>0.004134</td>\n",
       "      <td>0.009264</td>\n",
       "      <td>0.009264</td>\n",
       "      <td>0.149898</td>\n",
       "      <td>0.981685</td>\n",
       "      <td>04:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>172</td>\n",
       "      <td>0.004068</td>\n",
       "      <td>0.009239</td>\n",
       "      <td>0.009239</td>\n",
       "      <td>0.155390</td>\n",
       "      <td>0.981734</td>\n",
       "      <td>04:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>173</td>\n",
       "      <td>0.003945</td>\n",
       "      <td>0.009427</td>\n",
       "      <td>0.009427</td>\n",
       "      <td>0.131964</td>\n",
       "      <td>0.981383</td>\n",
       "      <td>04:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>174</td>\n",
       "      <td>0.003912</td>\n",
       "      <td>0.009319</td>\n",
       "      <td>0.009319</td>\n",
       "      <td>0.137255</td>\n",
       "      <td>0.981583</td>\n",
       "      <td>04:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>175</td>\n",
       "      <td>0.003843</td>\n",
       "      <td>0.009114</td>\n",
       "      <td>0.009114</td>\n",
       "      <td>0.157732</td>\n",
       "      <td>0.981982</td>\n",
       "      <td>04:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>176</td>\n",
       "      <td>0.003827</td>\n",
       "      <td>0.009217</td>\n",
       "      <td>0.009217</td>\n",
       "      <td>0.150049</td>\n",
       "      <td>0.981769</td>\n",
       "      <td>04:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>177</td>\n",
       "      <td>0.003874</td>\n",
       "      <td>0.009165</td>\n",
       "      <td>0.009165</td>\n",
       "      <td>0.137362</td>\n",
       "      <td>0.981885</td>\n",
       "      <td>04:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>178</td>\n",
       "      <td>0.003779</td>\n",
       "      <td>0.009081</td>\n",
       "      <td>0.009081</td>\n",
       "      <td>0.148129</td>\n",
       "      <td>0.982050</td>\n",
       "      <td>04:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>179</td>\n",
       "      <td>0.003933</td>\n",
       "      <td>0.009262</td>\n",
       "      <td>0.009262</td>\n",
       "      <td>0.139089</td>\n",
       "      <td>0.981701</td>\n",
       "      <td>04:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>180</td>\n",
       "      <td>0.003701</td>\n",
       "      <td>0.009142</td>\n",
       "      <td>0.009142</td>\n",
       "      <td>0.149674</td>\n",
       "      <td>0.981926</td>\n",
       "      <td>04:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>181</td>\n",
       "      <td>0.003702</td>\n",
       "      <td>0.009028</td>\n",
       "      <td>0.009028</td>\n",
       "      <td>0.168177</td>\n",
       "      <td>0.982149</td>\n",
       "      <td>04:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>182</td>\n",
       "      <td>0.003763</td>\n",
       "      <td>0.009151</td>\n",
       "      <td>0.009151</td>\n",
       "      <td>0.148982</td>\n",
       "      <td>0.981902</td>\n",
       "      <td>04:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>183</td>\n",
       "      <td>0.003678</td>\n",
       "      <td>0.009071</td>\n",
       "      <td>0.009071</td>\n",
       "      <td>0.146120</td>\n",
       "      <td>0.982060</td>\n",
       "      <td>04:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>184</td>\n",
       "      <td>0.003619</td>\n",
       "      <td>0.008983</td>\n",
       "      <td>0.008983</td>\n",
       "      <td>0.152629</td>\n",
       "      <td>0.982229</td>\n",
       "      <td>04:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>185</td>\n",
       "      <td>0.003815</td>\n",
       "      <td>0.008985</td>\n",
       "      <td>0.008985</td>\n",
       "      <td>0.153222</td>\n",
       "      <td>0.982233</td>\n",
       "      <td>04:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>186</td>\n",
       "      <td>0.003591</td>\n",
       "      <td>0.008914</td>\n",
       "      <td>0.008914</td>\n",
       "      <td>0.158296</td>\n",
       "      <td>0.982372</td>\n",
       "      <td>04:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>187</td>\n",
       "      <td>0.003546</td>\n",
       "      <td>0.008945</td>\n",
       "      <td>0.008945</td>\n",
       "      <td>0.155156</td>\n",
       "      <td>0.982304</td>\n",
       "      <td>04:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>188</td>\n",
       "      <td>0.003583</td>\n",
       "      <td>0.008897</td>\n",
       "      <td>0.008897</td>\n",
       "      <td>0.150992</td>\n",
       "      <td>0.982402</td>\n",
       "      <td>04:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>189</td>\n",
       "      <td>0.003845</td>\n",
       "      <td>0.008913</td>\n",
       "      <td>0.008913</td>\n",
       "      <td>0.143937</td>\n",
       "      <td>0.982374</td>\n",
       "      <td>04:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>190</td>\n",
       "      <td>0.003626</td>\n",
       "      <td>0.008794</td>\n",
       "      <td>0.008794</td>\n",
       "      <td>0.152209</td>\n",
       "      <td>0.982604</td>\n",
       "      <td>04:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>191</td>\n",
       "      <td>0.003490</td>\n",
       "      <td>0.008773</td>\n",
       "      <td>0.008773</td>\n",
       "      <td>0.154553</td>\n",
       "      <td>0.982643</td>\n",
       "      <td>04:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>192</td>\n",
       "      <td>0.003368</td>\n",
       "      <td>0.008675</td>\n",
       "      <td>0.008675</td>\n",
       "      <td>0.153043</td>\n",
       "      <td>0.982835</td>\n",
       "      <td>04:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>193</td>\n",
       "      <td>0.003336</td>\n",
       "      <td>0.008848</td>\n",
       "      <td>0.008848</td>\n",
       "      <td>0.162541</td>\n",
       "      <td>0.982497</td>\n",
       "      <td>04:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>194</td>\n",
       "      <td>0.003309</td>\n",
       "      <td>0.008883</td>\n",
       "      <td>0.008883</td>\n",
       "      <td>0.143212</td>\n",
       "      <td>0.982427</td>\n",
       "      <td>04:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>195</td>\n",
       "      <td>0.003437</td>\n",
       "      <td>0.008969</td>\n",
       "      <td>0.008969</td>\n",
       "      <td>0.151178</td>\n",
       "      <td>0.982254</td>\n",
       "      <td>04:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>196</td>\n",
       "      <td>0.003359</td>\n",
       "      <td>0.008689</td>\n",
       "      <td>0.008689</td>\n",
       "      <td>0.145880</td>\n",
       "      <td>0.982811</td>\n",
       "      <td>04:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>197</td>\n",
       "      <td>0.003279</td>\n",
       "      <td>0.008994</td>\n",
       "      <td>0.008994</td>\n",
       "      <td>0.151541</td>\n",
       "      <td>0.982208</td>\n",
       "      <td>04:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>198</td>\n",
       "      <td>0.003272</td>\n",
       "      <td>0.008976</td>\n",
       "      <td>0.008976</td>\n",
       "      <td>0.136169</td>\n",
       "      <td>0.982246</td>\n",
       "      <td>04:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>199</td>\n",
       "      <td>0.003180</td>\n",
       "      <td>0.008763</td>\n",
       "      <td>0.008763</td>\n",
       "      <td>0.152280</td>\n",
       "      <td>0.982659</td>\n",
       "      <td>04:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.003262</td>\n",
       "      <td>0.008673</td>\n",
       "      <td>0.008673</td>\n",
       "      <td>0.162507</td>\n",
       "      <td>0.982843</td>\n",
       "      <td>04:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>201</td>\n",
       "      <td>0.003253</td>\n",
       "      <td>0.008756</td>\n",
       "      <td>0.008756</td>\n",
       "      <td>0.165217</td>\n",
       "      <td>0.982671</td>\n",
       "      <td>04:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>202</td>\n",
       "      <td>0.003142</td>\n",
       "      <td>0.008589</td>\n",
       "      <td>0.008589</td>\n",
       "      <td>0.155875</td>\n",
       "      <td>0.983007</td>\n",
       "      <td>04:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>203</td>\n",
       "      <td>0.003255</td>\n",
       "      <td>0.008893</td>\n",
       "      <td>0.008893</td>\n",
       "      <td>0.138972</td>\n",
       "      <td>0.982417</td>\n",
       "      <td>04:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>204</td>\n",
       "      <td>0.003160</td>\n",
       "      <td>0.008616</td>\n",
       "      <td>0.008616</td>\n",
       "      <td>0.156516</td>\n",
       "      <td>0.982954</td>\n",
       "      <td>04:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>205</td>\n",
       "      <td>0.003121</td>\n",
       "      <td>0.008619</td>\n",
       "      <td>0.008619</td>\n",
       "      <td>0.142709</td>\n",
       "      <td>0.982949</td>\n",
       "      <td>04:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>206</td>\n",
       "      <td>0.003050</td>\n",
       "      <td>0.008613</td>\n",
       "      <td>0.008613</td>\n",
       "      <td>0.144731</td>\n",
       "      <td>0.982953</td>\n",
       "      <td>04:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>207</td>\n",
       "      <td>0.003231</td>\n",
       "      <td>0.008601</td>\n",
       "      <td>0.008601</td>\n",
       "      <td>0.143153</td>\n",
       "      <td>0.982987</td>\n",
       "      <td>04:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>208</td>\n",
       "      <td>0.003037</td>\n",
       "      <td>0.008547</td>\n",
       "      <td>0.008547</td>\n",
       "      <td>0.144663</td>\n",
       "      <td>0.983091</td>\n",
       "      <td>04:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>209</td>\n",
       "      <td>0.003052</td>\n",
       "      <td>0.008435</td>\n",
       "      <td>0.008435</td>\n",
       "      <td>0.159616</td>\n",
       "      <td>0.983305</td>\n",
       "      <td>04:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>210</td>\n",
       "      <td>0.003109</td>\n",
       "      <td>0.008763</td>\n",
       "      <td>0.008763</td>\n",
       "      <td>0.141363</td>\n",
       "      <td>0.982661</td>\n",
       "      <td>04:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>211</td>\n",
       "      <td>0.003112</td>\n",
       "      <td>0.008579</td>\n",
       "      <td>0.008579</td>\n",
       "      <td>0.157045</td>\n",
       "      <td>0.983025</td>\n",
       "      <td>04:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>212</td>\n",
       "      <td>0.003053</td>\n",
       "      <td>0.008495</td>\n",
       "      <td>0.008495</td>\n",
       "      <td>0.153413</td>\n",
       "      <td>0.983187</td>\n",
       "      <td>04:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>213</td>\n",
       "      <td>0.002984</td>\n",
       "      <td>0.008536</td>\n",
       "      <td>0.008536</td>\n",
       "      <td>0.151381</td>\n",
       "      <td>0.983108</td>\n",
       "      <td>04:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>214</td>\n",
       "      <td>0.002925</td>\n",
       "      <td>0.008734</td>\n",
       "      <td>0.008734</td>\n",
       "      <td>0.114999</td>\n",
       "      <td>0.982735</td>\n",
       "      <td>04:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>215</td>\n",
       "      <td>0.002939</td>\n",
       "      <td>0.008601</td>\n",
       "      <td>0.008601</td>\n",
       "      <td>0.147172</td>\n",
       "      <td>0.982977</td>\n",
       "      <td>04:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>216</td>\n",
       "      <td>0.002990</td>\n",
       "      <td>0.008482</td>\n",
       "      <td>0.008482</td>\n",
       "      <td>0.146245</td>\n",
       "      <td>0.983222</td>\n",
       "      <td>04:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>217</td>\n",
       "      <td>0.002920</td>\n",
       "      <td>0.008542</td>\n",
       "      <td>0.008542</td>\n",
       "      <td>0.151599</td>\n",
       "      <td>0.983104</td>\n",
       "      <td>04:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>218</td>\n",
       "      <td>0.002815</td>\n",
       "      <td>0.008608</td>\n",
       "      <td>0.008608</td>\n",
       "      <td>0.119515</td>\n",
       "      <td>0.982978</td>\n",
       "      <td>04:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>219</td>\n",
       "      <td>0.002815</td>\n",
       "      <td>0.008565</td>\n",
       "      <td>0.008565</td>\n",
       "      <td>0.149143</td>\n",
       "      <td>0.983055</td>\n",
       "      <td>04:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>220</td>\n",
       "      <td>0.002874</td>\n",
       "      <td>0.008573</td>\n",
       "      <td>0.008573</td>\n",
       "      <td>0.131682</td>\n",
       "      <td>0.983034</td>\n",
       "      <td>04:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>221</td>\n",
       "      <td>0.002737</td>\n",
       "      <td>0.008396</td>\n",
       "      <td>0.008396</td>\n",
       "      <td>0.144695</td>\n",
       "      <td>0.983381</td>\n",
       "      <td>04:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>222</td>\n",
       "      <td>0.002757</td>\n",
       "      <td>0.008421</td>\n",
       "      <td>0.008421</td>\n",
       "      <td>0.141667</td>\n",
       "      <td>0.983334</td>\n",
       "      <td>04:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>223</td>\n",
       "      <td>0.002800</td>\n",
       "      <td>0.008363</td>\n",
       "      <td>0.008363</td>\n",
       "      <td>0.156787</td>\n",
       "      <td>0.983443</td>\n",
       "      <td>04:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>224</td>\n",
       "      <td>0.002805</td>\n",
       "      <td>0.008410</td>\n",
       "      <td>0.008410</td>\n",
       "      <td>0.140028</td>\n",
       "      <td>0.983360</td>\n",
       "      <td>04:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>225</td>\n",
       "      <td>0.003051</td>\n",
       "      <td>0.008403</td>\n",
       "      <td>0.008403</td>\n",
       "      <td>0.133848</td>\n",
       "      <td>0.983368</td>\n",
       "      <td>04:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>226</td>\n",
       "      <td>0.002858</td>\n",
       "      <td>0.008267</td>\n",
       "      <td>0.008267</td>\n",
       "      <td>0.155799</td>\n",
       "      <td>0.983638</td>\n",
       "      <td>04:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>227</td>\n",
       "      <td>0.002457</td>\n",
       "      <td>0.008151</td>\n",
       "      <td>0.008151</td>\n",
       "      <td>0.144587</td>\n",
       "      <td>0.983865</td>\n",
       "      <td>04:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>228</td>\n",
       "      <td>0.002411</td>\n",
       "      <td>0.008088</td>\n",
       "      <td>0.008088</td>\n",
       "      <td>0.146333</td>\n",
       "      <td>0.983991</td>\n",
       "      <td>04:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>229</td>\n",
       "      <td>0.002407</td>\n",
       "      <td>0.008096</td>\n",
       "      <td>0.008096</td>\n",
       "      <td>0.136359</td>\n",
       "      <td>0.983972</td>\n",
       "      <td>04:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>230</td>\n",
       "      <td>0.002351</td>\n",
       "      <td>0.008185</td>\n",
       "      <td>0.008185</td>\n",
       "      <td>0.144440</td>\n",
       "      <td>0.983796</td>\n",
       "      <td>04:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>231</td>\n",
       "      <td>0.002348</td>\n",
       "      <td>0.008142</td>\n",
       "      <td>0.008142</td>\n",
       "      <td>0.151952</td>\n",
       "      <td>0.983878</td>\n",
       "      <td>04:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>232</td>\n",
       "      <td>0.002235</td>\n",
       "      <td>0.008090</td>\n",
       "      <td>0.008090</td>\n",
       "      <td>0.142692</td>\n",
       "      <td>0.983984</td>\n",
       "      <td>04:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>233</td>\n",
       "      <td>0.002325</td>\n",
       "      <td>0.008097</td>\n",
       "      <td>0.008097</td>\n",
       "      <td>0.144094</td>\n",
       "      <td>0.983971</td>\n",
       "      <td>04:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>234</td>\n",
       "      <td>0.002292</td>\n",
       "      <td>0.008084</td>\n",
       "      <td>0.008084</td>\n",
       "      <td>0.146468</td>\n",
       "      <td>0.983997</td>\n",
       "      <td>04:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>235</td>\n",
       "      <td>0.002198</td>\n",
       "      <td>0.008134</td>\n",
       "      <td>0.008134</td>\n",
       "      <td>0.126392</td>\n",
       "      <td>0.983904</td>\n",
       "      <td>04:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>236</td>\n",
       "      <td>0.002246</td>\n",
       "      <td>0.008029</td>\n",
       "      <td>0.008029</td>\n",
       "      <td>0.145065</td>\n",
       "      <td>0.984107</td>\n",
       "      <td>04:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>237</td>\n",
       "      <td>0.002243</td>\n",
       "      <td>0.008090</td>\n",
       "      <td>0.008090</td>\n",
       "      <td>0.139601</td>\n",
       "      <td>0.983989</td>\n",
       "      <td>04:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>238</td>\n",
       "      <td>0.002164</td>\n",
       "      <td>0.008005</td>\n",
       "      <td>0.008005</td>\n",
       "      <td>0.146543</td>\n",
       "      <td>0.984154</td>\n",
       "      <td>04:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>239</td>\n",
       "      <td>0.002227</td>\n",
       "      <td>0.008026</td>\n",
       "      <td>0.008026</td>\n",
       "      <td>0.142017</td>\n",
       "      <td>0.984110</td>\n",
       "      <td>04:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>240</td>\n",
       "      <td>0.002224</td>\n",
       "      <td>0.008092</td>\n",
       "      <td>0.008092</td>\n",
       "      <td>0.135389</td>\n",
       "      <td>0.983981</td>\n",
       "      <td>04:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>241</td>\n",
       "      <td>0.002130</td>\n",
       "      <td>0.008002</td>\n",
       "      <td>0.008002</td>\n",
       "      <td>0.148775</td>\n",
       "      <td>0.984156</td>\n",
       "      <td>04:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>242</td>\n",
       "      <td>0.002074</td>\n",
       "      <td>0.008165</td>\n",
       "      <td>0.008165</td>\n",
       "      <td>0.151688</td>\n",
       "      <td>0.983837</td>\n",
       "      <td>04:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>243</td>\n",
       "      <td>0.002208</td>\n",
       "      <td>0.008056</td>\n",
       "      <td>0.008056</td>\n",
       "      <td>0.145431</td>\n",
       "      <td>0.984047</td>\n",
       "      <td>04:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>244</td>\n",
       "      <td>0.002230</td>\n",
       "      <td>0.008017</td>\n",
       "      <td>0.008017</td>\n",
       "      <td>0.146941</td>\n",
       "      <td>0.984122</td>\n",
       "      <td>04:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>245</td>\n",
       "      <td>0.002222</td>\n",
       "      <td>0.008001</td>\n",
       "      <td>0.008001</td>\n",
       "      <td>0.150333</td>\n",
       "      <td>0.984161</td>\n",
       "      <td>04:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>246</td>\n",
       "      <td>0.002207</td>\n",
       "      <td>0.008025</td>\n",
       "      <td>0.008025</td>\n",
       "      <td>0.139046</td>\n",
       "      <td>0.984112</td>\n",
       "      <td>04:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>247</td>\n",
       "      <td>0.002151</td>\n",
       "      <td>0.008007</td>\n",
       "      <td>0.008007</td>\n",
       "      <td>0.146038</td>\n",
       "      <td>0.984145</td>\n",
       "      <td>04:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>248</td>\n",
       "      <td>0.002152</td>\n",
       "      <td>0.008097</td>\n",
       "      <td>0.008097</td>\n",
       "      <td>0.145568</td>\n",
       "      <td>0.983974</td>\n",
       "      <td>04:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>249</td>\n",
       "      <td>0.002159</td>\n",
       "      <td>0.008107</td>\n",
       "      <td>0.008107</td>\n",
       "      <td>0.123760</td>\n",
       "      <td>0.983965</td>\n",
       "      <td>04:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>0.002122</td>\n",
       "      <td>0.007981</td>\n",
       "      <td>0.007981</td>\n",
       "      <td>0.146956</td>\n",
       "      <td>0.984194</td>\n",
       "      <td>04:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>251</td>\n",
       "      <td>0.002124</td>\n",
       "      <td>0.008047</td>\n",
       "      <td>0.008047</td>\n",
       "      <td>0.144518</td>\n",
       "      <td>0.984069</td>\n",
       "      <td>04:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>252</td>\n",
       "      <td>0.002228</td>\n",
       "      <td>0.008020</td>\n",
       "      <td>0.008020</td>\n",
       "      <td>0.150123</td>\n",
       "      <td>0.984123</td>\n",
       "      <td>04:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>253</td>\n",
       "      <td>0.002169</td>\n",
       "      <td>0.007965</td>\n",
       "      <td>0.007965</td>\n",
       "      <td>0.148161</td>\n",
       "      <td>0.984234</td>\n",
       "      <td>04:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>254</td>\n",
       "      <td>0.002185</td>\n",
       "      <td>0.008011</td>\n",
       "      <td>0.008011</td>\n",
       "      <td>0.151840</td>\n",
       "      <td>0.984138</td>\n",
       "      <td>04:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>255</td>\n",
       "      <td>0.002126</td>\n",
       "      <td>0.008155</td>\n",
       "      <td>0.008155</td>\n",
       "      <td>0.141326</td>\n",
       "      <td>0.983856</td>\n",
       "      <td>04:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>256</td>\n",
       "      <td>0.002136</td>\n",
       "      <td>0.007984</td>\n",
       "      <td>0.007984</td>\n",
       "      <td>0.149032</td>\n",
       "      <td>0.984192</td>\n",
       "      <td>04:00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>\n",
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "                background: #F44336;\n",
       "            }\n",
       "        </style>\n",
       "      <progress value='25' class='' max='25', style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      100.00% [25/25 00:22<00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Better model found at epoch 0 with valid_loss value: 0.27596670389175415.\n",
      "Better model found at epoch 1 with valid_loss value: 0.11078394204378128.\n",
      "Better model found at epoch 2 with valid_loss value: 0.09720931947231293.\n",
      "Better model found at epoch 3 with valid_loss value: 0.07252483069896698.\n",
      "Better model found at epoch 4 with valid_loss value: 0.06815541535615921.\n",
      "Better model found at epoch 6 with valid_loss value: 0.05666894093155861.\n",
      "Better model found at epoch 7 with valid_loss value: 0.05271410942077637.\n",
      "Better model found at epoch 8 with valid_loss value: 0.048200543969869614.\n",
      "Better model found at epoch 10 with valid_loss value: 0.043685805052518845.\n",
      "Better model found at epoch 11 with valid_loss value: 0.03827884420752525.\n",
      "Better model found at epoch 12 with valid_loss value: 0.036419838666915894.\n",
      "Better model found at epoch 15 with valid_loss value: 0.031563516706228256.\n",
      "Better model found at epoch 19 with valid_loss value: 0.030112363398075104.\n",
      "Better model found at epoch 23 with valid_loss value: 0.02820833958685398.\n",
      "Better model found at epoch 24 with valid_loss value: 0.023979410529136658.\n",
      "Better model found at epoch 28 with valid_loss value: 0.022845499217510223.\n",
      "Better model found at epoch 29 with valid_loss value: 0.02213216945528984.\n",
      "on end of epoch#34: start annealing from 0.001 to 0.0001\n",
      "Better model found at epoch 35 with valid_loss value: 0.019941702485084534.\n",
      "Better model found at epoch 36 with valid_loss value: 0.01941896416246891.\n",
      "Better model found at epoch 37 with valid_loss value: 0.019161425530910492.\n",
      "Better model found at epoch 38 with valid_loss value: 0.01887710765004158.\n",
      "Better model found at epoch 39 with valid_loss value: 0.018508173525333405.\n",
      "Better model found at epoch 40 with valid_loss value: 0.01838095486164093.\n",
      "Better model found at epoch 41 with valid_loss value: 0.018180081620812416.\n",
      "Better model found at epoch 42 with valid_loss value: 0.017931267619132996.\n",
      "Better model found at epoch 43 with valid_loss value: 0.017710087820887566.\n",
      "Better model found at epoch 44 with valid_loss value: 0.017634527757763863.\n",
      "Better model found at epoch 45 with valid_loss value: 0.01760055497288704.\n",
      "Better model found at epoch 46 with valid_loss value: 0.0173141248524189.\n",
      "Better model found at epoch 48 with valid_loss value: 0.01708444207906723.\n",
      "Better model found at epoch 49 with valid_loss value: 0.01694193296134472.\n",
      "Better model found at epoch 50 with valid_loss value: 0.01674738898873329.\n",
      "Better model found at epoch 51 with valid_loss value: 0.016606781631708145.\n",
      "Better model found at epoch 52 with valid_loss value: 0.016537981107831.\n",
      "Better model found at epoch 53 with valid_loss value: 0.016454128548502922.\n",
      "Better model found at epoch 55 with valid_loss value: 0.016423607245087624.\n",
      "Better model found at epoch 56 with valid_loss value: 0.016355380415916443.\n",
      "Better model found at epoch 57 with valid_loss value: 0.016268447041511536.\n",
      "Better model found at epoch 58 with valid_loss value: 0.01592821441590786.\n",
      "Better model found at epoch 59 with valid_loss value: 0.01580599509179592.\n",
      "Better model found at epoch 60 with valid_loss value: 0.015679102391004562.\n",
      "Better model found at epoch 61 with valid_loss value: 0.015494558960199356.\n",
      "Better model found at epoch 64 with valid_loss value: 0.015377643518149853.\n",
      "Better model found at epoch 65 with valid_loss value: 0.015177318826317787.\n",
      "Better model found at epoch 66 with valid_loss value: 0.015150696970522404.\n",
      "Better model found at epoch 67 with valid_loss value: 0.015069859102368355.\n",
      "Better model found at epoch 69 with valid_loss value: 0.015016060322523117.\n",
      "Better model found at epoch 70 with valid_loss value: 0.014687671326100826.\n",
      "Better model found at epoch 71 with valid_loss value: 0.014586224220693111.\n",
      "Better model found at epoch 74 with valid_loss value: 0.01441160961985588.\n",
      "Better model found at epoch 75 with valid_loss value: 0.014354157261550426.\n",
      "Better model found at epoch 76 with valid_loss value: 0.01422009989619255.\n",
      "Better model found at epoch 77 with valid_loss value: 0.014202067628502846.\n",
      "Better model found at epoch 78 with valid_loss value: 0.014134795404970646.\n",
      "Better model found at epoch 79 with valid_loss value: 0.014062008820474148.\n",
      "Better model found at epoch 80 with valid_loss value: 0.01397514808923006.\n",
      "Better model found at epoch 81 with valid_loss value: 0.013743782415986061.\n",
      "Better model found at epoch 82 with valid_loss value: 0.013658451847732067.\n",
      "Better model found at epoch 83 with valid_loss value: 0.013496086932718754.\n",
      "Better model found at epoch 85 with valid_loss value: 0.013459279201924801.\n",
      "Better model found at epoch 86 with valid_loss value: 0.013287823647260666.\n",
      "Better model found at epoch 88 with valid_loss value: 0.01312037743628025.\n",
      "Better model found at epoch 91 with valid_loss value: 0.012956562452018261.\n",
      "Better model found at epoch 94 with valid_loss value: 0.012805125676095486.\n",
      "Better model found at epoch 95 with valid_loss value: 0.01273003313690424.\n",
      "Better model found at epoch 96 with valid_loss value: 0.012688951566815376.\n",
      "Better model found at epoch 97 with valid_loss value: 0.01260312832891941.\n",
      "Better model found at epoch 99 with valid_loss value: 0.012464041821658611.\n",
      "Better model found at epoch 101 with valid_loss value: 0.012225410901010036.\n",
      "Better model found at epoch 104 with valid_loss value: 0.011927179992198944.\n",
      "Better model found at epoch 108 with valid_loss value: 0.011811166070401669.\n",
      "Better model found at epoch 109 with valid_loss value: 0.011661526747047901.\n",
      "Better model found at epoch 111 with valid_loss value: 0.011608612723648548.\n",
      "Better model found at epoch 113 with valid_loss value: 0.01142815314233303.\n",
      "Better model found at epoch 115 with valid_loss value: 0.011229596100747585.\n",
      "Better model found at epoch 121 with valid_loss value: 0.010958630591630936.\n",
      "Better model found at epoch 124 with valid_loss value: 0.010867062024772167.\n",
      "Better model found at epoch 126 with valid_loss value: 0.010847385041415691.\n",
      "Better model found at epoch 127 with valid_loss value: 0.010708441957831383.\n",
      "Better model found at epoch 130 with valid_loss value: 0.010514688678085804.\n",
      "Better model found at epoch 134 with valid_loss value: 0.010322418063879013.\n",
      "Better model found at epoch 135 with valid_loss value: 0.010273042134940624.\n",
      "Better model found at epoch 139 with valid_loss value: 0.01013967301696539.\n",
      "Better model found at epoch 141 with valid_loss value: 0.010082483291625977.\n",
      "Better model found at epoch 145 with valid_loss value: 0.009961471892893314.\n",
      "Better model found at epoch 149 with valid_loss value: 0.00988927111029625.\n",
      "Better model found at epoch 150 with valid_loss value: 0.009797981008887291.\n",
      "Better model found at epoch 154 with valid_loss value: 0.009777124039828777.\n",
      "Better model found at epoch 157 with valid_loss value: 0.009533614851534367.\n",
      "Better model found at epoch 158 with valid_loss value: 0.00947741512209177.\n",
      "Better model found at epoch 164 with valid_loss value: 0.009352795779705048.\n",
      "Better model found at epoch 168 with valid_loss value: 0.009271476417779922.\n",
      "Better model found at epoch 170 with valid_loss value: 0.009270860813558102.\n",
      "Better model found at epoch 171 with valid_loss value: 0.009263784624636173.\n",
      "Better model found at epoch 172 with valid_loss value: 0.009238717146217823.\n",
      "Better model found at epoch 175 with valid_loss value: 0.009113564155995846.\n",
      "Better model found at epoch 178 with valid_loss value: 0.009080813266336918.\n",
      "Better model found at epoch 181 with valid_loss value: 0.009027826599776745.\n",
      "Better model found at epoch 184 with valid_loss value: 0.008982999250292778.\n",
      "Better model found at epoch 186 with valid_loss value: 0.008914492093026638.\n",
      "Better model found at epoch 188 with valid_loss value: 0.008897252380847931.\n",
      "Better model found at epoch 190 with valid_loss value: 0.008794400840997696.\n",
      "Better model found at epoch 191 with valid_loss value: 0.008772504515945911.\n",
      "Better model found at epoch 192 with valid_loss value: 0.008674969896674156.\n",
      "Better model found at epoch 200 with valid_loss value: 0.008673458360135555.\n",
      "Better model found at epoch 202 with valid_loss value: 0.008589050732553005.\n",
      "Better model found at epoch 208 with valid_loss value: 0.008546578697860241.\n",
      "Better model found at epoch 209 with valid_loss value: 0.008435292169451714.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Better model found at epoch 221 with valid_loss value: 0.008396374993026257.\n",
      "Better model found at epoch 223 with valid_loss value: 0.008363435044884682.\n",
      "Better model found at epoch 226 with valid_loss value: 0.008266935124993324.\n",
      "on end of epoch#226: start annealing from 0.0001 to 1e-05\n",
      "Better model found at epoch 227 with valid_loss value: 0.008151202462613583.\n",
      "Better model found at epoch 228 with valid_loss value: 0.008087804540991783.\n",
      "Better model found at epoch 234 with valid_loss value: 0.008083577267825603.\n",
      "Better model found at epoch 236 with valid_loss value: 0.008028528653085232.\n",
      "Better model found at epoch 238 with valid_loss value: 0.008004915900528431.\n",
      "Better model found at epoch 241 with valid_loss value: 0.008002088405191898.\n",
      "Better model found at epoch 245 with valid_loss value: 0.008000807836651802.\n",
      "Better model found at epoch 250 with valid_loss value: 0.007980945520102978.\n",
      "Better model found at epoch 253 with valid_loss value: 0.007964667864143848.\n",
      "on end of epoch#256: start annealing from 1e-05 to 1.0000000000000002e-06\n"
     ]
    }
   ],
   "source": [
    "lrs = [1e-3]\n",
    "checkpoints = [None]\n",
    "opts = [optf_adam]\n",
    "\n",
    "nb_train_script_logger.multi_train(get_learn=getlearn, \n",
    "            epoch_len=1e9, epochs=500,\n",
    "            opts=opts, lrs=lrs, checkpoints=checkpoints,\n",
    "            tb_log_root='./tb_log/',\n",
    "            autoSave=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "backup_train_logs(tag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "epochs:256,train_loss:0.002,mask_iou:0.984"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### 200_et数据集，无transform,resnet18，allres,dice,adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "tag = '__'.join(['resnet18', 'allres', 'dice_loss', 'dataset200_et','adam'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "getlearn = partial(get_learn, data = data_200et, model_name = 'resnet18'\n",
    "                   , loss_func_name = 'dice_loss', allres = True, tag = tag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Learner(data=ImageDataBunch;\n",
       "\n",
       "Train: LabelList (320 items)\n",
       "x: SegmentationItemList\n",
       "Image (3, 512, 512),Image (3, 512, 512),Image (3, 512, 512),Image (3, 512, 512),Image (3, 512, 512)\n",
       "y: SegmentationLabelList\n",
       "ImageSegment (1, 512, 512),ImageSegment (1, 512, 512),ImageSegment (1, 512, 512),ImageSegment (1, 512, 512),ImageSegment (1, 512, 512)\n",
       "Path: data/dataset_20200715_200_et/image;\n",
       "\n",
       "Valid: LabelList (40 items)\n",
       "x: SegmentationItemList\n",
       "Image (3, 512, 512),Image (3, 512, 512),Image (3, 512, 512),Image (3, 512, 512),Image (3, 512, 512)\n",
       "y: SegmentationLabelList\n",
       "ImageSegment (1, 512, 512),ImageSegment (1, 512, 512),ImageSegment (1, 512, 512),ImageSegment (1, 512, 512),ImageSegment (1, 512, 512)\n",
       "Path: data/dataset_20200715_200_et/image;\n",
       "\n",
       "Test: None, model=DataParallel(\n",
       "  (module): Resnet_UNet(\n",
       "    (down_blocks): ModuleList(\n",
       "      (0): Sequential(\n",
       "        (0): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "        (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU(inplace=True)\n",
       "      )\n",
       "      (1): Sequential(\n",
       "        (0): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "        (1): Sequential(\n",
       "          (0): BasicBlock(\n",
       "            (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "            (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (relu): ReLU(inplace=True)\n",
       "            (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "            (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "          (1): BasicBlock(\n",
       "            (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "            (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (relu): ReLU(inplace=True)\n",
       "            (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "            (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (2): Sequential(\n",
       "        (0): BasicBlock(\n",
       "          (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (downsample): Sequential(\n",
       "            (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "            (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (1): BasicBlock(\n",
       "          (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (3): Sequential(\n",
       "        (0): BasicBlock(\n",
       "          (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (downsample): Sequential(\n",
       "            (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "            (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (1): BasicBlock(\n",
       "          (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (4): Sequential(\n",
       "        (0): BasicBlock(\n",
       "          (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (downsample): Sequential(\n",
       "            (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "            (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (1): BasicBlock(\n",
       "          (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (bridge): Bridge(\n",
       "      (conv1): Conv_Bn_ReLu(\n",
       "        (conv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (bn): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (conv2): Conv_Bn_ReLu(\n",
       "        (conv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (bn): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (side_layers): ModuleList(\n",
       "      (0): Conv_Bn_ReLu(\n",
       "        (conv): Conv2d(3, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (1): Conv_Bn_ReLu(\n",
       "        (conv): Conv2d(64, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (2): Conv_Bn_ReLu(\n",
       "        (conv): Conv2d(64, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (3): Conv_Bn_ReLu(\n",
       "        (conv): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (4): Conv_Bn_ReLu(\n",
       "        (conv): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (up_blocks): ModuleList(\n",
       "      (0): UpBlock(\n",
       "        (upsample): Sequential(\n",
       "          (0): ConvTranspose2d(512, 256, kernel_size=(2, 2), stride=(2, 2), bias=False)\n",
       "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (conv1): Conv_Bn_ReLu(\n",
       "          (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (conv2): Conv_Bn_ReLu(\n",
       "          (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): UpBlock(\n",
       "        (upsample): Sequential(\n",
       "          (0): ConvTranspose2d(256, 128, kernel_size=(2, 2), stride=(2, 2), bias=False)\n",
       "          (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (conv1): Conv_Bn_ReLu(\n",
       "          (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (conv2): Conv_Bn_ReLu(\n",
       "          (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (2): UpBlock(\n",
       "        (upsample): Sequential(\n",
       "          (0): ConvTranspose2d(128, 128, kernel_size=(2, 2), stride=(2, 2), bias=False)\n",
       "          (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (conv1): Conv_Bn_ReLu(\n",
       "          (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (conv2): Conv_Bn_ReLu(\n",
       "          (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (3): UpBlock(\n",
       "        (upsample): Sequential(\n",
       "          (0): ConvTranspose2d(128, 128, kernel_size=(2, 2), stride=(2, 2), bias=False)\n",
       "          (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (conv1): Conv_Bn_ReLu(\n",
       "          (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (conv2): Conv_Bn_ReLu(\n",
       "          (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (4): UpBlock(\n",
       "        (upsample): Sequential(\n",
       "          (0): ConvTranspose2d(128, 128, kernel_size=(2, 2), stride=(2, 2), bias=False)\n",
       "          (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (conv1): Conv_Bn_ReLu(\n",
       "          (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (conv2): Conv_Bn_ReLu(\n",
       "          (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (head): Conv2d(128, 1, kernel_size=(1, 1), stride=(1, 1))\n",
       "  )\n",
       "), opt_func=functools.partial(<class 'torch.optim.adam.Adam'>, betas=(0.9, 0.99)), loss_func=<function dice_loss at 0x7f90e485f730>, metrics=[<function dice_loss at 0x7f90e485f730>, functools.partial(<function balance_bce at 0x7f90e485f840>, balance_ratio=1), <function mask_iou at 0x7f90e485f950>], true_wd=True, bn_wd=True, wd=0.01, train_bn=True, path=PosixPath('data/dataset_20200715_200_et/image'), model_dir='models', callback_fns=[functools.partial(<class 'fastai.basic_train.Recorder'>, add_time=True, silent=False)], callbacks=[], layer_groups=[ModuleList(\n",
       "  (0): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "  (1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "  (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "  (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "  (4): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "  (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "  (6): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "  (7): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "  (8): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "  (9): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "  (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "  (11): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "  (12): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "  (13): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "  (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "  (15): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "  (16): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "  (17): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "  (18): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "  (19): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "), ModuleList(\n",
       "  (0): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (5): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (6): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (7): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (8): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (9): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (10): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (11): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (12): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (13): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (14): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (15): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (16): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (17): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (18): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (19): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (20): Bridge(\n",
       "    (conv1): Conv_Bn_ReLu(\n",
       "      (conv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (bn): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (conv2): Conv_Bn_ReLu(\n",
       "      (conv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (bn): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (21): ModuleList(\n",
       "    (0): Conv_Bn_ReLu(\n",
       "      (conv): Conv2d(3, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (1): Conv_Bn_ReLu(\n",
       "      (conv): Conv2d(64, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (2): Conv_Bn_ReLu(\n",
       "      (conv): Conv2d(64, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (3): Conv_Bn_ReLu(\n",
       "      (conv): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (4): Conv_Bn_ReLu(\n",
       "      (conv): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (22): ModuleList(\n",
       "    (0): UpBlock(\n",
       "      (upsample): Sequential(\n",
       "        (0): ConvTranspose2d(512, 256, kernel_size=(2, 2), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (conv1): Conv_Bn_ReLu(\n",
       "        (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (conv2): Conv_Bn_ReLu(\n",
       "        (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): UpBlock(\n",
       "      (upsample): Sequential(\n",
       "        (0): ConvTranspose2d(256, 128, kernel_size=(2, 2), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (conv1): Conv_Bn_ReLu(\n",
       "        (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (conv2): Conv_Bn_ReLu(\n",
       "        (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (2): UpBlock(\n",
       "      (upsample): Sequential(\n",
       "        (0): ConvTranspose2d(128, 128, kernel_size=(2, 2), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (conv1): Conv_Bn_ReLu(\n",
       "        (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (conv2): Conv_Bn_ReLu(\n",
       "        (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (3): UpBlock(\n",
       "      (upsample): Sequential(\n",
       "        (0): ConvTranspose2d(128, 128, kernel_size=(2, 2), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (conv1): Conv_Bn_ReLu(\n",
       "        (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (conv2): Conv_Bn_ReLu(\n",
       "        (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (4): UpBlock(\n",
       "      (upsample): Sequential(\n",
       "        (0): ConvTranspose2d(128, 128, kernel_size=(2, 2), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (conv1): Conv_Bn_ReLu(\n",
       "        (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (conv2): Conv_Bn_ReLu(\n",
       "        (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (23): Conv2d(128, 1, kernel_size=(1, 1), stride=(1, 1))\n",
       ")], add_time=True, silent=False)"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "getlearn()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "                background: #F44336;\n",
       "            }\n",
       "        </style>\n",
       "      <progress value='4' class='' max='5', style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      80.00% [4/5 02:49<00:42]\n",
       "    </div>\n",
       "    \n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>dice_loss</th>\n",
       "      <th>balance_bce</th>\n",
       "      <th>mask_iou</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.742158</td>\n",
       "      <td>#na#</td>\n",
       "      <td>00:49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.701088</td>\n",
       "      <td>#na#</td>\n",
       "      <td>00:40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.615661</td>\n",
       "      <td>#na#</td>\n",
       "      <td>00:39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.539798</td>\n",
       "      <td>#na#</td>\n",
       "      <td>00:40</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>\n",
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "                background: #F44336;\n",
       "            }\n",
       "        </style>\n",
       "      <progress value='19' class='' max='20', style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      95.00% [19/20 00:37<00:01 0.5082]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LR Finder is complete, type {learner_name}.recorder.plot() to see the graph.\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "learn = getlearn()\n",
    "learn.opt_func = optf_adam\n",
    "learn.lr_find()\n",
    "learn.recorder.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ALLERT: You are using CumtomEpochLength, please make sure that your training dataloader is using random sampler, or this may cause problem.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "                background: #F44336;\n",
       "            }\n",
       "        </style>\n",
       "      <progress value='114' class='' max='500', style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      22.80% [114/500 1:20:28<4:32:28]\n",
       "    </div>\n",
       "    \n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>dice_loss</th>\n",
       "      <th>balance_bce</th>\n",
       "      <th>mask_iou</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.537512</td>\n",
       "      <td>0.673003</td>\n",
       "      <td>0.673003</td>\n",
       "      <td>0.577600</td>\n",
       "      <td>0.362814</td>\n",
       "      <td>00:43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.496501</td>\n",
       "      <td>0.480625</td>\n",
       "      <td>0.480625</td>\n",
       "      <td>0.695043</td>\n",
       "      <td>0.412130</td>\n",
       "      <td>00:42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.459644</td>\n",
       "      <td>0.328490</td>\n",
       "      <td>0.328490</td>\n",
       "      <td>0.692735</td>\n",
       "      <td>0.556367</td>\n",
       "      <td>00:42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.386793</td>\n",
       "      <td>0.250233</td>\n",
       "      <td>0.250233</td>\n",
       "      <td>0.835298</td>\n",
       "      <td>0.616732</td>\n",
       "      <td>00:42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.322920</td>\n",
       "      <td>0.470282</td>\n",
       "      <td>0.470282</td>\n",
       "      <td>1.543848</td>\n",
       "      <td>0.369673</td>\n",
       "      <td>00:43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.275181</td>\n",
       "      <td>0.311829</td>\n",
       "      <td>0.311829</td>\n",
       "      <td>1.338466</td>\n",
       "      <td>0.529710</td>\n",
       "      <td>00:43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.245175</td>\n",
       "      <td>0.246989</td>\n",
       "      <td>0.246989</td>\n",
       "      <td>1.640386</td>\n",
       "      <td>0.615743</td>\n",
       "      <td>00:42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.213537</td>\n",
       "      <td>0.194699</td>\n",
       "      <td>0.194699</td>\n",
       "      <td>0.543641</td>\n",
       "      <td>0.685413</td>\n",
       "      <td>00:42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.189658</td>\n",
       "      <td>0.174415</td>\n",
       "      <td>0.174415</td>\n",
       "      <td>1.038536</td>\n",
       "      <td>0.712113</td>\n",
       "      <td>00:42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.167762</td>\n",
       "      <td>0.140198</td>\n",
       "      <td>0.140198</td>\n",
       "      <td>0.751412</td>\n",
       "      <td>0.757339</td>\n",
       "      <td>00:42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.149000</td>\n",
       "      <td>0.142398</td>\n",
       "      <td>0.142398</td>\n",
       "      <td>0.862615</td>\n",
       "      <td>0.757829</td>\n",
       "      <td>00:42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.133091</td>\n",
       "      <td>0.114813</td>\n",
       "      <td>0.114813</td>\n",
       "      <td>0.396218</td>\n",
       "      <td>0.797411</td>\n",
       "      <td>00:42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.121950</td>\n",
       "      <td>0.131684</td>\n",
       "      <td>0.131684</td>\n",
       "      <td>0.871844</td>\n",
       "      <td>0.769334</td>\n",
       "      <td>00:42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.109521</td>\n",
       "      <td>0.087270</td>\n",
       "      <td>0.087270</td>\n",
       "      <td>0.453581</td>\n",
       "      <td>0.841434</td>\n",
       "      <td>00:42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.101651</td>\n",
       "      <td>0.087258</td>\n",
       "      <td>0.087258</td>\n",
       "      <td>0.529025</td>\n",
       "      <td>0.841129</td>\n",
       "      <td>00:42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.095786</td>\n",
       "      <td>0.098803</td>\n",
       "      <td>0.098803</td>\n",
       "      <td>0.338143</td>\n",
       "      <td>0.821647</td>\n",
       "      <td>00:42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.091437</td>\n",
       "      <td>0.093294</td>\n",
       "      <td>0.093294</td>\n",
       "      <td>0.382193</td>\n",
       "      <td>0.830388</td>\n",
       "      <td>00:42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.085938</td>\n",
       "      <td>0.102212</td>\n",
       "      <td>0.102212</td>\n",
       "      <td>0.673069</td>\n",
       "      <td>0.816458</td>\n",
       "      <td>00:42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.079358</td>\n",
       "      <td>0.071857</td>\n",
       "      <td>0.071857</td>\n",
       "      <td>0.426660</td>\n",
       "      <td>0.866828</td>\n",
       "      <td>00:42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.076869</td>\n",
       "      <td>0.077294</td>\n",
       "      <td>0.077294</td>\n",
       "      <td>0.300985</td>\n",
       "      <td>0.857323</td>\n",
       "      <td>00:42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.074539</td>\n",
       "      <td>0.073495</td>\n",
       "      <td>0.073495</td>\n",
       "      <td>0.535508</td>\n",
       "      <td>0.864015</td>\n",
       "      <td>00:42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>0.069785</td>\n",
       "      <td>0.067152</td>\n",
       "      <td>0.067152</td>\n",
       "      <td>0.334937</td>\n",
       "      <td>0.875200</td>\n",
       "      <td>00:41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>0.067939</td>\n",
       "      <td>0.080325</td>\n",
       "      <td>0.080325</td>\n",
       "      <td>0.571115</td>\n",
       "      <td>0.853020</td>\n",
       "      <td>00:42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>0.065237</td>\n",
       "      <td>0.169699</td>\n",
       "      <td>0.169699</td>\n",
       "      <td>1.585108</td>\n",
       "      <td>0.716294</td>\n",
       "      <td>00:42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>0.062757</td>\n",
       "      <td>0.072646</td>\n",
       "      <td>0.072646</td>\n",
       "      <td>0.371145</td>\n",
       "      <td>0.865682</td>\n",
       "      <td>00:42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>0.058062</td>\n",
       "      <td>0.057418</td>\n",
       "      <td>0.057418</td>\n",
       "      <td>0.344824</td>\n",
       "      <td>0.892157</td>\n",
       "      <td>00:42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>0.054525</td>\n",
       "      <td>0.057729</td>\n",
       "      <td>0.057729</td>\n",
       "      <td>0.352317</td>\n",
       "      <td>0.891702</td>\n",
       "      <td>00:42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>0.051173</td>\n",
       "      <td>0.081665</td>\n",
       "      <td>0.081665</td>\n",
       "      <td>0.413812</td>\n",
       "      <td>0.859655</td>\n",
       "      <td>00:42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28</td>\n",
       "      <td>0.049812</td>\n",
       "      <td>0.060754</td>\n",
       "      <td>0.060754</td>\n",
       "      <td>0.376157</td>\n",
       "      <td>0.886613</td>\n",
       "      <td>00:42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29</td>\n",
       "      <td>0.048030</td>\n",
       "      <td>0.052127</td>\n",
       "      <td>0.052127</td>\n",
       "      <td>0.319012</td>\n",
       "      <td>0.901690</td>\n",
       "      <td>00:42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>0.045853</td>\n",
       "      <td>0.054192</td>\n",
       "      <td>0.054192</td>\n",
       "      <td>0.315220</td>\n",
       "      <td>0.897543</td>\n",
       "      <td>00:42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31</td>\n",
       "      <td>0.046279</td>\n",
       "      <td>0.065032</td>\n",
       "      <td>0.065032</td>\n",
       "      <td>0.329573</td>\n",
       "      <td>0.878982</td>\n",
       "      <td>00:42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32</td>\n",
       "      <td>0.048306</td>\n",
       "      <td>0.125062</td>\n",
       "      <td>0.125062</td>\n",
       "      <td>0.963606</td>\n",
       "      <td>0.779358</td>\n",
       "      <td>00:42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33</td>\n",
       "      <td>0.049579</td>\n",
       "      <td>0.083124</td>\n",
       "      <td>0.083124</td>\n",
       "      <td>0.421723</td>\n",
       "      <td>0.847677</td>\n",
       "      <td>00:42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34</td>\n",
       "      <td>0.051270</td>\n",
       "      <td>0.074497</td>\n",
       "      <td>0.074497</td>\n",
       "      <td>0.678384</td>\n",
       "      <td>0.862272</td>\n",
       "      <td>00:42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35</td>\n",
       "      <td>0.052692</td>\n",
       "      <td>0.082629</td>\n",
       "      <td>0.082629</td>\n",
       "      <td>0.603057</td>\n",
       "      <td>0.847953</td>\n",
       "      <td>00:42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36</td>\n",
       "      <td>0.051110</td>\n",
       "      <td>0.057469</td>\n",
       "      <td>0.057469</td>\n",
       "      <td>0.464543</td>\n",
       "      <td>0.892076</td>\n",
       "      <td>00:42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>37</td>\n",
       "      <td>0.046910</td>\n",
       "      <td>0.054083</td>\n",
       "      <td>0.054083</td>\n",
       "      <td>0.425705</td>\n",
       "      <td>0.898225</td>\n",
       "      <td>00:42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>38</td>\n",
       "      <td>0.043376</td>\n",
       "      <td>0.054838</td>\n",
       "      <td>0.054838</td>\n",
       "      <td>0.431488</td>\n",
       "      <td>0.896421</td>\n",
       "      <td>00:42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>39</td>\n",
       "      <td>0.041064</td>\n",
       "      <td>0.049962</td>\n",
       "      <td>0.049962</td>\n",
       "      <td>0.374062</td>\n",
       "      <td>0.905611</td>\n",
       "      <td>00:42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>0.039098</td>\n",
       "      <td>0.050750</td>\n",
       "      <td>0.050750</td>\n",
       "      <td>0.396818</td>\n",
       "      <td>0.904130</td>\n",
       "      <td>00:42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>41</td>\n",
       "      <td>0.038180</td>\n",
       "      <td>0.052130</td>\n",
       "      <td>0.052130</td>\n",
       "      <td>0.383263</td>\n",
       "      <td>0.901694</td>\n",
       "      <td>00:42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>42</td>\n",
       "      <td>0.036506</td>\n",
       "      <td>0.054058</td>\n",
       "      <td>0.054058</td>\n",
       "      <td>0.421529</td>\n",
       "      <td>0.897890</td>\n",
       "      <td>00:42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>43</td>\n",
       "      <td>0.035241</td>\n",
       "      <td>0.052716</td>\n",
       "      <td>0.052716</td>\n",
       "      <td>0.402459</td>\n",
       "      <td>0.900373</td>\n",
       "      <td>00:42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>44</td>\n",
       "      <td>0.034267</td>\n",
       "      <td>0.052097</td>\n",
       "      <td>0.052097</td>\n",
       "      <td>0.417279</td>\n",
       "      <td>0.901563</td>\n",
       "      <td>00:42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>45</td>\n",
       "      <td>0.033698</td>\n",
       "      <td>0.049037</td>\n",
       "      <td>0.049037</td>\n",
       "      <td>0.370658</td>\n",
       "      <td>0.907259</td>\n",
       "      <td>00:42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>46</td>\n",
       "      <td>0.033087</td>\n",
       "      <td>0.051267</td>\n",
       "      <td>0.051267</td>\n",
       "      <td>0.409172</td>\n",
       "      <td>0.903109</td>\n",
       "      <td>00:42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>47</td>\n",
       "      <td>0.032569</td>\n",
       "      <td>0.048550</td>\n",
       "      <td>0.048550</td>\n",
       "      <td>0.376646</td>\n",
       "      <td>0.908065</td>\n",
       "      <td>00:42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>48</td>\n",
       "      <td>0.032074</td>\n",
       "      <td>0.048253</td>\n",
       "      <td>0.048253</td>\n",
       "      <td>0.394201</td>\n",
       "      <td>0.908488</td>\n",
       "      <td>00:41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>49</td>\n",
       "      <td>0.032216</td>\n",
       "      <td>0.049134</td>\n",
       "      <td>0.049134</td>\n",
       "      <td>0.429432</td>\n",
       "      <td>0.906933</td>\n",
       "      <td>00:42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.031895</td>\n",
       "      <td>0.047864</td>\n",
       "      <td>0.047864</td>\n",
       "      <td>0.397535</td>\n",
       "      <td>0.909211</td>\n",
       "      <td>00:42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>51</td>\n",
       "      <td>0.031763</td>\n",
       "      <td>0.046812</td>\n",
       "      <td>0.046812</td>\n",
       "      <td>0.363449</td>\n",
       "      <td>0.911067</td>\n",
       "      <td>00:42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>52</td>\n",
       "      <td>0.031189</td>\n",
       "      <td>0.048448</td>\n",
       "      <td>0.048448</td>\n",
       "      <td>0.394266</td>\n",
       "      <td>0.908283</td>\n",
       "      <td>00:42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>53</td>\n",
       "      <td>0.031254</td>\n",
       "      <td>0.046459</td>\n",
       "      <td>0.046459</td>\n",
       "      <td>0.378222</td>\n",
       "      <td>0.911769</td>\n",
       "      <td>00:42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>54</td>\n",
       "      <td>0.030996</td>\n",
       "      <td>0.046974</td>\n",
       "      <td>0.046974</td>\n",
       "      <td>0.378254</td>\n",
       "      <td>0.910892</td>\n",
       "      <td>00:42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>55</td>\n",
       "      <td>0.030573</td>\n",
       "      <td>0.046282</td>\n",
       "      <td>0.046282</td>\n",
       "      <td>0.315979</td>\n",
       "      <td>0.912162</td>\n",
       "      <td>00:42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>56</td>\n",
       "      <td>0.030591</td>\n",
       "      <td>0.047217</td>\n",
       "      <td>0.047217</td>\n",
       "      <td>0.380256</td>\n",
       "      <td>0.910493</td>\n",
       "      <td>00:41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>57</td>\n",
       "      <td>0.030299</td>\n",
       "      <td>0.048707</td>\n",
       "      <td>0.048707</td>\n",
       "      <td>0.407058</td>\n",
       "      <td>0.907693</td>\n",
       "      <td>00:42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>58</td>\n",
       "      <td>0.029752</td>\n",
       "      <td>0.048284</td>\n",
       "      <td>0.048284</td>\n",
       "      <td>0.392774</td>\n",
       "      <td>0.908575</td>\n",
       "      <td>00:41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>59</td>\n",
       "      <td>0.029285</td>\n",
       "      <td>0.046266</td>\n",
       "      <td>0.046266</td>\n",
       "      <td>0.383041</td>\n",
       "      <td>0.912175</td>\n",
       "      <td>00:42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>0.028738</td>\n",
       "      <td>0.046185</td>\n",
       "      <td>0.046185</td>\n",
       "      <td>0.373631</td>\n",
       "      <td>0.912338</td>\n",
       "      <td>00:42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>61</td>\n",
       "      <td>0.028302</td>\n",
       "      <td>0.046715</td>\n",
       "      <td>0.046715</td>\n",
       "      <td>0.385205</td>\n",
       "      <td>0.911251</td>\n",
       "      <td>00:42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>62</td>\n",
       "      <td>0.027959</td>\n",
       "      <td>0.046926</td>\n",
       "      <td>0.046926</td>\n",
       "      <td>0.365369</td>\n",
       "      <td>0.910989</td>\n",
       "      <td>00:42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>63</td>\n",
       "      <td>0.027966</td>\n",
       "      <td>0.046530</td>\n",
       "      <td>0.046530</td>\n",
       "      <td>0.384358</td>\n",
       "      <td>0.911748</td>\n",
       "      <td>00:42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>64</td>\n",
       "      <td>0.027715</td>\n",
       "      <td>0.047420</td>\n",
       "      <td>0.047420</td>\n",
       "      <td>0.393803</td>\n",
       "      <td>0.910042</td>\n",
       "      <td>00:42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>65</td>\n",
       "      <td>0.027265</td>\n",
       "      <td>0.047285</td>\n",
       "      <td>0.047285</td>\n",
       "      <td>0.427928</td>\n",
       "      <td>0.910187</td>\n",
       "      <td>00:42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>66</td>\n",
       "      <td>0.027201</td>\n",
       "      <td>0.045975</td>\n",
       "      <td>0.045975</td>\n",
       "      <td>0.386704</td>\n",
       "      <td>0.912629</td>\n",
       "      <td>00:41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>67</td>\n",
       "      <td>0.026588</td>\n",
       "      <td>0.045821</td>\n",
       "      <td>0.045821</td>\n",
       "      <td>0.392812</td>\n",
       "      <td>0.912929</td>\n",
       "      <td>00:42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>68</td>\n",
       "      <td>0.026781</td>\n",
       "      <td>0.045250</td>\n",
       "      <td>0.045250</td>\n",
       "      <td>0.346448</td>\n",
       "      <td>0.913870</td>\n",
       "      <td>00:41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>69</td>\n",
       "      <td>0.026403</td>\n",
       "      <td>0.045754</td>\n",
       "      <td>0.045754</td>\n",
       "      <td>0.372494</td>\n",
       "      <td>0.913082</td>\n",
       "      <td>00:42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>0.026573</td>\n",
       "      <td>0.044178</td>\n",
       "      <td>0.044178</td>\n",
       "      <td>0.348343</td>\n",
       "      <td>0.915912</td>\n",
       "      <td>00:42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>71</td>\n",
       "      <td>0.027229</td>\n",
       "      <td>0.043501</td>\n",
       "      <td>0.043501</td>\n",
       "      <td>0.353268</td>\n",
       "      <td>0.917226</td>\n",
       "      <td>00:41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>72</td>\n",
       "      <td>0.026871</td>\n",
       "      <td>0.044647</td>\n",
       "      <td>0.044647</td>\n",
       "      <td>0.403028</td>\n",
       "      <td>0.915012</td>\n",
       "      <td>00:42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>73</td>\n",
       "      <td>0.026386</td>\n",
       "      <td>0.043502</td>\n",
       "      <td>0.043502</td>\n",
       "      <td>0.350475</td>\n",
       "      <td>0.917072</td>\n",
       "      <td>00:42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>74</td>\n",
       "      <td>0.025840</td>\n",
       "      <td>0.043760</td>\n",
       "      <td>0.043760</td>\n",
       "      <td>0.357612</td>\n",
       "      <td>0.916769</td>\n",
       "      <td>00:42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>75</td>\n",
       "      <td>0.025302</td>\n",
       "      <td>0.044180</td>\n",
       "      <td>0.044180</td>\n",
       "      <td>0.390416</td>\n",
       "      <td>0.915916</td>\n",
       "      <td>00:42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>76</td>\n",
       "      <td>0.024916</td>\n",
       "      <td>0.043892</td>\n",
       "      <td>0.043892</td>\n",
       "      <td>0.367000</td>\n",
       "      <td>0.916389</td>\n",
       "      <td>00:41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>77</td>\n",
       "      <td>0.024659</td>\n",
       "      <td>0.043230</td>\n",
       "      <td>0.043230</td>\n",
       "      <td>0.389384</td>\n",
       "      <td>0.917558</td>\n",
       "      <td>00:42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>78</td>\n",
       "      <td>0.024359</td>\n",
       "      <td>0.044294</td>\n",
       "      <td>0.044294</td>\n",
       "      <td>0.401054</td>\n",
       "      <td>0.915681</td>\n",
       "      <td>00:41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>79</td>\n",
       "      <td>0.024902</td>\n",
       "      <td>0.043423</td>\n",
       "      <td>0.043423</td>\n",
       "      <td>0.351948</td>\n",
       "      <td>0.917248</td>\n",
       "      <td>00:42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>0.024869</td>\n",
       "      <td>0.041832</td>\n",
       "      <td>0.041832</td>\n",
       "      <td>0.366323</td>\n",
       "      <td>0.920105</td>\n",
       "      <td>00:42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>81</td>\n",
       "      <td>0.024778</td>\n",
       "      <td>0.043089</td>\n",
       "      <td>0.043089</td>\n",
       "      <td>0.378733</td>\n",
       "      <td>0.917838</td>\n",
       "      <td>00:42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>82</td>\n",
       "      <td>0.024886</td>\n",
       "      <td>0.044641</td>\n",
       "      <td>0.044641</td>\n",
       "      <td>0.380700</td>\n",
       "      <td>0.915144</td>\n",
       "      <td>00:42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>83</td>\n",
       "      <td>0.024542</td>\n",
       "      <td>0.044967</td>\n",
       "      <td>0.044967</td>\n",
       "      <td>0.422438</td>\n",
       "      <td>0.914522</td>\n",
       "      <td>00:41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>84</td>\n",
       "      <td>0.024486</td>\n",
       "      <td>0.043822</td>\n",
       "      <td>0.043822</td>\n",
       "      <td>0.399702</td>\n",
       "      <td>0.916469</td>\n",
       "      <td>00:42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>85</td>\n",
       "      <td>0.024242</td>\n",
       "      <td>0.044482</td>\n",
       "      <td>0.044482</td>\n",
       "      <td>0.399748</td>\n",
       "      <td>0.915348</td>\n",
       "      <td>00:42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>86</td>\n",
       "      <td>0.023958</td>\n",
       "      <td>0.044029</td>\n",
       "      <td>0.044029</td>\n",
       "      <td>0.369517</td>\n",
       "      <td>0.916215</td>\n",
       "      <td>00:41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>87</td>\n",
       "      <td>0.024003</td>\n",
       "      <td>0.043359</td>\n",
       "      <td>0.043359</td>\n",
       "      <td>0.368315</td>\n",
       "      <td>0.917312</td>\n",
       "      <td>00:42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>88</td>\n",
       "      <td>0.023986</td>\n",
       "      <td>0.043778</td>\n",
       "      <td>0.043778</td>\n",
       "      <td>0.373634</td>\n",
       "      <td>0.916623</td>\n",
       "      <td>00:42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>89</td>\n",
       "      <td>0.023824</td>\n",
       "      <td>0.043672</td>\n",
       "      <td>0.043672</td>\n",
       "      <td>0.366026</td>\n",
       "      <td>0.916798</td>\n",
       "      <td>00:42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>0.023448</td>\n",
       "      <td>0.044067</td>\n",
       "      <td>0.044067</td>\n",
       "      <td>0.389647</td>\n",
       "      <td>0.916181</td>\n",
       "      <td>00:42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>91</td>\n",
       "      <td>0.023954</td>\n",
       "      <td>0.043528</td>\n",
       "      <td>0.043528</td>\n",
       "      <td>0.376193</td>\n",
       "      <td>0.917024</td>\n",
       "      <td>00:41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>92</td>\n",
       "      <td>0.023717</td>\n",
       "      <td>0.043250</td>\n",
       "      <td>0.043250</td>\n",
       "      <td>0.400822</td>\n",
       "      <td>0.917611</td>\n",
       "      <td>00:41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>93</td>\n",
       "      <td>0.023506</td>\n",
       "      <td>0.042899</td>\n",
       "      <td>0.042899</td>\n",
       "      <td>0.370108</td>\n",
       "      <td>0.918236</td>\n",
       "      <td>00:42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>94</td>\n",
       "      <td>0.023473</td>\n",
       "      <td>0.043903</td>\n",
       "      <td>0.043903</td>\n",
       "      <td>0.394788</td>\n",
       "      <td>0.916432</td>\n",
       "      <td>00:42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>95</td>\n",
       "      <td>0.023513</td>\n",
       "      <td>0.043538</td>\n",
       "      <td>0.043538</td>\n",
       "      <td>0.398900</td>\n",
       "      <td>0.917073</td>\n",
       "      <td>00:42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>96</td>\n",
       "      <td>0.023281</td>\n",
       "      <td>0.043369</td>\n",
       "      <td>0.043369</td>\n",
       "      <td>0.386815</td>\n",
       "      <td>0.917313</td>\n",
       "      <td>00:42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>97</td>\n",
       "      <td>0.023399</td>\n",
       "      <td>0.043507</td>\n",
       "      <td>0.043507</td>\n",
       "      <td>0.397632</td>\n",
       "      <td>0.917100</td>\n",
       "      <td>00:42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>98</td>\n",
       "      <td>0.023286</td>\n",
       "      <td>0.043488</td>\n",
       "      <td>0.043488</td>\n",
       "      <td>0.378514</td>\n",
       "      <td>0.917138</td>\n",
       "      <td>00:42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>99</td>\n",
       "      <td>0.023213</td>\n",
       "      <td>0.044118</td>\n",
       "      <td>0.044118</td>\n",
       "      <td>0.423355</td>\n",
       "      <td>0.915977</td>\n",
       "      <td>00:42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.022924</td>\n",
       "      <td>0.043421</td>\n",
       "      <td>0.043421</td>\n",
       "      <td>0.400006</td>\n",
       "      <td>0.917260</td>\n",
       "      <td>00:42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>101</td>\n",
       "      <td>0.022763</td>\n",
       "      <td>0.042581</td>\n",
       "      <td>0.042581</td>\n",
       "      <td>0.372410</td>\n",
       "      <td>0.918811</td>\n",
       "      <td>00:42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>102</td>\n",
       "      <td>0.022635</td>\n",
       "      <td>0.043263</td>\n",
       "      <td>0.043263</td>\n",
       "      <td>0.392477</td>\n",
       "      <td>0.917556</td>\n",
       "      <td>00:42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>103</td>\n",
       "      <td>0.022524</td>\n",
       "      <td>0.042678</td>\n",
       "      <td>0.042678</td>\n",
       "      <td>0.389399</td>\n",
       "      <td>0.918700</td>\n",
       "      <td>00:42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>104</td>\n",
       "      <td>0.022440</td>\n",
       "      <td>0.042564</td>\n",
       "      <td>0.042564</td>\n",
       "      <td>0.374259</td>\n",
       "      <td>0.918773</td>\n",
       "      <td>00:42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>105</td>\n",
       "      <td>0.022436</td>\n",
       "      <td>0.043074</td>\n",
       "      <td>0.043074</td>\n",
       "      <td>0.377925</td>\n",
       "      <td>0.917860</td>\n",
       "      <td>00:42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>106</td>\n",
       "      <td>0.022870</td>\n",
       "      <td>0.041728</td>\n",
       "      <td>0.041728</td>\n",
       "      <td>0.376461</td>\n",
       "      <td>0.920290</td>\n",
       "      <td>00:42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>107</td>\n",
       "      <td>0.022930</td>\n",
       "      <td>0.042499</td>\n",
       "      <td>0.042499</td>\n",
       "      <td>0.361777</td>\n",
       "      <td>0.918928</td>\n",
       "      <td>00:42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>108</td>\n",
       "      <td>0.022715</td>\n",
       "      <td>0.042740</td>\n",
       "      <td>0.042740</td>\n",
       "      <td>0.375764</td>\n",
       "      <td>0.918499</td>\n",
       "      <td>00:42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>109</td>\n",
       "      <td>0.022763</td>\n",
       "      <td>0.042419</td>\n",
       "      <td>0.042419</td>\n",
       "      <td>0.384959</td>\n",
       "      <td>0.919109</td>\n",
       "      <td>00:42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>110</td>\n",
       "      <td>0.022875</td>\n",
       "      <td>0.042941</td>\n",
       "      <td>0.042941</td>\n",
       "      <td>0.372413</td>\n",
       "      <td>0.918170</td>\n",
       "      <td>00:42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>111</td>\n",
       "      <td>0.023314</td>\n",
       "      <td>0.042704</td>\n",
       "      <td>0.042704</td>\n",
       "      <td>0.387881</td>\n",
       "      <td>0.918597</td>\n",
       "      <td>00:42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>112</td>\n",
       "      <td>0.023098</td>\n",
       "      <td>0.042789</td>\n",
       "      <td>0.042789</td>\n",
       "      <td>0.390938</td>\n",
       "      <td>0.918377</td>\n",
       "      <td>00:42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>113</td>\n",
       "      <td>0.022900</td>\n",
       "      <td>0.043016</td>\n",
       "      <td>0.043016</td>\n",
       "      <td>0.392881</td>\n",
       "      <td>0.918029</td>\n",
       "      <td>00:42</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>\n",
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "                background: #F44336;\n",
       "            }\n",
       "        </style>\n",
       "      <progress value='3' class='' max='3', style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      100.00% [3/3 00:02<00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Better model found at epoch 0 with valid_loss value: 0.6730028986930847.\n",
      "Better model found at epoch 1 with valid_loss value: 0.4806247651576996.\n",
      "Better model found at epoch 2 with valid_loss value: 0.3284897804260254.\n",
      "Better model found at epoch 3 with valid_loss value: 0.2502334713935852.\n",
      "Better model found at epoch 6 with valid_loss value: 0.24698877334594727.\n",
      "Better model found at epoch 7 with valid_loss value: 0.19469943642616272.\n",
      "Better model found at epoch 8 with valid_loss value: 0.17441478371620178.\n",
      "Better model found at epoch 9 with valid_loss value: 0.1401984989643097.\n",
      "Better model found at epoch 11 with valid_loss value: 0.11481285095214844.\n",
      "Better model found at epoch 13 with valid_loss value: 0.0872698649764061.\n",
      "Better model found at epoch 14 with valid_loss value: 0.08725808560848236.\n",
      "Better model found at epoch 18 with valid_loss value: 0.07185699045658112.\n",
      "Better model found at epoch 21 with valid_loss value: 0.06715203821659088.\n",
      "Better model found at epoch 25 with valid_loss value: 0.05741802603006363.\n",
      "Better model found at epoch 29 with valid_loss value: 0.05212687328457832.\n",
      "on end of epoch#35: start annealing from 0.001 to 0.0001\n",
      "Better model found at epoch 39 with valid_loss value: 0.04996156692504883.\n",
      "Better model found at epoch 45 with valid_loss value: 0.04903696849942207.\n",
      "Better model found at epoch 47 with valid_loss value: 0.04854973405599594.\n",
      "Better model found at epoch 48 with valid_loss value: 0.04825303703546524.\n",
      "Better model found at epoch 50 with valid_loss value: 0.047863997519016266.\n",
      "Better model found at epoch 51 with valid_loss value: 0.046811867505311966.\n",
      "Better model found at epoch 53 with valid_loss value: 0.046459149569272995.\n",
      "Better model found at epoch 55 with valid_loss value: 0.046282269060611725.\n",
      "Better model found at epoch 59 with valid_loss value: 0.0462663397192955.\n",
      "Better model found at epoch 60 with valid_loss value: 0.04618488624691963.\n",
      "Better model found at epoch 66 with valid_loss value: 0.04597511142492294.\n",
      "Better model found at epoch 67 with valid_loss value: 0.045821405947208405.\n",
      "Better model found at epoch 68 with valid_loss value: 0.04525049775838852.\n",
      "Better model found at epoch 70 with valid_loss value: 0.04417797178030014.\n",
      "Better model found at epoch 71 with valid_loss value: 0.043501030653715134.\n",
      "Better model found at epoch 77 with valid_loss value: 0.0432300791144371.\n",
      "Better model found at epoch 80 with valid_loss value: 0.04183156415820122.\n",
      "on end of epoch#83: start annealing from 0.0001 to 1e-05\n",
      "Better model found at epoch 106 with valid_loss value: 0.04172809049487114.\n",
      "on end of epoch#113: start annealing from 1e-05 to 1.0000000000000002e-06\n"
     ]
    }
   ],
   "source": [
    "lrs = [1e-3]\n",
    "checkpoints = [None]\n",
    "opts = [optf_adam]\n",
    "\n",
    "nb_train_script_logger.multi_train(get_learn=getlearn, \n",
    "            epoch_len=1e9, epochs=500,\n",
    "            opts=opts, lrs=lrs, checkpoints=checkpoints,\n",
    "            tb_log_root='./tb_log/',\n",
    "            autoSave=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "backup_train_logs(tag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "epochs:113,train_loss:0.023,mask_iou:0.918"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 阶段结果：\n",
    "2000数据集效果最好，mask_iou 0.984.在28轮的时候就到了0.955，耗时约112min。143轮到达0.98，耗时约572min。最后在256轮的时候停止。  \n",
    "\n",
    "200数据集mask_iou 0.922,240轮，耗时约88min。  \n",
    "\n",
    "200+et(elastic transform)数据集，mask_iou 0.918,113轮，耗时约79min。\n",
    "\n",
    "其中200+et数据集虽然数量多于200数据集，但是效果并没有更好，不过收敛要略快一些。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 比较allres和vanila"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 200数据集，无transform，resnet18，vanila，dice，adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "tag = '__'.join(['resnet18', 'vanila', 'dice_loss', 'dataset200','adam'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "getlearn = partial(get_learn, data = data_200, model_name = 'resnet18'\n",
    "                   , loss_func_name = 'dice_loss', allres = False, tag = tag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Learner(data=ImageDataBunch;\n",
       "\n",
       "Train: LabelList (160 items)\n",
       "x: SegmentationItemList\n",
       "Image (3, 512, 512),Image (3, 512, 512),Image (3, 512, 512),Image (3, 512, 512),Image (3, 512, 512)\n",
       "y: SegmentationLabelList\n",
       "ImageSegment (1, 512, 512),ImageSegment (1, 512, 512),ImageSegment (1, 512, 512),ImageSegment (1, 512, 512),ImageSegment (1, 512, 512)\n",
       "Path: data/dataset_20200715/image;\n",
       "\n",
       "Valid: LabelList (40 items)\n",
       "x: SegmentationItemList\n",
       "Image (3, 512, 512),Image (3, 512, 512),Image (3, 512, 512),Image (3, 512, 512),Image (3, 512, 512)\n",
       "y: SegmentationLabelList\n",
       "ImageSegment (1, 512, 512),ImageSegment (1, 512, 512),ImageSegment (1, 512, 512),ImageSegment (1, 512, 512),ImageSegment (1, 512, 512)\n",
       "Path: data/dataset_20200715/image;\n",
       "\n",
       "Test: None, model=DataParallel(\n",
       "  (module): Resnet_UNet(\n",
       "    (down_blocks): ModuleList(\n",
       "      (0): Sequential(\n",
       "        (0): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "        (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU(inplace=True)\n",
       "      )\n",
       "      (1): Sequential(\n",
       "        (0): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "        (1): Sequential(\n",
       "          (0): BasicBlock(\n",
       "            (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "            (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (relu): ReLU(inplace=True)\n",
       "            (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "            (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "          (1): BasicBlock(\n",
       "            (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "            (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (relu): ReLU(inplace=True)\n",
       "            (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "            (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (2): Sequential(\n",
       "        (0): BasicBlock(\n",
       "          (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (downsample): Sequential(\n",
       "            (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "            (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (1): BasicBlock(\n",
       "          (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (3): Sequential(\n",
       "        (0): BasicBlock(\n",
       "          (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (downsample): Sequential(\n",
       "            (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "            (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (1): BasicBlock(\n",
       "          (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (4): Sequential(\n",
       "        (0): BasicBlock(\n",
       "          (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (downsample): Sequential(\n",
       "            (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "            (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (1): BasicBlock(\n",
       "          (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (bridge): Bridge(\n",
       "      (conv1): Conv_Bn_ReLu(\n",
       "        (conv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (bn): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (conv2): Conv_Bn_ReLu(\n",
       "        (conv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (bn): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (up_blocks): ModuleList(\n",
       "      (0): UpBlock(\n",
       "        (upsample): Sequential(\n",
       "          (0): ConvTranspose2d(512, 256, kernel_size=(2, 2), stride=(2, 2), bias=False)\n",
       "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (conv1): Conv_Bn_ReLu(\n",
       "          (conv): Conv2d(512, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (conv2): Conv_Bn_ReLu(\n",
       "          (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): UpBlock(\n",
       "        (upsample): Sequential(\n",
       "          (0): ConvTranspose2d(256, 128, kernel_size=(2, 2), stride=(2, 2), bias=False)\n",
       "          (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (conv1): Conv_Bn_ReLu(\n",
       "          (conv): Conv2d(256, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (conv2): Conv_Bn_ReLu(\n",
       "          (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (2): UpBlock(\n",
       "        (upsample): Sequential(\n",
       "          (0): ConvTranspose2d(128, 128, kernel_size=(2, 2), stride=(2, 2), bias=False)\n",
       "          (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (conv1): Conv_Bn_ReLu(\n",
       "          (conv): Conv2d(192, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (conv2): Conv_Bn_ReLu(\n",
       "          (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (3): UpBlock(\n",
       "        (upsample): Sequential(\n",
       "          (0): ConvTranspose2d(128, 128, kernel_size=(2, 2), stride=(2, 2), bias=False)\n",
       "          (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (conv1): Conv_Bn_ReLu(\n",
       "          (conv): Conv2d(192, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (conv2): Conv_Bn_ReLu(\n",
       "          (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (4): UpBlock(\n",
       "        (upsample): Sequential(\n",
       "          (0): ConvTranspose2d(128, 128, kernel_size=(2, 2), stride=(2, 2), bias=False)\n",
       "          (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (conv1): Conv_Bn_ReLu(\n",
       "          (conv): Conv2d(131, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (conv2): Conv_Bn_ReLu(\n",
       "          (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (head): Conv2d(128, 1, kernel_size=(1, 1), stride=(1, 1))\n",
       "  )\n",
       "), opt_func=functools.partial(<class 'torch.optim.adam.Adam'>, betas=(0.9, 0.99)), loss_func=<function dice_loss at 0x7f90e485f730>, metrics=[<function dice_loss at 0x7f90e485f730>, functools.partial(<function balance_bce at 0x7f90e485f840>, balance_ratio=1), <function mask_iou at 0x7f90e485f950>], true_wd=True, bn_wd=True, wd=0.01, train_bn=True, path=PosixPath('data/dataset_20200715/image'), model_dir='models', callback_fns=[functools.partial(<class 'fastai.basic_train.Recorder'>, add_time=True, silent=False)], callbacks=[], layer_groups=[ModuleList(\n",
       "  (0): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "  (1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "  (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "  (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "  (4): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "  (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "  (6): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "  (7): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "  (8): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "  (9): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "  (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "  (11): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "  (12): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "  (13): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "  (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "  (15): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "  (16): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "  (17): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "  (18): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "  (19): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "), ModuleList(\n",
       "  (0): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (5): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (6): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (7): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (8): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (9): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (10): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (11): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (12): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (13): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (14): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (15): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (16): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (17): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (18): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (19): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (20): Bridge(\n",
       "    (conv1): Conv_Bn_ReLu(\n",
       "      (conv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (bn): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (conv2): Conv_Bn_ReLu(\n",
       "      (conv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (bn): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (21): ModuleList(\n",
       "    (0): UpBlock(\n",
       "      (upsample): Sequential(\n",
       "        (0): ConvTranspose2d(512, 256, kernel_size=(2, 2), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (conv1): Conv_Bn_ReLu(\n",
       "        (conv): Conv2d(512, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (conv2): Conv_Bn_ReLu(\n",
       "        (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): UpBlock(\n",
       "      (upsample): Sequential(\n",
       "        (0): ConvTranspose2d(256, 128, kernel_size=(2, 2), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (conv1): Conv_Bn_ReLu(\n",
       "        (conv): Conv2d(256, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (conv2): Conv_Bn_ReLu(\n",
       "        (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (2): UpBlock(\n",
       "      (upsample): Sequential(\n",
       "        (0): ConvTranspose2d(128, 128, kernel_size=(2, 2), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (conv1): Conv_Bn_ReLu(\n",
       "        (conv): Conv2d(192, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (conv2): Conv_Bn_ReLu(\n",
       "        (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (3): UpBlock(\n",
       "      (upsample): Sequential(\n",
       "        (0): ConvTranspose2d(128, 128, kernel_size=(2, 2), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (conv1): Conv_Bn_ReLu(\n",
       "        (conv): Conv2d(192, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (conv2): Conv_Bn_ReLu(\n",
       "        (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (4): UpBlock(\n",
       "      (upsample): Sequential(\n",
       "        (0): ConvTranspose2d(128, 128, kernel_size=(2, 2), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (conv1): Conv_Bn_ReLu(\n",
       "        (conv): Conv2d(131, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (conv2): Conv_Bn_ReLu(\n",
       "        (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (22): Conv2d(128, 1, kernel_size=(1, 1), stride=(1, 1))\n",
       ")], add_time=True, silent=False)"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "getlearn()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ALLERT: You are using CumtomEpochLength, please make sure that your training dataloader is using random sampler, or this may cause problem.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "                background: #F44336;\n",
       "            }\n",
       "        </style>\n",
       "      <progress value='145' class='' max='500', style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      29.00% [145/500 57:12<2:20:03]\n",
       "    </div>\n",
       "    \n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>dice_loss</th>\n",
       "      <th>balance_bce</th>\n",
       "      <th>mask_iou</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.576685</td>\n",
       "      <td>0.772869</td>\n",
       "      <td>0.772869</td>\n",
       "      <td>0.801760</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>00:33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.399787</td>\n",
       "      <td>0.819375</td>\n",
       "      <td>0.819375</td>\n",
       "      <td>0.978611</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>00:23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.318971</td>\n",
       "      <td>0.912573</td>\n",
       "      <td>0.912573</td>\n",
       "      <td>1.469705</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>00:23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.281842</td>\n",
       "      <td>0.818690</td>\n",
       "      <td>0.818690</td>\n",
       "      <td>1.331423</td>\n",
       "      <td>0.000515</td>\n",
       "      <td>00:24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.260006</td>\n",
       "      <td>0.183279</td>\n",
       "      <td>0.183279</td>\n",
       "      <td>0.487018</td>\n",
       "      <td>0.712198</td>\n",
       "      <td>00:24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.232149</td>\n",
       "      <td>0.203520</td>\n",
       "      <td>0.203520</td>\n",
       "      <td>0.738799</td>\n",
       "      <td>0.667679</td>\n",
       "      <td>00:23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.205030</td>\n",
       "      <td>0.097140</td>\n",
       "      <td>0.097140</td>\n",
       "      <td>0.332182</td>\n",
       "      <td>0.830428</td>\n",
       "      <td>00:23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.183030</td>\n",
       "      <td>0.253476</td>\n",
       "      <td>0.253476</td>\n",
       "      <td>1.106604</td>\n",
       "      <td>0.602793</td>\n",
       "      <td>00:23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.167124</td>\n",
       "      <td>0.156643</td>\n",
       "      <td>0.156643</td>\n",
       "      <td>0.685680</td>\n",
       "      <td>0.751681</td>\n",
       "      <td>00:23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.152730</td>\n",
       "      <td>0.180903</td>\n",
       "      <td>0.180903</td>\n",
       "      <td>0.601902</td>\n",
       "      <td>0.696833</td>\n",
       "      <td>00:23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.147748</td>\n",
       "      <td>0.176775</td>\n",
       "      <td>0.176775</td>\n",
       "      <td>0.876502</td>\n",
       "      <td>0.704222</td>\n",
       "      <td>00:23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.143753</td>\n",
       "      <td>0.131257</td>\n",
       "      <td>0.131257</td>\n",
       "      <td>0.725687</td>\n",
       "      <td>0.771946</td>\n",
       "      <td>00:23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.136340</td>\n",
       "      <td>0.087709</td>\n",
       "      <td>0.087709</td>\n",
       "      <td>0.413711</td>\n",
       "      <td>0.840304</td>\n",
       "      <td>00:23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.132459</td>\n",
       "      <td>0.154257</td>\n",
       "      <td>0.154257</td>\n",
       "      <td>0.864113</td>\n",
       "      <td>0.736922</td>\n",
       "      <td>00:23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.124217</td>\n",
       "      <td>0.089363</td>\n",
       "      <td>0.089363</td>\n",
       "      <td>0.300511</td>\n",
       "      <td>0.840050</td>\n",
       "      <td>00:23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.118039</td>\n",
       "      <td>0.082843</td>\n",
       "      <td>0.082843</td>\n",
       "      <td>0.393166</td>\n",
       "      <td>0.849943</td>\n",
       "      <td>00:23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.110889</td>\n",
       "      <td>0.079224</td>\n",
       "      <td>0.079224</td>\n",
       "      <td>0.332953</td>\n",
       "      <td>0.862611</td>\n",
       "      <td>00:23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.104318</td>\n",
       "      <td>0.086479</td>\n",
       "      <td>0.086479</td>\n",
       "      <td>0.347234</td>\n",
       "      <td>0.845634</td>\n",
       "      <td>00:23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.097997</td>\n",
       "      <td>0.076370</td>\n",
       "      <td>0.076370</td>\n",
       "      <td>0.316137</td>\n",
       "      <td>0.861049</td>\n",
       "      <td>00:23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.092066</td>\n",
       "      <td>0.076390</td>\n",
       "      <td>0.076390</td>\n",
       "      <td>0.308122</td>\n",
       "      <td>0.859997</td>\n",
       "      <td>00:23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.087618</td>\n",
       "      <td>0.074166</td>\n",
       "      <td>0.074166</td>\n",
       "      <td>0.391780</td>\n",
       "      <td>0.863402</td>\n",
       "      <td>00:23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>0.082899</td>\n",
       "      <td>0.074935</td>\n",
       "      <td>0.074935</td>\n",
       "      <td>0.377095</td>\n",
       "      <td>0.862701</td>\n",
       "      <td>00:23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>0.079172</td>\n",
       "      <td>0.075803</td>\n",
       "      <td>0.075803</td>\n",
       "      <td>0.377172</td>\n",
       "      <td>0.860598</td>\n",
       "      <td>00:23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>0.074548</td>\n",
       "      <td>0.060589</td>\n",
       "      <td>0.060589</td>\n",
       "      <td>0.297497</td>\n",
       "      <td>0.886896</td>\n",
       "      <td>00:23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>0.070611</td>\n",
       "      <td>0.069464</td>\n",
       "      <td>0.069464</td>\n",
       "      <td>0.381686</td>\n",
       "      <td>0.872722</td>\n",
       "      <td>00:23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>0.067270</td>\n",
       "      <td>0.061085</td>\n",
       "      <td>0.061085</td>\n",
       "      <td>0.287969</td>\n",
       "      <td>0.886095</td>\n",
       "      <td>00:23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>0.065682</td>\n",
       "      <td>0.096572</td>\n",
       "      <td>0.096572</td>\n",
       "      <td>0.440894</td>\n",
       "      <td>0.828720</td>\n",
       "      <td>00:23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>0.064146</td>\n",
       "      <td>0.092460</td>\n",
       "      <td>0.092460</td>\n",
       "      <td>0.589967</td>\n",
       "      <td>0.832574</td>\n",
       "      <td>00:23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28</td>\n",
       "      <td>0.069824</td>\n",
       "      <td>0.124358</td>\n",
       "      <td>0.124358</td>\n",
       "      <td>0.878179</td>\n",
       "      <td>0.779618</td>\n",
       "      <td>00:23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29</td>\n",
       "      <td>0.075652</td>\n",
       "      <td>0.131928</td>\n",
       "      <td>0.131928</td>\n",
       "      <td>1.015232</td>\n",
       "      <td>0.767927</td>\n",
       "      <td>00:23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>0.076219</td>\n",
       "      <td>0.115617</td>\n",
       "      <td>0.115617</td>\n",
       "      <td>0.473617</td>\n",
       "      <td>0.794449</td>\n",
       "      <td>00:23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31</td>\n",
       "      <td>0.074550</td>\n",
       "      <td>0.083497</td>\n",
       "      <td>0.083497</td>\n",
       "      <td>0.579981</td>\n",
       "      <td>0.846720</td>\n",
       "      <td>00:23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32</td>\n",
       "      <td>0.073935</td>\n",
       "      <td>0.186941</td>\n",
       "      <td>0.186941</td>\n",
       "      <td>1.150967</td>\n",
       "      <td>0.686046</td>\n",
       "      <td>00:23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33</td>\n",
       "      <td>0.071220</td>\n",
       "      <td>0.064182</td>\n",
       "      <td>0.064182</td>\n",
       "      <td>0.386523</td>\n",
       "      <td>0.879932</td>\n",
       "      <td>00:23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34</td>\n",
       "      <td>0.067313</td>\n",
       "      <td>0.062704</td>\n",
       "      <td>0.062704</td>\n",
       "      <td>0.329717</td>\n",
       "      <td>0.882663</td>\n",
       "      <td>00:23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35</td>\n",
       "      <td>0.064060</td>\n",
       "      <td>0.062306</td>\n",
       "      <td>0.062306</td>\n",
       "      <td>0.356789</td>\n",
       "      <td>0.883176</td>\n",
       "      <td>00:23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36</td>\n",
       "      <td>0.061369</td>\n",
       "      <td>0.062940</td>\n",
       "      <td>0.062940</td>\n",
       "      <td>0.385564</td>\n",
       "      <td>0.882057</td>\n",
       "      <td>00:23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>37</td>\n",
       "      <td>0.060419</td>\n",
       "      <td>0.063716</td>\n",
       "      <td>0.063716</td>\n",
       "      <td>0.392190</td>\n",
       "      <td>0.880665</td>\n",
       "      <td>00:23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>38</td>\n",
       "      <td>0.057604</td>\n",
       "      <td>0.066197</td>\n",
       "      <td>0.066197</td>\n",
       "      <td>0.415824</td>\n",
       "      <td>0.876616</td>\n",
       "      <td>00:23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>39</td>\n",
       "      <td>0.055228</td>\n",
       "      <td>0.062798</td>\n",
       "      <td>0.062798</td>\n",
       "      <td>0.387536</td>\n",
       "      <td>0.882548</td>\n",
       "      <td>00:23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>0.052946</td>\n",
       "      <td>0.058912</td>\n",
       "      <td>0.058912</td>\n",
       "      <td>0.347787</td>\n",
       "      <td>0.889342</td>\n",
       "      <td>00:23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>41</td>\n",
       "      <td>0.050742</td>\n",
       "      <td>0.057890</td>\n",
       "      <td>0.057890</td>\n",
       "      <td>0.336428</td>\n",
       "      <td>0.891101</td>\n",
       "      <td>00:23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>42</td>\n",
       "      <td>0.049043</td>\n",
       "      <td>0.057749</td>\n",
       "      <td>0.057749</td>\n",
       "      <td>0.351227</td>\n",
       "      <td>0.891369</td>\n",
       "      <td>00:23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>43</td>\n",
       "      <td>0.047973</td>\n",
       "      <td>0.055325</td>\n",
       "      <td>0.055325</td>\n",
       "      <td>0.300908</td>\n",
       "      <td>0.895759</td>\n",
       "      <td>00:23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>44</td>\n",
       "      <td>0.046680</td>\n",
       "      <td>0.057801</td>\n",
       "      <td>0.057801</td>\n",
       "      <td>0.323352</td>\n",
       "      <td>0.891269</td>\n",
       "      <td>00:23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>45</td>\n",
       "      <td>0.046037</td>\n",
       "      <td>0.055168</td>\n",
       "      <td>0.055168</td>\n",
       "      <td>0.296061</td>\n",
       "      <td>0.896030</td>\n",
       "      <td>00:23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>46</td>\n",
       "      <td>0.045216</td>\n",
       "      <td>0.056282</td>\n",
       "      <td>0.056282</td>\n",
       "      <td>0.335235</td>\n",
       "      <td>0.893957</td>\n",
       "      <td>00:23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>47</td>\n",
       "      <td>0.044074</td>\n",
       "      <td>0.057179</td>\n",
       "      <td>0.057179</td>\n",
       "      <td>0.348863</td>\n",
       "      <td>0.892394</td>\n",
       "      <td>00:23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>48</td>\n",
       "      <td>0.043489</td>\n",
       "      <td>0.057532</td>\n",
       "      <td>0.057532</td>\n",
       "      <td>0.368694</td>\n",
       "      <td>0.891622</td>\n",
       "      <td>00:23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>49</td>\n",
       "      <td>0.042670</td>\n",
       "      <td>0.054217</td>\n",
       "      <td>0.054217</td>\n",
       "      <td>0.321969</td>\n",
       "      <td>0.897628</td>\n",
       "      <td>00:23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.041936</td>\n",
       "      <td>0.053670</td>\n",
       "      <td>0.053670</td>\n",
       "      <td>0.342311</td>\n",
       "      <td>0.898513</td>\n",
       "      <td>00:23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>51</td>\n",
       "      <td>0.041272</td>\n",
       "      <td>0.053640</td>\n",
       "      <td>0.053640</td>\n",
       "      <td>0.331570</td>\n",
       "      <td>0.898552</td>\n",
       "      <td>00:23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>52</td>\n",
       "      <td>0.040549</td>\n",
       "      <td>0.051774</td>\n",
       "      <td>0.051774</td>\n",
       "      <td>0.276895</td>\n",
       "      <td>0.902168</td>\n",
       "      <td>00:23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>53</td>\n",
       "      <td>0.040066</td>\n",
       "      <td>0.053951</td>\n",
       "      <td>0.053951</td>\n",
       "      <td>0.306370</td>\n",
       "      <td>0.898132</td>\n",
       "      <td>00:23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>54</td>\n",
       "      <td>0.039344</td>\n",
       "      <td>0.052304</td>\n",
       "      <td>0.052304</td>\n",
       "      <td>0.289893</td>\n",
       "      <td>0.901333</td>\n",
       "      <td>00:23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>55</td>\n",
       "      <td>0.038854</td>\n",
       "      <td>0.052630</td>\n",
       "      <td>0.052630</td>\n",
       "      <td>0.301440</td>\n",
       "      <td>0.900447</td>\n",
       "      <td>00:23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>56</td>\n",
       "      <td>0.038504</td>\n",
       "      <td>0.054395</td>\n",
       "      <td>0.054395</td>\n",
       "      <td>0.330852</td>\n",
       "      <td>0.897254</td>\n",
       "      <td>00:23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>57</td>\n",
       "      <td>0.038008</td>\n",
       "      <td>0.053839</td>\n",
       "      <td>0.053839</td>\n",
       "      <td>0.347097</td>\n",
       "      <td>0.898197</td>\n",
       "      <td>00:23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>58</td>\n",
       "      <td>0.037480</td>\n",
       "      <td>0.051495</td>\n",
       "      <td>0.051495</td>\n",
       "      <td>0.314286</td>\n",
       "      <td>0.902527</td>\n",
       "      <td>00:23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>59</td>\n",
       "      <td>0.037193</td>\n",
       "      <td>0.053038</td>\n",
       "      <td>0.053038</td>\n",
       "      <td>0.319665</td>\n",
       "      <td>0.899712</td>\n",
       "      <td>00:23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>0.036645</td>\n",
       "      <td>0.051569</td>\n",
       "      <td>0.051569</td>\n",
       "      <td>0.336308</td>\n",
       "      <td>0.902275</td>\n",
       "      <td>00:23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>61</td>\n",
       "      <td>0.036204</td>\n",
       "      <td>0.052307</td>\n",
       "      <td>0.052307</td>\n",
       "      <td>0.354539</td>\n",
       "      <td>0.900894</td>\n",
       "      <td>00:23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>62</td>\n",
       "      <td>0.035923</td>\n",
       "      <td>0.052964</td>\n",
       "      <td>0.052964</td>\n",
       "      <td>0.368513</td>\n",
       "      <td>0.899725</td>\n",
       "      <td>00:23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>63</td>\n",
       "      <td>0.035469</td>\n",
       "      <td>0.052031</td>\n",
       "      <td>0.052031</td>\n",
       "      <td>0.336514</td>\n",
       "      <td>0.901485</td>\n",
       "      <td>00:23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>64</td>\n",
       "      <td>0.034830</td>\n",
       "      <td>0.052246</td>\n",
       "      <td>0.052246</td>\n",
       "      <td>0.345552</td>\n",
       "      <td>0.901079</td>\n",
       "      <td>00:23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>65</td>\n",
       "      <td>0.034455</td>\n",
       "      <td>0.052824</td>\n",
       "      <td>0.052824</td>\n",
       "      <td>0.344223</td>\n",
       "      <td>0.900030</td>\n",
       "      <td>00:23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>66</td>\n",
       "      <td>0.033936</td>\n",
       "      <td>0.052571</td>\n",
       "      <td>0.052571</td>\n",
       "      <td>0.330674</td>\n",
       "      <td>0.900486</td>\n",
       "      <td>00:23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>67</td>\n",
       "      <td>0.033662</td>\n",
       "      <td>0.052093</td>\n",
       "      <td>0.052093</td>\n",
       "      <td>0.349650</td>\n",
       "      <td>0.901382</td>\n",
       "      <td>00:23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>68</td>\n",
       "      <td>0.033386</td>\n",
       "      <td>0.050159</td>\n",
       "      <td>0.050159</td>\n",
       "      <td>0.323415</td>\n",
       "      <td>0.904792</td>\n",
       "      <td>00:23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>69</td>\n",
       "      <td>0.032995</td>\n",
       "      <td>0.051046</td>\n",
       "      <td>0.051046</td>\n",
       "      <td>0.347934</td>\n",
       "      <td>0.903259</td>\n",
       "      <td>00:23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>0.032872</td>\n",
       "      <td>0.052838</td>\n",
       "      <td>0.052838</td>\n",
       "      <td>0.366182</td>\n",
       "      <td>0.899964</td>\n",
       "      <td>00:23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>71</td>\n",
       "      <td>0.032751</td>\n",
       "      <td>0.054868</td>\n",
       "      <td>0.054868</td>\n",
       "      <td>0.388632</td>\n",
       "      <td>0.896366</td>\n",
       "      <td>00:23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>72</td>\n",
       "      <td>0.032568</td>\n",
       "      <td>0.052208</td>\n",
       "      <td>0.052208</td>\n",
       "      <td>0.338244</td>\n",
       "      <td>0.901180</td>\n",
       "      <td>00:23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>73</td>\n",
       "      <td>0.032133</td>\n",
       "      <td>0.051247</td>\n",
       "      <td>0.051247</td>\n",
       "      <td>0.334073</td>\n",
       "      <td>0.902810</td>\n",
       "      <td>00:23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>74</td>\n",
       "      <td>0.031793</td>\n",
       "      <td>0.051982</td>\n",
       "      <td>0.051982</td>\n",
       "      <td>0.330812</td>\n",
       "      <td>0.901500</td>\n",
       "      <td>00:23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>75</td>\n",
       "      <td>0.031548</td>\n",
       "      <td>0.050898</td>\n",
       "      <td>0.050898</td>\n",
       "      <td>0.338308</td>\n",
       "      <td>0.903498</td>\n",
       "      <td>00:23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>76</td>\n",
       "      <td>0.031128</td>\n",
       "      <td>0.050878</td>\n",
       "      <td>0.050878</td>\n",
       "      <td>0.332365</td>\n",
       "      <td>0.903492</td>\n",
       "      <td>00:23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>77</td>\n",
       "      <td>0.030704</td>\n",
       "      <td>0.054118</td>\n",
       "      <td>0.054118</td>\n",
       "      <td>0.381741</td>\n",
       "      <td>0.897626</td>\n",
       "      <td>00:23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>78</td>\n",
       "      <td>0.030479</td>\n",
       "      <td>0.051520</td>\n",
       "      <td>0.051520</td>\n",
       "      <td>0.372717</td>\n",
       "      <td>0.902279</td>\n",
       "      <td>00:23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>79</td>\n",
       "      <td>0.030219</td>\n",
       "      <td>0.053767</td>\n",
       "      <td>0.053767</td>\n",
       "      <td>0.396391</td>\n",
       "      <td>0.898298</td>\n",
       "      <td>00:23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>0.029705</td>\n",
       "      <td>0.053276</td>\n",
       "      <td>0.053276</td>\n",
       "      <td>0.391761</td>\n",
       "      <td>0.899176</td>\n",
       "      <td>00:23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>81</td>\n",
       "      <td>0.029212</td>\n",
       "      <td>0.050269</td>\n",
       "      <td>0.050269</td>\n",
       "      <td>0.344023</td>\n",
       "      <td>0.904600</td>\n",
       "      <td>00:23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>82</td>\n",
       "      <td>0.028811</td>\n",
       "      <td>0.052303</td>\n",
       "      <td>0.052303</td>\n",
       "      <td>0.379765</td>\n",
       "      <td>0.900858</td>\n",
       "      <td>00:23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>83</td>\n",
       "      <td>0.028326</td>\n",
       "      <td>0.050873</td>\n",
       "      <td>0.050873</td>\n",
       "      <td>0.351256</td>\n",
       "      <td>0.903482</td>\n",
       "      <td>00:23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>84</td>\n",
       "      <td>0.028012</td>\n",
       "      <td>0.049252</td>\n",
       "      <td>0.049252</td>\n",
       "      <td>0.346829</td>\n",
       "      <td>0.906366</td>\n",
       "      <td>00:23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>85</td>\n",
       "      <td>0.027712</td>\n",
       "      <td>0.049984</td>\n",
       "      <td>0.049984</td>\n",
       "      <td>0.339100</td>\n",
       "      <td>0.905115</td>\n",
       "      <td>00:23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>86</td>\n",
       "      <td>0.027589</td>\n",
       "      <td>0.048732</td>\n",
       "      <td>0.048732</td>\n",
       "      <td>0.358087</td>\n",
       "      <td>0.907316</td>\n",
       "      <td>00:23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>87</td>\n",
       "      <td>0.027461</td>\n",
       "      <td>0.047572</td>\n",
       "      <td>0.047572</td>\n",
       "      <td>0.368836</td>\n",
       "      <td>0.909412</td>\n",
       "      <td>00:23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>88</td>\n",
       "      <td>0.027420</td>\n",
       "      <td>0.048163</td>\n",
       "      <td>0.048163</td>\n",
       "      <td>0.336510</td>\n",
       "      <td>0.908353</td>\n",
       "      <td>00:23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>89</td>\n",
       "      <td>0.027102</td>\n",
       "      <td>0.047383</td>\n",
       "      <td>0.047383</td>\n",
       "      <td>0.342869</td>\n",
       "      <td>0.909726</td>\n",
       "      <td>00:23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>0.026831</td>\n",
       "      <td>0.048376</td>\n",
       "      <td>0.048376</td>\n",
       "      <td>0.362459</td>\n",
       "      <td>0.907958</td>\n",
       "      <td>00:23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>91</td>\n",
       "      <td>0.026516</td>\n",
       "      <td>0.048635</td>\n",
       "      <td>0.048635</td>\n",
       "      <td>0.346230</td>\n",
       "      <td>0.907484</td>\n",
       "      <td>00:23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>92</td>\n",
       "      <td>0.026175</td>\n",
       "      <td>0.047775</td>\n",
       "      <td>0.047775</td>\n",
       "      <td>0.344919</td>\n",
       "      <td>0.909028</td>\n",
       "      <td>00:23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>93</td>\n",
       "      <td>0.026292</td>\n",
       "      <td>0.048907</td>\n",
       "      <td>0.048907</td>\n",
       "      <td>0.374105</td>\n",
       "      <td>0.906990</td>\n",
       "      <td>00:23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>94</td>\n",
       "      <td>0.025906</td>\n",
       "      <td>0.048027</td>\n",
       "      <td>0.048027</td>\n",
       "      <td>0.341233</td>\n",
       "      <td>0.908560</td>\n",
       "      <td>00:23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>95</td>\n",
       "      <td>0.025485</td>\n",
       "      <td>0.047761</td>\n",
       "      <td>0.047761</td>\n",
       "      <td>0.356828</td>\n",
       "      <td>0.909050</td>\n",
       "      <td>00:23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>96</td>\n",
       "      <td>0.025118</td>\n",
       "      <td>0.049758</td>\n",
       "      <td>0.049758</td>\n",
       "      <td>0.400598</td>\n",
       "      <td>0.905488</td>\n",
       "      <td>00:23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>97</td>\n",
       "      <td>0.024816</td>\n",
       "      <td>0.047609</td>\n",
       "      <td>0.047609</td>\n",
       "      <td>0.375761</td>\n",
       "      <td>0.909353</td>\n",
       "      <td>00:23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>98</td>\n",
       "      <td>0.024590</td>\n",
       "      <td>0.047137</td>\n",
       "      <td>0.047137</td>\n",
       "      <td>0.361999</td>\n",
       "      <td>0.910217</td>\n",
       "      <td>00:23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>99</td>\n",
       "      <td>0.024314</td>\n",
       "      <td>0.045456</td>\n",
       "      <td>0.045456</td>\n",
       "      <td>0.318461</td>\n",
       "      <td>0.913242</td>\n",
       "      <td>00:23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.024032</td>\n",
       "      <td>0.045658</td>\n",
       "      <td>0.045658</td>\n",
       "      <td>0.358249</td>\n",
       "      <td>0.912869</td>\n",
       "      <td>00:23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>101</td>\n",
       "      <td>0.023739</td>\n",
       "      <td>0.046303</td>\n",
       "      <td>0.046303</td>\n",
       "      <td>0.364925</td>\n",
       "      <td>0.911678</td>\n",
       "      <td>00:23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>102</td>\n",
       "      <td>0.023473</td>\n",
       "      <td>0.050635</td>\n",
       "      <td>0.050635</td>\n",
       "      <td>0.414969</td>\n",
       "      <td>0.903815</td>\n",
       "      <td>00:23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>103</td>\n",
       "      <td>0.023207</td>\n",
       "      <td>0.047403</td>\n",
       "      <td>0.047403</td>\n",
       "      <td>0.363253</td>\n",
       "      <td>0.909644</td>\n",
       "      <td>00:23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>104</td>\n",
       "      <td>0.022931</td>\n",
       "      <td>0.046379</td>\n",
       "      <td>0.046379</td>\n",
       "      <td>0.357436</td>\n",
       "      <td>0.911499</td>\n",
       "      <td>00:23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>105</td>\n",
       "      <td>0.022758</td>\n",
       "      <td>0.048345</td>\n",
       "      <td>0.048345</td>\n",
       "      <td>0.426426</td>\n",
       "      <td>0.907937</td>\n",
       "      <td>00:23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>106</td>\n",
       "      <td>0.022628</td>\n",
       "      <td>0.046910</td>\n",
       "      <td>0.046910</td>\n",
       "      <td>0.396282</td>\n",
       "      <td>0.910564</td>\n",
       "      <td>00:23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>107</td>\n",
       "      <td>0.022414</td>\n",
       "      <td>0.045324</td>\n",
       "      <td>0.045324</td>\n",
       "      <td>0.356827</td>\n",
       "      <td>0.913461</td>\n",
       "      <td>00:23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>108</td>\n",
       "      <td>0.022455</td>\n",
       "      <td>0.044657</td>\n",
       "      <td>0.044657</td>\n",
       "      <td>0.324955</td>\n",
       "      <td>0.914747</td>\n",
       "      <td>00:23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>109</td>\n",
       "      <td>0.022508</td>\n",
       "      <td>0.046616</td>\n",
       "      <td>0.046616</td>\n",
       "      <td>0.382608</td>\n",
       "      <td>0.911164</td>\n",
       "      <td>00:23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>110</td>\n",
       "      <td>0.022595</td>\n",
       "      <td>0.047229</td>\n",
       "      <td>0.047229</td>\n",
       "      <td>0.407068</td>\n",
       "      <td>0.910052</td>\n",
       "      <td>00:23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>111</td>\n",
       "      <td>0.022715</td>\n",
       "      <td>0.044958</td>\n",
       "      <td>0.044958</td>\n",
       "      <td>0.349779</td>\n",
       "      <td>0.914137</td>\n",
       "      <td>00:23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>112</td>\n",
       "      <td>0.022670</td>\n",
       "      <td>0.044826</td>\n",
       "      <td>0.044826</td>\n",
       "      <td>0.355363</td>\n",
       "      <td>0.914380</td>\n",
       "      <td>00:23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>113</td>\n",
       "      <td>0.022348</td>\n",
       "      <td>0.046397</td>\n",
       "      <td>0.046397</td>\n",
       "      <td>0.399279</td>\n",
       "      <td>0.911539</td>\n",
       "      <td>00:23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>114</td>\n",
       "      <td>0.022057</td>\n",
       "      <td>0.044969</td>\n",
       "      <td>0.044969</td>\n",
       "      <td>0.367214</td>\n",
       "      <td>0.914128</td>\n",
       "      <td>00:23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>115</td>\n",
       "      <td>0.021743</td>\n",
       "      <td>0.044241</td>\n",
       "      <td>0.044241</td>\n",
       "      <td>0.366440</td>\n",
       "      <td>0.915467</td>\n",
       "      <td>00:23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>116</td>\n",
       "      <td>0.021531</td>\n",
       "      <td>0.044365</td>\n",
       "      <td>0.044365</td>\n",
       "      <td>0.361380</td>\n",
       "      <td>0.915208</td>\n",
       "      <td>00:23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>117</td>\n",
       "      <td>0.021313</td>\n",
       "      <td>0.044510</td>\n",
       "      <td>0.044510</td>\n",
       "      <td>0.374269</td>\n",
       "      <td>0.914970</td>\n",
       "      <td>00:23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>118</td>\n",
       "      <td>0.021167</td>\n",
       "      <td>0.044273</td>\n",
       "      <td>0.044273</td>\n",
       "      <td>0.383578</td>\n",
       "      <td>0.915377</td>\n",
       "      <td>00:23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>119</td>\n",
       "      <td>0.020836</td>\n",
       "      <td>0.043335</td>\n",
       "      <td>0.043335</td>\n",
       "      <td>0.377827</td>\n",
       "      <td>0.917095</td>\n",
       "      <td>00:23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120</td>\n",
       "      <td>0.020740</td>\n",
       "      <td>0.043517</td>\n",
       "      <td>0.043517</td>\n",
       "      <td>0.382868</td>\n",
       "      <td>0.916797</td>\n",
       "      <td>00:23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>121</td>\n",
       "      <td>0.020686</td>\n",
       "      <td>0.044803</td>\n",
       "      <td>0.044803</td>\n",
       "      <td>0.390821</td>\n",
       "      <td>0.914455</td>\n",
       "      <td>00:23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>122</td>\n",
       "      <td>0.020532</td>\n",
       "      <td>0.045167</td>\n",
       "      <td>0.045167</td>\n",
       "      <td>0.377381</td>\n",
       "      <td>0.913784</td>\n",
       "      <td>00:23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>123</td>\n",
       "      <td>0.020337</td>\n",
       "      <td>0.044103</td>\n",
       "      <td>0.044103</td>\n",
       "      <td>0.374775</td>\n",
       "      <td>0.915689</td>\n",
       "      <td>00:23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>124</td>\n",
       "      <td>0.020370</td>\n",
       "      <td>0.044316</td>\n",
       "      <td>0.044316</td>\n",
       "      <td>0.381588</td>\n",
       "      <td>0.915348</td>\n",
       "      <td>00:23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>125</td>\n",
       "      <td>0.020329</td>\n",
       "      <td>0.045134</td>\n",
       "      <td>0.045134</td>\n",
       "      <td>0.397151</td>\n",
       "      <td>0.913819</td>\n",
       "      <td>00:23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>126</td>\n",
       "      <td>0.020235</td>\n",
       "      <td>0.044658</td>\n",
       "      <td>0.044658</td>\n",
       "      <td>0.381269</td>\n",
       "      <td>0.914672</td>\n",
       "      <td>00:23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>127</td>\n",
       "      <td>0.020067</td>\n",
       "      <td>0.044347</td>\n",
       "      <td>0.044347</td>\n",
       "      <td>0.372158</td>\n",
       "      <td>0.915320</td>\n",
       "      <td>00:23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>128</td>\n",
       "      <td>0.020048</td>\n",
       "      <td>0.044186</td>\n",
       "      <td>0.044186</td>\n",
       "      <td>0.377718</td>\n",
       "      <td>0.915573</td>\n",
       "      <td>00:23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>129</td>\n",
       "      <td>0.020056</td>\n",
       "      <td>0.043447</td>\n",
       "      <td>0.043447</td>\n",
       "      <td>0.363075</td>\n",
       "      <td>0.916914</td>\n",
       "      <td>00:23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>130</td>\n",
       "      <td>0.020068</td>\n",
       "      <td>0.044293</td>\n",
       "      <td>0.044293</td>\n",
       "      <td>0.373080</td>\n",
       "      <td>0.915355</td>\n",
       "      <td>00:23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>131</td>\n",
       "      <td>0.020002</td>\n",
       "      <td>0.043764</td>\n",
       "      <td>0.043764</td>\n",
       "      <td>0.364705</td>\n",
       "      <td>0.916350</td>\n",
       "      <td>00:23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>132</td>\n",
       "      <td>0.019958</td>\n",
       "      <td>0.044013</td>\n",
       "      <td>0.044013</td>\n",
       "      <td>0.366426</td>\n",
       "      <td>0.915898</td>\n",
       "      <td>00:23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>133</td>\n",
       "      <td>0.019907</td>\n",
       "      <td>0.044657</td>\n",
       "      <td>0.044657</td>\n",
       "      <td>0.373483</td>\n",
       "      <td>0.914707</td>\n",
       "      <td>00:23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>134</td>\n",
       "      <td>0.019847</td>\n",
       "      <td>0.043675</td>\n",
       "      <td>0.043675</td>\n",
       "      <td>0.380901</td>\n",
       "      <td>0.916499</td>\n",
       "      <td>00:23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>135</td>\n",
       "      <td>0.019708</td>\n",
       "      <td>0.043598</td>\n",
       "      <td>0.043598</td>\n",
       "      <td>0.373785</td>\n",
       "      <td>0.916665</td>\n",
       "      <td>00:23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>136</td>\n",
       "      <td>0.019634</td>\n",
       "      <td>0.043345</td>\n",
       "      <td>0.043345</td>\n",
       "      <td>0.365859</td>\n",
       "      <td>0.917127</td>\n",
       "      <td>00:23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>137</td>\n",
       "      <td>0.019673</td>\n",
       "      <td>0.044018</td>\n",
       "      <td>0.044018</td>\n",
       "      <td>0.379360</td>\n",
       "      <td>0.915842</td>\n",
       "      <td>00:23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>138</td>\n",
       "      <td>0.019529</td>\n",
       "      <td>0.044132</td>\n",
       "      <td>0.044132</td>\n",
       "      <td>0.376691</td>\n",
       "      <td>0.915679</td>\n",
       "      <td>00:23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>139</td>\n",
       "      <td>0.019991</td>\n",
       "      <td>0.043941</td>\n",
       "      <td>0.043941</td>\n",
       "      <td>0.381769</td>\n",
       "      <td>0.915994</td>\n",
       "      <td>00:23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>140</td>\n",
       "      <td>0.019838</td>\n",
       "      <td>0.043302</td>\n",
       "      <td>0.043302</td>\n",
       "      <td>0.360049</td>\n",
       "      <td>0.917208</td>\n",
       "      <td>00:23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>141</td>\n",
       "      <td>0.019739</td>\n",
       "      <td>0.043738</td>\n",
       "      <td>0.043738</td>\n",
       "      <td>0.369596</td>\n",
       "      <td>0.916397</td>\n",
       "      <td>00:23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>142</td>\n",
       "      <td>0.019640</td>\n",
       "      <td>0.044115</td>\n",
       "      <td>0.044115</td>\n",
       "      <td>0.372312</td>\n",
       "      <td>0.915659</td>\n",
       "      <td>00:23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>143</td>\n",
       "      <td>0.019547</td>\n",
       "      <td>0.044065</td>\n",
       "      <td>0.044065</td>\n",
       "      <td>0.370122</td>\n",
       "      <td>0.915783</td>\n",
       "      <td>00:23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>144</td>\n",
       "      <td>0.019370</td>\n",
       "      <td>0.043737</td>\n",
       "      <td>0.043737</td>\n",
       "      <td>0.376828</td>\n",
       "      <td>0.916394</td>\n",
       "      <td>00:23</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>\n",
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "                background: #F44336;\n",
       "            }\n",
       "        </style>\n",
       "      <progress value='3' class='' max='3', style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      100.00% [3/3 00:02<00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Better model found at epoch 0 with valid_loss value: 0.7728694081306458.\n",
      "Better model found at epoch 4 with valid_loss value: 0.1832788586616516.\n",
      "Better model found at epoch 6 with valid_loss value: 0.09714015573263168.\n",
      "Better model found at epoch 12 with valid_loss value: 0.08770924806594849.\n",
      "Better model found at epoch 15 with valid_loss value: 0.08284346759319305.\n",
      "Better model found at epoch 16 with valid_loss value: 0.07922443002462387.\n",
      "Better model found at epoch 18 with valid_loss value: 0.07636988162994385.\n",
      "Better model found at epoch 20 with valid_loss value: 0.07416640222072601.\n",
      "Better model found at epoch 23 with valid_loss value: 0.06058864668011665.\n",
      "on end of epoch#32: start annealing from 0.001 to 0.0001\n",
      "Better model found at epoch 40 with valid_loss value: 0.0589120015501976.\n",
      "Better model found at epoch 41 with valid_loss value: 0.05789027363061905.\n",
      "Better model found at epoch 42 with valid_loss value: 0.05774946138262749.\n",
      "Better model found at epoch 43 with valid_loss value: 0.05532471090555191.\n",
      "Better model found at epoch 45 with valid_loss value: 0.05516784265637398.\n",
      "Better model found at epoch 49 with valid_loss value: 0.05421696975827217.\n",
      "Better model found at epoch 50 with valid_loss value: 0.05367022752761841.\n",
      "Better model found at epoch 51 with valid_loss value: 0.05363999679684639.\n",
      "Better model found at epoch 52 with valid_loss value: 0.051774002611637115.\n",
      "Better model found at epoch 58 with valid_loss value: 0.051494598388671875.\n",
      "Better model found at epoch 68 with valid_loss value: 0.05015890672802925.\n",
      "Better model found at epoch 84 with valid_loss value: 0.049252379685640335.\n",
      "Better model found at epoch 86 with valid_loss value: 0.04873216152191162.\n",
      "Better model found at epoch 87 with valid_loss value: 0.047572456300258636.\n",
      "Better model found at epoch 89 with valid_loss value: 0.047383081167936325.\n",
      "Better model found at epoch 98 with valid_loss value: 0.04713708162307739.\n",
      "Better model found at epoch 99 with valid_loss value: 0.045456159859895706.\n",
      "Better model found at epoch 107 with valid_loss value: 0.04532383754849434.\n",
      "Better model found at epoch 108 with valid_loss value: 0.04465677589178085.\n",
      "on end of epoch#112: start annealing from 0.0001 to 1e-05\n",
      "Better model found at epoch 115 with valid_loss value: 0.044241368770599365.\n",
      "Better model found at epoch 119 with valid_loss value: 0.04333515092730522.\n",
      "Better model found at epoch 140 with valid_loss value: 0.043302346020936966.\n",
      "on end of epoch#143: start annealing from 1e-05 to 1.0000000000000002e-06\n"
     ]
    }
   ],
   "source": [
    "lrs = [1e-3]\n",
    "checkpoints = [None]\n",
    "opts = [optf_adam]\n",
    "\n",
    "nb_train_script_logger.multi_train(get_learn=getlearn, \n",
    "            epoch_len=1e9, epochs=500,\n",
    "            opts=opts, lrs=lrs, checkpoints=checkpoints,\n",
    "            tb_log_root='./tb_log/',\n",
    "            autoSave=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "backup_train_logs(tag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs:144,train_loss:0.019,mask_iou:0.916"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 阶段结果  \n",
    "vanlia得到的mask_iou略低于allres，收敛快于allres，144:240epochs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 比较dice和balance_bce"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 200数据集，无transform，resnet18，allresh，balance_bce，adam，balance_ratio=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "tag = '__'.join(['resnet18', 'allres', 'balance_bce', 'dataset200','adam', 'balance_ratio_1'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "getlearn = partial(get_learn, data = data_200, model_name = 'resnet18'\n",
    "            , loss_func_name = 'balance_bce', allres = True, balance_ratio = 1, tag = tag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Learner(data=ImageDataBunch;\n",
       "\n",
       "Train: LabelList (160 items)\n",
       "x: SegmentationItemList\n",
       "Image (3, 512, 512),Image (3, 512, 512),Image (3, 512, 512),Image (3, 512, 512),Image (3, 512, 512)\n",
       "y: SegmentationLabelList\n",
       "ImageSegment (1, 512, 512),ImageSegment (1, 512, 512),ImageSegment (1, 512, 512),ImageSegment (1, 512, 512),ImageSegment (1, 512, 512)\n",
       "Path: data/dataset_20200715/image;\n",
       "\n",
       "Valid: LabelList (40 items)\n",
       "x: SegmentationItemList\n",
       "Image (3, 512, 512),Image (3, 512, 512),Image (3, 512, 512),Image (3, 512, 512),Image (3, 512, 512)\n",
       "y: SegmentationLabelList\n",
       "ImageSegment (1, 512, 512),ImageSegment (1, 512, 512),ImageSegment (1, 512, 512),ImageSegment (1, 512, 512),ImageSegment (1, 512, 512)\n",
       "Path: data/dataset_20200715/image;\n",
       "\n",
       "Test: None, model=DataParallel(\n",
       "  (module): Resnet_UNet(\n",
       "    (down_blocks): ModuleList(\n",
       "      (0): Sequential(\n",
       "        (0): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "        (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU(inplace=True)\n",
       "      )\n",
       "      (1): Sequential(\n",
       "        (0): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "        (1): Sequential(\n",
       "          (0): BasicBlock(\n",
       "            (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "            (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (relu): ReLU(inplace=True)\n",
       "            (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "            (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "          (1): BasicBlock(\n",
       "            (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "            (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (relu): ReLU(inplace=True)\n",
       "            (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "            (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (2): Sequential(\n",
       "        (0): BasicBlock(\n",
       "          (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (downsample): Sequential(\n",
       "            (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "            (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (1): BasicBlock(\n",
       "          (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (3): Sequential(\n",
       "        (0): BasicBlock(\n",
       "          (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (downsample): Sequential(\n",
       "            (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "            (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (1): BasicBlock(\n",
       "          (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (4): Sequential(\n",
       "        (0): BasicBlock(\n",
       "          (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (downsample): Sequential(\n",
       "            (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "            (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (1): BasicBlock(\n",
       "          (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (bridge): Bridge(\n",
       "      (conv1): Conv_Bn_ReLu(\n",
       "        (conv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (bn): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (conv2): Conv_Bn_ReLu(\n",
       "        (conv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (bn): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (side_layers): ModuleList(\n",
       "      (0): Conv_Bn_ReLu(\n",
       "        (conv): Conv2d(3, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (1): Conv_Bn_ReLu(\n",
       "        (conv): Conv2d(64, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (2): Conv_Bn_ReLu(\n",
       "        (conv): Conv2d(64, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (3): Conv_Bn_ReLu(\n",
       "        (conv): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (4): Conv_Bn_ReLu(\n",
       "        (conv): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (up_blocks): ModuleList(\n",
       "      (0): UpBlock(\n",
       "        (upsample): Sequential(\n",
       "          (0): ConvTranspose2d(512, 256, kernel_size=(2, 2), stride=(2, 2), bias=False)\n",
       "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (conv1): Conv_Bn_ReLu(\n",
       "          (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (conv2): Conv_Bn_ReLu(\n",
       "          (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): UpBlock(\n",
       "        (upsample): Sequential(\n",
       "          (0): ConvTranspose2d(256, 128, kernel_size=(2, 2), stride=(2, 2), bias=False)\n",
       "          (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (conv1): Conv_Bn_ReLu(\n",
       "          (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (conv2): Conv_Bn_ReLu(\n",
       "          (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (2): UpBlock(\n",
       "        (upsample): Sequential(\n",
       "          (0): ConvTranspose2d(128, 128, kernel_size=(2, 2), stride=(2, 2), bias=False)\n",
       "          (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (conv1): Conv_Bn_ReLu(\n",
       "          (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (conv2): Conv_Bn_ReLu(\n",
       "          (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (3): UpBlock(\n",
       "        (upsample): Sequential(\n",
       "          (0): ConvTranspose2d(128, 128, kernel_size=(2, 2), stride=(2, 2), bias=False)\n",
       "          (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (conv1): Conv_Bn_ReLu(\n",
       "          (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (conv2): Conv_Bn_ReLu(\n",
       "          (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (4): UpBlock(\n",
       "        (upsample): Sequential(\n",
       "          (0): ConvTranspose2d(128, 128, kernel_size=(2, 2), stride=(2, 2), bias=False)\n",
       "          (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (conv1): Conv_Bn_ReLu(\n",
       "          (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (conv2): Conv_Bn_ReLu(\n",
       "          (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (head): Conv2d(128, 1, kernel_size=(1, 1), stride=(1, 1))\n",
       "  )\n",
       "), opt_func=functools.partial(<class 'torch.optim.adam.Adam'>, betas=(0.9, 0.99)), loss_func=functools.partial(<function balance_bce at 0x7f90e485f840>, balance_ratio=1), metrics=[<function dice_loss at 0x7f90e485f730>, functools.partial(<function balance_bce at 0x7f90e485f840>, balance_ratio=1), <function mask_iou at 0x7f90e485f950>], true_wd=True, bn_wd=True, wd=0.01, train_bn=True, path=PosixPath('data/dataset_20200715/image'), model_dir='models', callback_fns=[functools.partial(<class 'fastai.basic_train.Recorder'>, add_time=True, silent=False)], callbacks=[], layer_groups=[ModuleList(\n",
       "  (0): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "  (1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "  (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "  (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "  (4): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "  (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "  (6): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "  (7): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "  (8): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "  (9): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "  (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "  (11): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "  (12): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "  (13): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "  (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "  (15): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "  (16): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "  (17): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "  (18): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "  (19): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "), ModuleList(\n",
       "  (0): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (5): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (6): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (7): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (8): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (9): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (10): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (11): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (12): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (13): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (14): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (15): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (16): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (17): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (18): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (19): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (20): Bridge(\n",
       "    (conv1): Conv_Bn_ReLu(\n",
       "      (conv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (bn): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (conv2): Conv_Bn_ReLu(\n",
       "      (conv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (bn): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (21): ModuleList(\n",
       "    (0): Conv_Bn_ReLu(\n",
       "      (conv): Conv2d(3, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (1): Conv_Bn_ReLu(\n",
       "      (conv): Conv2d(64, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (2): Conv_Bn_ReLu(\n",
       "      (conv): Conv2d(64, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (3): Conv_Bn_ReLu(\n",
       "      (conv): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (4): Conv_Bn_ReLu(\n",
       "      (conv): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (22): ModuleList(\n",
       "    (0): UpBlock(\n",
       "      (upsample): Sequential(\n",
       "        (0): ConvTranspose2d(512, 256, kernel_size=(2, 2), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (conv1): Conv_Bn_ReLu(\n",
       "        (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (conv2): Conv_Bn_ReLu(\n",
       "        (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): UpBlock(\n",
       "      (upsample): Sequential(\n",
       "        (0): ConvTranspose2d(256, 128, kernel_size=(2, 2), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (conv1): Conv_Bn_ReLu(\n",
       "        (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (conv2): Conv_Bn_ReLu(\n",
       "        (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (2): UpBlock(\n",
       "      (upsample): Sequential(\n",
       "        (0): ConvTranspose2d(128, 128, kernel_size=(2, 2), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (conv1): Conv_Bn_ReLu(\n",
       "        (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (conv2): Conv_Bn_ReLu(\n",
       "        (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (3): UpBlock(\n",
       "      (upsample): Sequential(\n",
       "        (0): ConvTranspose2d(128, 128, kernel_size=(2, 2), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (conv1): Conv_Bn_ReLu(\n",
       "        (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (conv2): Conv_Bn_ReLu(\n",
       "        (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (4): UpBlock(\n",
       "      (upsample): Sequential(\n",
       "        (0): ConvTranspose2d(128, 128, kernel_size=(2, 2), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (conv1): Conv_Bn_ReLu(\n",
       "        (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (conv2): Conv_Bn_ReLu(\n",
       "        (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (23): Conv2d(128, 1, kernel_size=(1, 1), stride=(1, 1))\n",
       ")], add_time=True, silent=False)"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "getlearn()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ALLERT: You are using CumtomEpochLength, please make sure that your training dataloader is using random sampler, or this may cause problem.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "                background: #F44336;\n",
       "            }\n",
       "        </style>\n",
       "      <progress value='140' class='' max='500', style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      28.00% [140/500 51:30<2:12:27]\n",
       "    </div>\n",
       "    \n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>dice_loss</th>\n",
       "      <th>balance_bce</th>\n",
       "      <th>mask_iou</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.614686</td>\n",
       "      <td>0.685084</td>\n",
       "      <td>0.720879</td>\n",
       "      <td>0.685084</td>\n",
       "      <td>0.307355</td>\n",
       "      <td>00:22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.567236</td>\n",
       "      <td>0.671655</td>\n",
       "      <td>0.720669</td>\n",
       "      <td>0.671655</td>\n",
       "      <td>0.017879</td>\n",
       "      <td>00:22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.540802</td>\n",
       "      <td>0.613617</td>\n",
       "      <td>0.685822</td>\n",
       "      <td>0.613617</td>\n",
       "      <td>0.365067</td>\n",
       "      <td>00:22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.526255</td>\n",
       "      <td>0.514447</td>\n",
       "      <td>0.611106</td>\n",
       "      <td>0.514447</td>\n",
       "      <td>0.425178</td>\n",
       "      <td>00:22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.504923</td>\n",
       "      <td>0.467339</td>\n",
       "      <td>0.581368</td>\n",
       "      <td>0.467339</td>\n",
       "      <td>0.471608</td>\n",
       "      <td>00:22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.478278</td>\n",
       "      <td>0.334012</td>\n",
       "      <td>0.460968</td>\n",
       "      <td>0.334012</td>\n",
       "      <td>0.604405</td>\n",
       "      <td>00:22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.445903</td>\n",
       "      <td>0.277304</td>\n",
       "      <td>0.403558</td>\n",
       "      <td>0.277304</td>\n",
       "      <td>0.678554</td>\n",
       "      <td>00:22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.405267</td>\n",
       "      <td>0.312291</td>\n",
       "      <td>0.396872</td>\n",
       "      <td>0.312291</td>\n",
       "      <td>0.652147</td>\n",
       "      <td>00:22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.368486</td>\n",
       "      <td>0.216927</td>\n",
       "      <td>0.293845</td>\n",
       "      <td>0.216927</td>\n",
       "      <td>0.667931</td>\n",
       "      <td>00:22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.339699</td>\n",
       "      <td>0.517806</td>\n",
       "      <td>0.329220</td>\n",
       "      <td>0.517806</td>\n",
       "      <td>0.560916</td>\n",
       "      <td>00:22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.313734</td>\n",
       "      <td>0.166626</td>\n",
       "      <td>0.225770</td>\n",
       "      <td>0.166626</td>\n",
       "      <td>0.745324</td>\n",
       "      <td>00:22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.287697</td>\n",
       "      <td>0.166907</td>\n",
       "      <td>0.165458</td>\n",
       "      <td>0.166907</td>\n",
       "      <td>0.793297</td>\n",
       "      <td>00:21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.261390</td>\n",
       "      <td>0.147901</td>\n",
       "      <td>0.204105</td>\n",
       "      <td>0.147901</td>\n",
       "      <td>0.775687</td>\n",
       "      <td>00:21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.236406</td>\n",
       "      <td>0.140055</td>\n",
       "      <td>0.160633</td>\n",
       "      <td>0.140055</td>\n",
       "      <td>0.805002</td>\n",
       "      <td>00:21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.217204</td>\n",
       "      <td>0.156952</td>\n",
       "      <td>0.170985</td>\n",
       "      <td>0.156952</td>\n",
       "      <td>0.788818</td>\n",
       "      <td>00:22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.199088</td>\n",
       "      <td>0.149842</td>\n",
       "      <td>0.132476</td>\n",
       "      <td>0.149842</td>\n",
       "      <td>0.805257</td>\n",
       "      <td>00:22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.187284</td>\n",
       "      <td>0.182099</td>\n",
       "      <td>0.128566</td>\n",
       "      <td>0.182099</td>\n",
       "      <td>0.822995</td>\n",
       "      <td>00:22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.171720</td>\n",
       "      <td>0.112164</td>\n",
       "      <td>0.157014</td>\n",
       "      <td>0.112164</td>\n",
       "      <td>0.796582</td>\n",
       "      <td>00:21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.157177</td>\n",
       "      <td>0.164533</td>\n",
       "      <td>0.105679</td>\n",
       "      <td>0.164533</td>\n",
       "      <td>0.851209</td>\n",
       "      <td>00:22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.148144</td>\n",
       "      <td>0.127305</td>\n",
       "      <td>0.143908</td>\n",
       "      <td>0.127305</td>\n",
       "      <td>0.779589</td>\n",
       "      <td>00:22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.137730</td>\n",
       "      <td>0.102738</td>\n",
       "      <td>0.147833</td>\n",
       "      <td>0.102738</td>\n",
       "      <td>0.820348</td>\n",
       "      <td>00:22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>0.128718</td>\n",
       "      <td>0.330165</td>\n",
       "      <td>0.138389</td>\n",
       "      <td>0.330165</td>\n",
       "      <td>0.798172</td>\n",
       "      <td>00:22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>0.127643</td>\n",
       "      <td>0.204046</td>\n",
       "      <td>0.189392</td>\n",
       "      <td>0.204046</td>\n",
       "      <td>0.771827</td>\n",
       "      <td>00:21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>0.123884</td>\n",
       "      <td>0.259293</td>\n",
       "      <td>0.180977</td>\n",
       "      <td>0.259293</td>\n",
       "      <td>0.772459</td>\n",
       "      <td>00:21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>0.116464</td>\n",
       "      <td>0.115077</td>\n",
       "      <td>0.165574</td>\n",
       "      <td>0.115077</td>\n",
       "      <td>0.784982</td>\n",
       "      <td>00:22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>0.108239</td>\n",
       "      <td>0.091306</td>\n",
       "      <td>0.091972</td>\n",
       "      <td>0.091306</td>\n",
       "      <td>0.870124</td>\n",
       "      <td>00:22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>0.100499</td>\n",
       "      <td>0.124883</td>\n",
       "      <td>0.109253</td>\n",
       "      <td>0.124883</td>\n",
       "      <td>0.858511</td>\n",
       "      <td>00:21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>0.100265</td>\n",
       "      <td>0.229151</td>\n",
       "      <td>0.121678</td>\n",
       "      <td>0.229151</td>\n",
       "      <td>0.836748</td>\n",
       "      <td>00:22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28</td>\n",
       "      <td>0.093835</td>\n",
       "      <td>0.091524</td>\n",
       "      <td>0.096000</td>\n",
       "      <td>0.091524</td>\n",
       "      <td>0.857699</td>\n",
       "      <td>00:22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29</td>\n",
       "      <td>0.087281</td>\n",
       "      <td>0.118397</td>\n",
       "      <td>0.090957</td>\n",
       "      <td>0.118397</td>\n",
       "      <td>0.865059</td>\n",
       "      <td>00:22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>0.081324</td>\n",
       "      <td>0.085766</td>\n",
       "      <td>0.078965</td>\n",
       "      <td>0.085766</td>\n",
       "      <td>0.887494</td>\n",
       "      <td>00:22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31</td>\n",
       "      <td>0.079716</td>\n",
       "      <td>0.108092</td>\n",
       "      <td>0.135975</td>\n",
       "      <td>0.108092</td>\n",
       "      <td>0.797076</td>\n",
       "      <td>00:21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32</td>\n",
       "      <td>0.074295</td>\n",
       "      <td>0.091708</td>\n",
       "      <td>0.133622</td>\n",
       "      <td>0.091708</td>\n",
       "      <td>0.846944</td>\n",
       "      <td>00:21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33</td>\n",
       "      <td>0.071083</td>\n",
       "      <td>0.102660</td>\n",
       "      <td>0.099733</td>\n",
       "      <td>0.102660</td>\n",
       "      <td>0.861979</td>\n",
       "      <td>00:22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34</td>\n",
       "      <td>0.067439</td>\n",
       "      <td>0.093127</td>\n",
       "      <td>0.115591</td>\n",
       "      <td>0.093127</td>\n",
       "      <td>0.825569</td>\n",
       "      <td>00:22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35</td>\n",
       "      <td>0.063554</td>\n",
       "      <td>0.160916</td>\n",
       "      <td>0.109236</td>\n",
       "      <td>0.160916</td>\n",
       "      <td>0.854551</td>\n",
       "      <td>00:22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36</td>\n",
       "      <td>0.060633</td>\n",
       "      <td>0.170362</td>\n",
       "      <td>0.094814</td>\n",
       "      <td>0.170362</td>\n",
       "      <td>0.865717</td>\n",
       "      <td>00:21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>37</td>\n",
       "      <td>0.058624</td>\n",
       "      <td>0.092810</td>\n",
       "      <td>0.088038</td>\n",
       "      <td>0.092810</td>\n",
       "      <td>0.872922</td>\n",
       "      <td>00:22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>38</td>\n",
       "      <td>0.055593</td>\n",
       "      <td>0.081286</td>\n",
       "      <td>0.086139</td>\n",
       "      <td>0.081286</td>\n",
       "      <td>0.867128</td>\n",
       "      <td>00:21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>39</td>\n",
       "      <td>0.052244</td>\n",
       "      <td>0.113122</td>\n",
       "      <td>0.076765</td>\n",
       "      <td>0.113122</td>\n",
       "      <td>0.880471</td>\n",
       "      <td>00:22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>0.050019</td>\n",
       "      <td>0.110337</td>\n",
       "      <td>0.073919</td>\n",
       "      <td>0.110337</td>\n",
       "      <td>0.881068</td>\n",
       "      <td>00:22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>41</td>\n",
       "      <td>0.047326</td>\n",
       "      <td>0.096213</td>\n",
       "      <td>0.067570</td>\n",
       "      <td>0.096213</td>\n",
       "      <td>0.892503</td>\n",
       "      <td>00:21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>42</td>\n",
       "      <td>0.044651</td>\n",
       "      <td>0.085037</td>\n",
       "      <td>0.061500</td>\n",
       "      <td>0.085037</td>\n",
       "      <td>0.902614</td>\n",
       "      <td>00:21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>43</td>\n",
       "      <td>0.042514</td>\n",
       "      <td>0.072761</td>\n",
       "      <td>0.062451</td>\n",
       "      <td>0.072761</td>\n",
       "      <td>0.905574</td>\n",
       "      <td>00:21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>44</td>\n",
       "      <td>0.040551</td>\n",
       "      <td>0.082845</td>\n",
       "      <td>0.056645</td>\n",
       "      <td>0.082845</td>\n",
       "      <td>0.912071</td>\n",
       "      <td>00:22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>45</td>\n",
       "      <td>0.040542</td>\n",
       "      <td>0.157081</td>\n",
       "      <td>0.070337</td>\n",
       "      <td>0.157081</td>\n",
       "      <td>0.890142</td>\n",
       "      <td>00:22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>46</td>\n",
       "      <td>0.039315</td>\n",
       "      <td>0.076347</td>\n",
       "      <td>0.075245</td>\n",
       "      <td>0.076347</td>\n",
       "      <td>0.874856</td>\n",
       "      <td>00:22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>47</td>\n",
       "      <td>0.037489</td>\n",
       "      <td>0.083042</td>\n",
       "      <td>0.070009</td>\n",
       "      <td>0.083042</td>\n",
       "      <td>0.898495</td>\n",
       "      <td>00:21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>48</td>\n",
       "      <td>0.035625</td>\n",
       "      <td>0.090376</td>\n",
       "      <td>0.064357</td>\n",
       "      <td>0.090376</td>\n",
       "      <td>0.897329</td>\n",
       "      <td>00:22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>49</td>\n",
       "      <td>0.033880</td>\n",
       "      <td>0.074251</td>\n",
       "      <td>0.064412</td>\n",
       "      <td>0.074251</td>\n",
       "      <td>0.896998</td>\n",
       "      <td>00:22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.032326</td>\n",
       "      <td>0.079380</td>\n",
       "      <td>0.060429</td>\n",
       "      <td>0.079380</td>\n",
       "      <td>0.905137</td>\n",
       "      <td>00:22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>51</td>\n",
       "      <td>0.030952</td>\n",
       "      <td>0.075734</td>\n",
       "      <td>0.059656</td>\n",
       "      <td>0.075734</td>\n",
       "      <td>0.900972</td>\n",
       "      <td>00:21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>52</td>\n",
       "      <td>0.030480</td>\n",
       "      <td>0.074721</td>\n",
       "      <td>0.076464</td>\n",
       "      <td>0.074721</td>\n",
       "      <td>0.879408</td>\n",
       "      <td>00:22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>53</td>\n",
       "      <td>0.037360</td>\n",
       "      <td>0.326333</td>\n",
       "      <td>0.093814</td>\n",
       "      <td>0.326333</td>\n",
       "      <td>0.850211</td>\n",
       "      <td>00:21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>54</td>\n",
       "      <td>0.054622</td>\n",
       "      <td>0.279409</td>\n",
       "      <td>0.237643</td>\n",
       "      <td>0.279409</td>\n",
       "      <td>0.624076</td>\n",
       "      <td>00:22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>55</td>\n",
       "      <td>0.059427</td>\n",
       "      <td>0.137910</td>\n",
       "      <td>0.200379</td>\n",
       "      <td>0.137910</td>\n",
       "      <td>0.730074</td>\n",
       "      <td>00:22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>56</td>\n",
       "      <td>0.059417</td>\n",
       "      <td>0.088130</td>\n",
       "      <td>0.104002</td>\n",
       "      <td>0.088130</td>\n",
       "      <td>0.848151</td>\n",
       "      <td>00:21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>57</td>\n",
       "      <td>0.057279</td>\n",
       "      <td>0.113122</td>\n",
       "      <td>0.093339</td>\n",
       "      <td>0.113122</td>\n",
       "      <td>0.853963</td>\n",
       "      <td>00:21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>58</td>\n",
       "      <td>0.053651</td>\n",
       "      <td>0.095710</td>\n",
       "      <td>0.074339</td>\n",
       "      <td>0.095710</td>\n",
       "      <td>0.883959</td>\n",
       "      <td>00:21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>59</td>\n",
       "      <td>0.049589</td>\n",
       "      <td>0.086426</td>\n",
       "      <td>0.070325</td>\n",
       "      <td>0.086426</td>\n",
       "      <td>0.887883</td>\n",
       "      <td>00:22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>0.046427</td>\n",
       "      <td>0.082945</td>\n",
       "      <td>0.068091</td>\n",
       "      <td>0.082945</td>\n",
       "      <td>0.893424</td>\n",
       "      <td>00:22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>61</td>\n",
       "      <td>0.043512</td>\n",
       "      <td>0.087407</td>\n",
       "      <td>0.066478</td>\n",
       "      <td>0.087407</td>\n",
       "      <td>0.896692</td>\n",
       "      <td>00:21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>62</td>\n",
       "      <td>0.041210</td>\n",
       "      <td>0.085362</td>\n",
       "      <td>0.067161</td>\n",
       "      <td>0.085362</td>\n",
       "      <td>0.893848</td>\n",
       "      <td>00:22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>63</td>\n",
       "      <td>0.040222</td>\n",
       "      <td>0.090663</td>\n",
       "      <td>0.065995</td>\n",
       "      <td>0.090663</td>\n",
       "      <td>0.894672</td>\n",
       "      <td>00:21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>64</td>\n",
       "      <td>0.038105</td>\n",
       "      <td>0.094164</td>\n",
       "      <td>0.065205</td>\n",
       "      <td>0.094164</td>\n",
       "      <td>0.896192</td>\n",
       "      <td>00:22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>65</td>\n",
       "      <td>0.036435</td>\n",
       "      <td>0.089853</td>\n",
       "      <td>0.066373</td>\n",
       "      <td>0.089853</td>\n",
       "      <td>0.894994</td>\n",
       "      <td>00:22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>66</td>\n",
       "      <td>0.034705</td>\n",
       "      <td>0.095530</td>\n",
       "      <td>0.065120</td>\n",
       "      <td>0.095530</td>\n",
       "      <td>0.896764</td>\n",
       "      <td>00:21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>67</td>\n",
       "      <td>0.033211</td>\n",
       "      <td>0.087910</td>\n",
       "      <td>0.063750</td>\n",
       "      <td>0.087910</td>\n",
       "      <td>0.896711</td>\n",
       "      <td>00:21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>68</td>\n",
       "      <td>0.031986</td>\n",
       "      <td>0.084030</td>\n",
       "      <td>0.063985</td>\n",
       "      <td>0.084030</td>\n",
       "      <td>0.897317</td>\n",
       "      <td>00:22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>69</td>\n",
       "      <td>0.031101</td>\n",
       "      <td>0.090435</td>\n",
       "      <td>0.063193</td>\n",
       "      <td>0.090435</td>\n",
       "      <td>0.900408</td>\n",
       "      <td>00:22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>0.030363</td>\n",
       "      <td>0.098411</td>\n",
       "      <td>0.063088</td>\n",
       "      <td>0.098411</td>\n",
       "      <td>0.898480</td>\n",
       "      <td>00:21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>71</td>\n",
       "      <td>0.029454</td>\n",
       "      <td>0.090648</td>\n",
       "      <td>0.062348</td>\n",
       "      <td>0.090648</td>\n",
       "      <td>0.899502</td>\n",
       "      <td>00:21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>72</td>\n",
       "      <td>0.028768</td>\n",
       "      <td>0.093497</td>\n",
       "      <td>0.062387</td>\n",
       "      <td>0.093497</td>\n",
       "      <td>0.899851</td>\n",
       "      <td>00:22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>73</td>\n",
       "      <td>0.028133</td>\n",
       "      <td>0.098632</td>\n",
       "      <td>0.062331</td>\n",
       "      <td>0.098632</td>\n",
       "      <td>0.899463</td>\n",
       "      <td>00:21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>74</td>\n",
       "      <td>0.027718</td>\n",
       "      <td>0.102522</td>\n",
       "      <td>0.063002</td>\n",
       "      <td>0.102522</td>\n",
       "      <td>0.899431</td>\n",
       "      <td>00:22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>75</td>\n",
       "      <td>0.027148</td>\n",
       "      <td>0.099811</td>\n",
       "      <td>0.060926</td>\n",
       "      <td>0.099811</td>\n",
       "      <td>0.901216</td>\n",
       "      <td>00:22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>76</td>\n",
       "      <td>0.027099</td>\n",
       "      <td>0.109254</td>\n",
       "      <td>0.061704</td>\n",
       "      <td>0.109254</td>\n",
       "      <td>0.899903</td>\n",
       "      <td>00:21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>77</td>\n",
       "      <td>0.026512</td>\n",
       "      <td>0.104154</td>\n",
       "      <td>0.060265</td>\n",
       "      <td>0.104154</td>\n",
       "      <td>0.901699</td>\n",
       "      <td>00:21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>78</td>\n",
       "      <td>0.026110</td>\n",
       "      <td>0.107336</td>\n",
       "      <td>0.061036</td>\n",
       "      <td>0.107336</td>\n",
       "      <td>0.901264</td>\n",
       "      <td>00:21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>79</td>\n",
       "      <td>0.025720</td>\n",
       "      <td>0.094685</td>\n",
       "      <td>0.061092</td>\n",
       "      <td>0.094685</td>\n",
       "      <td>0.900786</td>\n",
       "      <td>00:22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>0.025266</td>\n",
       "      <td>0.098863</td>\n",
       "      <td>0.061189</td>\n",
       "      <td>0.098863</td>\n",
       "      <td>0.901501</td>\n",
       "      <td>00:22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>81</td>\n",
       "      <td>0.024932</td>\n",
       "      <td>0.101684</td>\n",
       "      <td>0.061066</td>\n",
       "      <td>0.101684</td>\n",
       "      <td>0.901789</td>\n",
       "      <td>00:21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>82</td>\n",
       "      <td>0.024533</td>\n",
       "      <td>0.099854</td>\n",
       "      <td>0.060989</td>\n",
       "      <td>0.099854</td>\n",
       "      <td>0.901220</td>\n",
       "      <td>00:21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>83</td>\n",
       "      <td>0.024307</td>\n",
       "      <td>0.107053</td>\n",
       "      <td>0.058917</td>\n",
       "      <td>0.107053</td>\n",
       "      <td>0.903453</td>\n",
       "      <td>00:21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>84</td>\n",
       "      <td>0.024054</td>\n",
       "      <td>0.096136</td>\n",
       "      <td>0.059149</td>\n",
       "      <td>0.096136</td>\n",
       "      <td>0.902611</td>\n",
       "      <td>00:22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>85</td>\n",
       "      <td>0.023918</td>\n",
       "      <td>0.097631</td>\n",
       "      <td>0.058831</td>\n",
       "      <td>0.097631</td>\n",
       "      <td>0.903329</td>\n",
       "      <td>00:22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>86</td>\n",
       "      <td>0.023618</td>\n",
       "      <td>0.104318</td>\n",
       "      <td>0.057479</td>\n",
       "      <td>0.104318</td>\n",
       "      <td>0.905630</td>\n",
       "      <td>00:21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>87</td>\n",
       "      <td>0.023334</td>\n",
       "      <td>0.101933</td>\n",
       "      <td>0.059159</td>\n",
       "      <td>0.101933</td>\n",
       "      <td>0.903471</td>\n",
       "      <td>00:22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>88</td>\n",
       "      <td>0.023012</td>\n",
       "      <td>0.098839</td>\n",
       "      <td>0.059094</td>\n",
       "      <td>0.098839</td>\n",
       "      <td>0.902594</td>\n",
       "      <td>00:21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>89</td>\n",
       "      <td>0.022821</td>\n",
       "      <td>0.123493</td>\n",
       "      <td>0.059529</td>\n",
       "      <td>0.123493</td>\n",
       "      <td>0.902434</td>\n",
       "      <td>00:22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>0.022566</td>\n",
       "      <td>0.103852</td>\n",
       "      <td>0.058065</td>\n",
       "      <td>0.103852</td>\n",
       "      <td>0.902222</td>\n",
       "      <td>00:22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>91</td>\n",
       "      <td>0.022403</td>\n",
       "      <td>0.098412</td>\n",
       "      <td>0.057379</td>\n",
       "      <td>0.098412</td>\n",
       "      <td>0.903987</td>\n",
       "      <td>00:21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>92</td>\n",
       "      <td>0.022255</td>\n",
       "      <td>0.091435</td>\n",
       "      <td>0.058705</td>\n",
       "      <td>0.091435</td>\n",
       "      <td>0.901578</td>\n",
       "      <td>00:21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>93</td>\n",
       "      <td>0.022152</td>\n",
       "      <td>0.108014</td>\n",
       "      <td>0.056552</td>\n",
       "      <td>0.108014</td>\n",
       "      <td>0.906899</td>\n",
       "      <td>00:21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>94</td>\n",
       "      <td>0.022077</td>\n",
       "      <td>0.113753</td>\n",
       "      <td>0.057779</td>\n",
       "      <td>0.113753</td>\n",
       "      <td>0.904061</td>\n",
       "      <td>00:22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>95</td>\n",
       "      <td>0.022025</td>\n",
       "      <td>0.110065</td>\n",
       "      <td>0.058284</td>\n",
       "      <td>0.110065</td>\n",
       "      <td>0.903958</td>\n",
       "      <td>00:22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>96</td>\n",
       "      <td>0.021828</td>\n",
       "      <td>0.097327</td>\n",
       "      <td>0.058406</td>\n",
       "      <td>0.097327</td>\n",
       "      <td>0.904207</td>\n",
       "      <td>00:21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>97</td>\n",
       "      <td>0.021562</td>\n",
       "      <td>0.100249</td>\n",
       "      <td>0.057109</td>\n",
       "      <td>0.100249</td>\n",
       "      <td>0.905008</td>\n",
       "      <td>00:22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>98</td>\n",
       "      <td>0.021275</td>\n",
       "      <td>0.106066</td>\n",
       "      <td>0.055212</td>\n",
       "      <td>0.106066</td>\n",
       "      <td>0.907920</td>\n",
       "      <td>00:21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>99</td>\n",
       "      <td>0.021498</td>\n",
       "      <td>0.137563</td>\n",
       "      <td>0.059368</td>\n",
       "      <td>0.137563</td>\n",
       "      <td>0.902454</td>\n",
       "      <td>00:22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.022137</td>\n",
       "      <td>0.131186</td>\n",
       "      <td>0.056329</td>\n",
       "      <td>0.131186</td>\n",
       "      <td>0.906488</td>\n",
       "      <td>00:22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>101</td>\n",
       "      <td>0.022099</td>\n",
       "      <td>0.105226</td>\n",
       "      <td>0.057251</td>\n",
       "      <td>0.105226</td>\n",
       "      <td>0.905242</td>\n",
       "      <td>00:22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>102</td>\n",
       "      <td>0.021933</td>\n",
       "      <td>0.097146</td>\n",
       "      <td>0.055976</td>\n",
       "      <td>0.097146</td>\n",
       "      <td>0.906863</td>\n",
       "      <td>00:21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>103</td>\n",
       "      <td>0.021671</td>\n",
       "      <td>0.098890</td>\n",
       "      <td>0.056030</td>\n",
       "      <td>0.098890</td>\n",
       "      <td>0.908151</td>\n",
       "      <td>00:21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>104</td>\n",
       "      <td>0.021299</td>\n",
       "      <td>0.094396</td>\n",
       "      <td>0.055886</td>\n",
       "      <td>0.094396</td>\n",
       "      <td>0.906603</td>\n",
       "      <td>00:22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>105</td>\n",
       "      <td>0.020953</td>\n",
       "      <td>0.097735</td>\n",
       "      <td>0.055572</td>\n",
       "      <td>0.097735</td>\n",
       "      <td>0.905931</td>\n",
       "      <td>00:22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>106</td>\n",
       "      <td>0.020633</td>\n",
       "      <td>0.100192</td>\n",
       "      <td>0.054060</td>\n",
       "      <td>0.100192</td>\n",
       "      <td>0.908702</td>\n",
       "      <td>00:22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>107</td>\n",
       "      <td>0.020340</td>\n",
       "      <td>0.098862</td>\n",
       "      <td>0.054439</td>\n",
       "      <td>0.098862</td>\n",
       "      <td>0.908767</td>\n",
       "      <td>00:21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>108</td>\n",
       "      <td>0.020145</td>\n",
       "      <td>0.098356</td>\n",
       "      <td>0.054162</td>\n",
       "      <td>0.098356</td>\n",
       "      <td>0.909963</td>\n",
       "      <td>00:22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>109</td>\n",
       "      <td>0.019931</td>\n",
       "      <td>0.103292</td>\n",
       "      <td>0.054500</td>\n",
       "      <td>0.103292</td>\n",
       "      <td>0.908801</td>\n",
       "      <td>00:22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>110</td>\n",
       "      <td>0.019898</td>\n",
       "      <td>0.106012</td>\n",
       "      <td>0.053645</td>\n",
       "      <td>0.106012</td>\n",
       "      <td>0.910120</td>\n",
       "      <td>00:22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>111</td>\n",
       "      <td>0.019761</td>\n",
       "      <td>0.101984</td>\n",
       "      <td>0.053577</td>\n",
       "      <td>0.101984</td>\n",
       "      <td>0.910060</td>\n",
       "      <td>00:21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>112</td>\n",
       "      <td>0.019599</td>\n",
       "      <td>0.101570</td>\n",
       "      <td>0.053944</td>\n",
       "      <td>0.101570</td>\n",
       "      <td>0.909432</td>\n",
       "      <td>00:21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>113</td>\n",
       "      <td>0.019464</td>\n",
       "      <td>0.099493</td>\n",
       "      <td>0.054042</td>\n",
       "      <td>0.099493</td>\n",
       "      <td>0.908372</td>\n",
       "      <td>00:21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>114</td>\n",
       "      <td>0.019392</td>\n",
       "      <td>0.098949</td>\n",
       "      <td>0.053915</td>\n",
       "      <td>0.098949</td>\n",
       "      <td>0.909377</td>\n",
       "      <td>00:22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>115</td>\n",
       "      <td>0.019284</td>\n",
       "      <td>0.106288</td>\n",
       "      <td>0.054372</td>\n",
       "      <td>0.106288</td>\n",
       "      <td>0.908531</td>\n",
       "      <td>00:21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>116</td>\n",
       "      <td>0.019218</td>\n",
       "      <td>0.113249</td>\n",
       "      <td>0.055281</td>\n",
       "      <td>0.113249</td>\n",
       "      <td>0.906064</td>\n",
       "      <td>00:22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>117</td>\n",
       "      <td>0.019116</td>\n",
       "      <td>0.097393</td>\n",
       "      <td>0.054328</td>\n",
       "      <td>0.097393</td>\n",
       "      <td>0.907599</td>\n",
       "      <td>00:21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>118</td>\n",
       "      <td>0.019149</td>\n",
       "      <td>0.107905</td>\n",
       "      <td>0.053892</td>\n",
       "      <td>0.107905</td>\n",
       "      <td>0.909372</td>\n",
       "      <td>00:21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>119</td>\n",
       "      <td>0.019212</td>\n",
       "      <td>0.101590</td>\n",
       "      <td>0.054151</td>\n",
       "      <td>0.101590</td>\n",
       "      <td>0.908214</td>\n",
       "      <td>00:22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120</td>\n",
       "      <td>0.019303</td>\n",
       "      <td>0.103073</td>\n",
       "      <td>0.054788</td>\n",
       "      <td>0.103073</td>\n",
       "      <td>0.907161</td>\n",
       "      <td>00:22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>121</td>\n",
       "      <td>0.019162</td>\n",
       "      <td>0.106222</td>\n",
       "      <td>0.054804</td>\n",
       "      <td>0.106222</td>\n",
       "      <td>0.907716</td>\n",
       "      <td>00:21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>122</td>\n",
       "      <td>0.019271</td>\n",
       "      <td>0.106229</td>\n",
       "      <td>0.054915</td>\n",
       "      <td>0.106229</td>\n",
       "      <td>0.908045</td>\n",
       "      <td>00:21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>123</td>\n",
       "      <td>0.019141</td>\n",
       "      <td>0.105883</td>\n",
       "      <td>0.054908</td>\n",
       "      <td>0.105883</td>\n",
       "      <td>0.908793</td>\n",
       "      <td>00:21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>124</td>\n",
       "      <td>0.019140</td>\n",
       "      <td>0.109962</td>\n",
       "      <td>0.055034</td>\n",
       "      <td>0.109962</td>\n",
       "      <td>0.907721</td>\n",
       "      <td>00:22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>125</td>\n",
       "      <td>0.019119</td>\n",
       "      <td>0.106963</td>\n",
       "      <td>0.054831</td>\n",
       "      <td>0.106963</td>\n",
       "      <td>0.907653</td>\n",
       "      <td>00:22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>126</td>\n",
       "      <td>0.019112</td>\n",
       "      <td>0.107503</td>\n",
       "      <td>0.054615</td>\n",
       "      <td>0.107503</td>\n",
       "      <td>0.908263</td>\n",
       "      <td>00:21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>127</td>\n",
       "      <td>0.019200</td>\n",
       "      <td>0.111396</td>\n",
       "      <td>0.054916</td>\n",
       "      <td>0.111396</td>\n",
       "      <td>0.907964</td>\n",
       "      <td>00:21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>128</td>\n",
       "      <td>0.019071</td>\n",
       "      <td>0.106644</td>\n",
       "      <td>0.053751</td>\n",
       "      <td>0.106644</td>\n",
       "      <td>0.908654</td>\n",
       "      <td>00:21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>129</td>\n",
       "      <td>0.019024</td>\n",
       "      <td>0.104073</td>\n",
       "      <td>0.053999</td>\n",
       "      <td>0.104073</td>\n",
       "      <td>0.909150</td>\n",
       "      <td>00:22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>130</td>\n",
       "      <td>0.019010</td>\n",
       "      <td>0.104129</td>\n",
       "      <td>0.053723</td>\n",
       "      <td>0.104129</td>\n",
       "      <td>0.909515</td>\n",
       "      <td>00:22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>131</td>\n",
       "      <td>0.018936</td>\n",
       "      <td>0.100377</td>\n",
       "      <td>0.054336</td>\n",
       "      <td>0.100377</td>\n",
       "      <td>0.908236</td>\n",
       "      <td>00:21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>132</td>\n",
       "      <td>0.018829</td>\n",
       "      <td>0.106595</td>\n",
       "      <td>0.053370</td>\n",
       "      <td>0.106595</td>\n",
       "      <td>0.909536</td>\n",
       "      <td>00:21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>133</td>\n",
       "      <td>0.018794</td>\n",
       "      <td>0.100242</td>\n",
       "      <td>0.054467</td>\n",
       "      <td>0.100242</td>\n",
       "      <td>0.907887</td>\n",
       "      <td>00:21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>134</td>\n",
       "      <td>0.018909</td>\n",
       "      <td>0.104881</td>\n",
       "      <td>0.054661</td>\n",
       "      <td>0.104881</td>\n",
       "      <td>0.908392</td>\n",
       "      <td>00:22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>135</td>\n",
       "      <td>0.018833</td>\n",
       "      <td>0.105890</td>\n",
       "      <td>0.054529</td>\n",
       "      <td>0.105890</td>\n",
       "      <td>0.907880</td>\n",
       "      <td>00:21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>136</td>\n",
       "      <td>0.018932</td>\n",
       "      <td>0.103234</td>\n",
       "      <td>0.053010</td>\n",
       "      <td>0.103234</td>\n",
       "      <td>0.909631</td>\n",
       "      <td>00:21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>137</td>\n",
       "      <td>0.018919</td>\n",
       "      <td>0.106869</td>\n",
       "      <td>0.052901</td>\n",
       "      <td>0.106869</td>\n",
       "      <td>0.910584</td>\n",
       "      <td>00:22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>138</td>\n",
       "      <td>0.019012</td>\n",
       "      <td>0.104852</td>\n",
       "      <td>0.054397</td>\n",
       "      <td>0.104852</td>\n",
       "      <td>0.907457</td>\n",
       "      <td>00:21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>139</td>\n",
       "      <td>0.018986</td>\n",
       "      <td>0.107122</td>\n",
       "      <td>0.053810</td>\n",
       "      <td>0.107122</td>\n",
       "      <td>0.908415</td>\n",
       "      <td>00:22</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>\n",
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "                background: #F44336;\n",
       "            }\n",
       "        </style>\n",
       "      <progress value='3' class='' max='3', style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      100.00% [3/3 00:02<00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Better model found at epoch 0 with valid_loss value: 0.6850839257240295.\n",
      "Better model found at epoch 1 with valid_loss value: 0.6716548204421997.\n",
      "Better model found at epoch 2 with valid_loss value: 0.6136173009872437.\n",
      "Better model found at epoch 3 with valid_loss value: 0.5144469141960144.\n",
      "Better model found at epoch 4 with valid_loss value: 0.46733856201171875.\n",
      "Better model found at epoch 5 with valid_loss value: 0.3340119421482086.\n",
      "Better model found at epoch 6 with valid_loss value: 0.2773037254810333.\n",
      "Better model found at epoch 8 with valid_loss value: 0.2169274389743805.\n",
      "Better model found at epoch 10 with valid_loss value: 0.1666257083415985.\n",
      "Better model found at epoch 12 with valid_loss value: 0.1479007452726364.\n",
      "Better model found at epoch 13 with valid_loss value: 0.1400548666715622.\n",
      "Better model found at epoch 17 with valid_loss value: 0.1121641993522644.\n",
      "Better model found at epoch 20 with valid_loss value: 0.10273797810077667.\n",
      "Better model found at epoch 25 with valid_loss value: 0.09130605310201645.\n",
      "Better model found at epoch 30 with valid_loss value: 0.08576591312885284.\n",
      "Better model found at epoch 38 with valid_loss value: 0.0812859982252121.\n",
      "Better model found at epoch 43 with valid_loss value: 0.07276050746440887.\n",
      "on end of epoch#57: start annealing from 0.001 to 0.0001\n",
      "on end of epoch#103: start annealing from 0.0001 to 1e-05\n",
      "on end of epoch#138: start annealing from 1e-05 to 1.0000000000000002e-06\n"
     ]
    }
   ],
   "source": [
    "lrs = [1e-3]\n",
    "checkpoints = [None]\n",
    "opts = [optf_adam]\n",
    "\n",
    "nb_train_script_logger.multi_train(get_learn=getlearn, \n",
    "            epoch_len=1e9, epochs=500,\n",
    "            opts=opts, lrs=lrs, checkpoints=checkpoints,\n",
    "            tb_log_root='./tb_log/',\n",
    "            autoSave=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "backup_train_logs(tag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs:139,train_loss:0.019,mask_iou:0.908"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 同样的resnet18,200数据集，adam，allres情况下，balance_bce(balance_ratio=1)的mask_iou=0.908低于dice_loss的0.922.收敛速度快于dice_loss 139:240epochs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 200数据集，无transform，resnet18，vanila，balance_bce，adam，balance_ratio=10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "tag = '__'.join(['resnet18', 'vanila', 'balance_bce', 'dataset200','adam', 'balance_ratio_10'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "getlearn = partial(get_learn, data = data_200, model_name = 'resnet18'\n",
    "                , loss_func_name = 'balance_bce', allres = False, balance_ratio = 10, tag = tag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Learner(data=ImageDataBunch;\n",
       "\n",
       "Train: LabelList (160 items)\n",
       "x: SegmentationItemList\n",
       "Image (3, 512, 512),Image (3, 512, 512),Image (3, 512, 512),Image (3, 512, 512),Image (3, 512, 512)\n",
       "y: SegmentationLabelList\n",
       "ImageSegment (1, 512, 512),ImageSegment (1, 512, 512),ImageSegment (1, 512, 512),ImageSegment (1, 512, 512),ImageSegment (1, 512, 512)\n",
       "Path: data/dataset_20200715/image;\n",
       "\n",
       "Valid: LabelList (40 items)\n",
       "x: SegmentationItemList\n",
       "Image (3, 512, 512),Image (3, 512, 512),Image (3, 512, 512),Image (3, 512, 512),Image (3, 512, 512)\n",
       "y: SegmentationLabelList\n",
       "ImageSegment (1, 512, 512),ImageSegment (1, 512, 512),ImageSegment (1, 512, 512),ImageSegment (1, 512, 512),ImageSegment (1, 512, 512)\n",
       "Path: data/dataset_20200715/image;\n",
       "\n",
       "Test: None, model=DataParallel(\n",
       "  (module): Resnet_UNet(\n",
       "    (down_blocks): ModuleList(\n",
       "      (0): Sequential(\n",
       "        (0): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "        (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU(inplace=True)\n",
       "      )\n",
       "      (1): Sequential(\n",
       "        (0): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "        (1): Sequential(\n",
       "          (0): BasicBlock(\n",
       "            (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "            (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (relu): ReLU(inplace=True)\n",
       "            (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "            (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "          (1): BasicBlock(\n",
       "            (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "            (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (relu): ReLU(inplace=True)\n",
       "            (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "            (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (2): Sequential(\n",
       "        (0): BasicBlock(\n",
       "          (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (downsample): Sequential(\n",
       "            (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "            (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (1): BasicBlock(\n",
       "          (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (3): Sequential(\n",
       "        (0): BasicBlock(\n",
       "          (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (downsample): Sequential(\n",
       "            (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "            (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (1): BasicBlock(\n",
       "          (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (4): Sequential(\n",
       "        (0): BasicBlock(\n",
       "          (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (downsample): Sequential(\n",
       "            (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "            (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (1): BasicBlock(\n",
       "          (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (bridge): Bridge(\n",
       "      (conv1): Conv_Bn_ReLu(\n",
       "        (conv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (bn): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (conv2): Conv_Bn_ReLu(\n",
       "        (conv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (bn): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (up_blocks): ModuleList(\n",
       "      (0): UpBlock(\n",
       "        (upsample): Sequential(\n",
       "          (0): ConvTranspose2d(512, 256, kernel_size=(2, 2), stride=(2, 2), bias=False)\n",
       "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (conv1): Conv_Bn_ReLu(\n",
       "          (conv): Conv2d(512, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (conv2): Conv_Bn_ReLu(\n",
       "          (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): UpBlock(\n",
       "        (upsample): Sequential(\n",
       "          (0): ConvTranspose2d(256, 128, kernel_size=(2, 2), stride=(2, 2), bias=False)\n",
       "          (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (conv1): Conv_Bn_ReLu(\n",
       "          (conv): Conv2d(256, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (conv2): Conv_Bn_ReLu(\n",
       "          (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (2): UpBlock(\n",
       "        (upsample): Sequential(\n",
       "          (0): ConvTranspose2d(128, 128, kernel_size=(2, 2), stride=(2, 2), bias=False)\n",
       "          (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (conv1): Conv_Bn_ReLu(\n",
       "          (conv): Conv2d(192, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (conv2): Conv_Bn_ReLu(\n",
       "          (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (3): UpBlock(\n",
       "        (upsample): Sequential(\n",
       "          (0): ConvTranspose2d(128, 128, kernel_size=(2, 2), stride=(2, 2), bias=False)\n",
       "          (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (conv1): Conv_Bn_ReLu(\n",
       "          (conv): Conv2d(192, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (conv2): Conv_Bn_ReLu(\n",
       "          (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (4): UpBlock(\n",
       "        (upsample): Sequential(\n",
       "          (0): ConvTranspose2d(128, 128, kernel_size=(2, 2), stride=(2, 2), bias=False)\n",
       "          (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (conv1): Conv_Bn_ReLu(\n",
       "          (conv): Conv2d(131, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (conv2): Conv_Bn_ReLu(\n",
       "          (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (head): Conv2d(128, 1, kernel_size=(1, 1), stride=(1, 1))\n",
       "  )\n",
       "), opt_func=functools.partial(<class 'torch.optim.adam.Adam'>, betas=(0.9, 0.99)), loss_func=functools.partial(<function balance_bce at 0x7f90e485f840>, balance_ratio=10), metrics=[<function dice_loss at 0x7f90e485f730>, functools.partial(<function balance_bce at 0x7f90e485f840>, balance_ratio=1), <function mask_iou at 0x7f90e485f950>], true_wd=True, bn_wd=True, wd=0.01, train_bn=True, path=PosixPath('data/dataset_20200715/image'), model_dir='models', callback_fns=[functools.partial(<class 'fastai.basic_train.Recorder'>, add_time=True, silent=False)], callbacks=[], layer_groups=[ModuleList(\n",
       "  (0): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "  (1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "  (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "  (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "  (4): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "  (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "  (6): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "  (7): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "  (8): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "  (9): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "  (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "  (11): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "  (12): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "  (13): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "  (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "  (15): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "  (16): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "  (17): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "  (18): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "  (19): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "), ModuleList(\n",
       "  (0): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (5): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (6): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (7): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (8): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (9): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (10): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (11): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (12): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (13): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (14): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (15): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (16): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (17): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (18): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (19): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (20): Bridge(\n",
       "    (conv1): Conv_Bn_ReLu(\n",
       "      (conv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (bn): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (conv2): Conv_Bn_ReLu(\n",
       "      (conv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (bn): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (21): ModuleList(\n",
       "    (0): UpBlock(\n",
       "      (upsample): Sequential(\n",
       "        (0): ConvTranspose2d(512, 256, kernel_size=(2, 2), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (conv1): Conv_Bn_ReLu(\n",
       "        (conv): Conv2d(512, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (conv2): Conv_Bn_ReLu(\n",
       "        (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): UpBlock(\n",
       "      (upsample): Sequential(\n",
       "        (0): ConvTranspose2d(256, 128, kernel_size=(2, 2), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (conv1): Conv_Bn_ReLu(\n",
       "        (conv): Conv2d(256, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (conv2): Conv_Bn_ReLu(\n",
       "        (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (2): UpBlock(\n",
       "      (upsample): Sequential(\n",
       "        (0): ConvTranspose2d(128, 128, kernel_size=(2, 2), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (conv1): Conv_Bn_ReLu(\n",
       "        (conv): Conv2d(192, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (conv2): Conv_Bn_ReLu(\n",
       "        (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (3): UpBlock(\n",
       "      (upsample): Sequential(\n",
       "        (0): ConvTranspose2d(128, 128, kernel_size=(2, 2), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (conv1): Conv_Bn_ReLu(\n",
       "        (conv): Conv2d(192, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (conv2): Conv_Bn_ReLu(\n",
       "        (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (4): UpBlock(\n",
       "      (upsample): Sequential(\n",
       "        (0): ConvTranspose2d(128, 128, kernel_size=(2, 2), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (conv1): Conv_Bn_ReLu(\n",
       "        (conv): Conv2d(131, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (conv2): Conv_Bn_ReLu(\n",
       "        (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (22): Conv2d(128, 1, kernel_size=(1, 1), stride=(1, 1))\n",
       ")], add_time=True, silent=False)"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "getlearn()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ALLERT: You are using CumtomEpochLength, please make sure that your training dataloader is using random sampler, or this may cause problem.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "                background: #F44336;\n",
       "            }\n",
       "        </style>\n",
       "      <progress value='137' class='' max='500', style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      27.40% [137/500 53:44<2:22:24]\n",
       "    </div>\n",
       "    \n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>dice_loss</th>\n",
       "      <th>balance_bce</th>\n",
       "      <th>mask_iou</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.666880</td>\n",
       "      <td>1.076214</td>\n",
       "      <td>0.760752</td>\n",
       "      <td>0.763481</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>00:23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.425888</td>\n",
       "      <td>0.821837</td>\n",
       "      <td>0.733969</td>\n",
       "      <td>0.699618</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>00:23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.315044</td>\n",
       "      <td>0.671926</td>\n",
       "      <td>0.713530</td>\n",
       "      <td>0.669372</td>\n",
       "      <td>0.411075</td>\n",
       "      <td>00:23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.264489</td>\n",
       "      <td>0.413788</td>\n",
       "      <td>0.638105</td>\n",
       "      <td>0.547537</td>\n",
       "      <td>0.304217</td>\n",
       "      <td>00:23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.229734</td>\n",
       "      <td>0.148682</td>\n",
       "      <td>0.472243</td>\n",
       "      <td>0.332627</td>\n",
       "      <td>0.463245</td>\n",
       "      <td>00:23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.200230</td>\n",
       "      <td>0.094716</td>\n",
       "      <td>0.454933</td>\n",
       "      <td>0.476458</td>\n",
       "      <td>0.430457</td>\n",
       "      <td>00:23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.175635</td>\n",
       "      <td>0.127277</td>\n",
       "      <td>0.493930</td>\n",
       "      <td>0.679007</td>\n",
       "      <td>0.362405</td>\n",
       "      <td>00:23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.159654</td>\n",
       "      <td>0.075649</td>\n",
       "      <td>0.379768</td>\n",
       "      <td>0.332332</td>\n",
       "      <td>0.480609</td>\n",
       "      <td>00:23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.143693</td>\n",
       "      <td>0.115664</td>\n",
       "      <td>0.464226</td>\n",
       "      <td>0.615209</td>\n",
       "      <td>0.378262</td>\n",
       "      <td>00:23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.132258</td>\n",
       "      <td>0.112356</td>\n",
       "      <td>0.240325</td>\n",
       "      <td>0.168519</td>\n",
       "      <td>0.688671</td>\n",
       "      <td>00:23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.126143</td>\n",
       "      <td>0.098964</td>\n",
       "      <td>0.395153</td>\n",
       "      <td>0.514938</td>\n",
       "      <td>0.450208</td>\n",
       "      <td>00:23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.118647</td>\n",
       "      <td>0.091577</td>\n",
       "      <td>0.392660</td>\n",
       "      <td>0.469485</td>\n",
       "      <td>0.473310</td>\n",
       "      <td>00:23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.112728</td>\n",
       "      <td>0.062602</td>\n",
       "      <td>0.299350</td>\n",
       "      <td>0.228054</td>\n",
       "      <td>0.602275</td>\n",
       "      <td>00:23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.106228</td>\n",
       "      <td>0.096523</td>\n",
       "      <td>0.424101</td>\n",
       "      <td>0.498747</td>\n",
       "      <td>0.432728</td>\n",
       "      <td>00:23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.098532</td>\n",
       "      <td>0.080502</td>\n",
       "      <td>0.255724</td>\n",
       "      <td>0.176512</td>\n",
       "      <td>0.697994</td>\n",
       "      <td>00:23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.091155</td>\n",
       "      <td>0.294119</td>\n",
       "      <td>0.250427</td>\n",
       "      <td>0.223356</td>\n",
       "      <td>0.709683</td>\n",
       "      <td>00:23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.096585</td>\n",
       "      <td>0.061600</td>\n",
       "      <td>0.289145</td>\n",
       "      <td>0.255728</td>\n",
       "      <td>0.588910</td>\n",
       "      <td>00:23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.097261</td>\n",
       "      <td>0.103779</td>\n",
       "      <td>0.367625</td>\n",
       "      <td>0.383434</td>\n",
       "      <td>0.550002</td>\n",
       "      <td>00:23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.097246</td>\n",
       "      <td>0.071700</td>\n",
       "      <td>0.355848</td>\n",
       "      <td>0.304058</td>\n",
       "      <td>0.531299</td>\n",
       "      <td>00:23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.102034</td>\n",
       "      <td>0.193085</td>\n",
       "      <td>0.529147</td>\n",
       "      <td>0.907311</td>\n",
       "      <td>0.309759</td>\n",
       "      <td>00:23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.100344</td>\n",
       "      <td>0.149059</td>\n",
       "      <td>0.401969</td>\n",
       "      <td>0.523278</td>\n",
       "      <td>0.458683</td>\n",
       "      <td>00:23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>0.095745</td>\n",
       "      <td>0.084398</td>\n",
       "      <td>0.270817</td>\n",
       "      <td>0.238206</td>\n",
       "      <td>0.632803</td>\n",
       "      <td>00:23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>0.090749</td>\n",
       "      <td>0.071892</td>\n",
       "      <td>0.234937</td>\n",
       "      <td>0.177571</td>\n",
       "      <td>0.684470</td>\n",
       "      <td>00:23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>0.085785</td>\n",
       "      <td>0.053645</td>\n",
       "      <td>0.283933</td>\n",
       "      <td>0.220462</td>\n",
       "      <td>0.585136</td>\n",
       "      <td>00:23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>0.083408</td>\n",
       "      <td>0.084091</td>\n",
       "      <td>0.241435</td>\n",
       "      <td>0.160267</td>\n",
       "      <td>0.689522</td>\n",
       "      <td>00:23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>0.082187</td>\n",
       "      <td>0.092932</td>\n",
       "      <td>0.318505</td>\n",
       "      <td>0.263293</td>\n",
       "      <td>0.557278</td>\n",
       "      <td>00:23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>0.082791</td>\n",
       "      <td>0.088271</td>\n",
       "      <td>0.335799</td>\n",
       "      <td>0.272898</td>\n",
       "      <td>0.548323</td>\n",
       "      <td>00:23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>0.082021</td>\n",
       "      <td>0.072030</td>\n",
       "      <td>0.352148</td>\n",
       "      <td>0.362755</td>\n",
       "      <td>0.512301</td>\n",
       "      <td>00:23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28</td>\n",
       "      <td>0.076380</td>\n",
       "      <td>0.080262</td>\n",
       "      <td>0.273371</td>\n",
       "      <td>0.185233</td>\n",
       "      <td>0.680496</td>\n",
       "      <td>00:23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29</td>\n",
       "      <td>0.070734</td>\n",
       "      <td>0.051223</td>\n",
       "      <td>0.237942</td>\n",
       "      <td>0.180030</td>\n",
       "      <td>0.688829</td>\n",
       "      <td>00:23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>0.065213</td>\n",
       "      <td>0.048986</td>\n",
       "      <td>0.218567</td>\n",
       "      <td>0.159908</td>\n",
       "      <td>0.731926</td>\n",
       "      <td>00:23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31</td>\n",
       "      <td>0.060048</td>\n",
       "      <td>0.077885</td>\n",
       "      <td>0.170540</td>\n",
       "      <td>0.115400</td>\n",
       "      <td>0.808707</td>\n",
       "      <td>00:23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32</td>\n",
       "      <td>0.055458</td>\n",
       "      <td>0.065610</td>\n",
       "      <td>0.172694</td>\n",
       "      <td>0.112967</td>\n",
       "      <td>0.803631</td>\n",
       "      <td>00:23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33</td>\n",
       "      <td>0.051242</td>\n",
       "      <td>0.170158</td>\n",
       "      <td>0.240780</td>\n",
       "      <td>0.212941</td>\n",
       "      <td>0.668467</td>\n",
       "      <td>00:23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34</td>\n",
       "      <td>0.048928</td>\n",
       "      <td>0.045654</td>\n",
       "      <td>0.220685</td>\n",
       "      <td>0.189759</td>\n",
       "      <td>0.674762</td>\n",
       "      <td>00:23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35</td>\n",
       "      <td>0.047475</td>\n",
       "      <td>0.058176</td>\n",
       "      <td>0.249630</td>\n",
       "      <td>0.224947</td>\n",
       "      <td>0.634601</td>\n",
       "      <td>00:23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36</td>\n",
       "      <td>0.049087</td>\n",
       "      <td>0.057426</td>\n",
       "      <td>0.305502</td>\n",
       "      <td>0.283791</td>\n",
       "      <td>0.544221</td>\n",
       "      <td>00:23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>37</td>\n",
       "      <td>0.047855</td>\n",
       "      <td>0.064603</td>\n",
       "      <td>0.322190</td>\n",
       "      <td>0.341009</td>\n",
       "      <td>0.558851</td>\n",
       "      <td>00:23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>38</td>\n",
       "      <td>0.060499</td>\n",
       "      <td>0.332440</td>\n",
       "      <td>0.224481</td>\n",
       "      <td>0.316104</td>\n",
       "      <td>0.663129</td>\n",
       "      <td>00:23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>39</td>\n",
       "      <td>0.073166</td>\n",
       "      <td>1.463101</td>\n",
       "      <td>0.233034</td>\n",
       "      <td>0.920071</td>\n",
       "      <td>0.639545</td>\n",
       "      <td>00:23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>0.078610</td>\n",
       "      <td>0.184351</td>\n",
       "      <td>0.473884</td>\n",
       "      <td>0.852691</td>\n",
       "      <td>0.352299</td>\n",
       "      <td>00:23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>41</td>\n",
       "      <td>0.076853</td>\n",
       "      <td>0.086790</td>\n",
       "      <td>0.325903</td>\n",
       "      <td>0.326214</td>\n",
       "      <td>0.518464</td>\n",
       "      <td>00:23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>42</td>\n",
       "      <td>0.073683</td>\n",
       "      <td>0.066230</td>\n",
       "      <td>0.301017</td>\n",
       "      <td>0.281076</td>\n",
       "      <td>0.555037</td>\n",
       "      <td>00:23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>43</td>\n",
       "      <td>0.069527</td>\n",
       "      <td>0.059392</td>\n",
       "      <td>0.268349</td>\n",
       "      <td>0.226950</td>\n",
       "      <td>0.603437</td>\n",
       "      <td>00:23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>44</td>\n",
       "      <td>0.065797</td>\n",
       "      <td>0.058089</td>\n",
       "      <td>0.240953</td>\n",
       "      <td>0.183988</td>\n",
       "      <td>0.658361</td>\n",
       "      <td>00:23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>45</td>\n",
       "      <td>0.074968</td>\n",
       "      <td>0.073632</td>\n",
       "      <td>0.218416</td>\n",
       "      <td>0.149162</td>\n",
       "      <td>0.708557</td>\n",
       "      <td>00:23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>46</td>\n",
       "      <td>0.069858</td>\n",
       "      <td>0.089422</td>\n",
       "      <td>0.215533</td>\n",
       "      <td>0.144860</td>\n",
       "      <td>0.735805</td>\n",
       "      <td>00:23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>47</td>\n",
       "      <td>0.066541</td>\n",
       "      <td>0.057554</td>\n",
       "      <td>0.234253</td>\n",
       "      <td>0.161575</td>\n",
       "      <td>0.693568</td>\n",
       "      <td>00:23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>48</td>\n",
       "      <td>0.062248</td>\n",
       "      <td>0.049882</td>\n",
       "      <td>0.241325</td>\n",
       "      <td>0.176395</td>\n",
       "      <td>0.679429</td>\n",
       "      <td>00:23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>49</td>\n",
       "      <td>0.058678</td>\n",
       "      <td>0.048587</td>\n",
       "      <td>0.244157</td>\n",
       "      <td>0.175639</td>\n",
       "      <td>0.689546</td>\n",
       "      <td>00:23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.055734</td>\n",
       "      <td>0.050515</td>\n",
       "      <td>0.213810</td>\n",
       "      <td>0.153555</td>\n",
       "      <td>0.720972</td>\n",
       "      <td>00:23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>51</td>\n",
       "      <td>0.057481</td>\n",
       "      <td>0.067057</td>\n",
       "      <td>0.246333</td>\n",
       "      <td>0.176235</td>\n",
       "      <td>0.682251</td>\n",
       "      <td>00:23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>52</td>\n",
       "      <td>0.054342</td>\n",
       "      <td>0.058677</td>\n",
       "      <td>0.228125</td>\n",
       "      <td>0.155837</td>\n",
       "      <td>0.728472</td>\n",
       "      <td>00:23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>53</td>\n",
       "      <td>0.051384</td>\n",
       "      <td>0.052154</td>\n",
       "      <td>0.219460</td>\n",
       "      <td>0.147633</td>\n",
       "      <td>0.737965</td>\n",
       "      <td>00:23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>54</td>\n",
       "      <td>0.048778</td>\n",
       "      <td>0.049772</td>\n",
       "      <td>0.210221</td>\n",
       "      <td>0.153355</td>\n",
       "      <td>0.738283</td>\n",
       "      <td>00:23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>55</td>\n",
       "      <td>0.045905</td>\n",
       "      <td>0.048830</td>\n",
       "      <td>0.211640</td>\n",
       "      <td>0.152400</td>\n",
       "      <td>0.737598</td>\n",
       "      <td>00:23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>56</td>\n",
       "      <td>0.044119</td>\n",
       "      <td>0.050103</td>\n",
       "      <td>0.210928</td>\n",
       "      <td>0.153782</td>\n",
       "      <td>0.729543</td>\n",
       "      <td>00:23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>57</td>\n",
       "      <td>0.041950</td>\n",
       "      <td>0.053049</td>\n",
       "      <td>0.195097</td>\n",
       "      <td>0.141800</td>\n",
       "      <td>0.753201</td>\n",
       "      <td>00:23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>58</td>\n",
       "      <td>0.039987</td>\n",
       "      <td>0.054256</td>\n",
       "      <td>0.190768</td>\n",
       "      <td>0.132709</td>\n",
       "      <td>0.762589</td>\n",
       "      <td>00:23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>59</td>\n",
       "      <td>0.038925</td>\n",
       "      <td>0.049275</td>\n",
       "      <td>0.189957</td>\n",
       "      <td>0.148760</td>\n",
       "      <td>0.744614</td>\n",
       "      <td>00:23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>0.038511</td>\n",
       "      <td>0.054192</td>\n",
       "      <td>0.175951</td>\n",
       "      <td>0.131216</td>\n",
       "      <td>0.762066</td>\n",
       "      <td>00:23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>61</td>\n",
       "      <td>0.038740</td>\n",
       "      <td>0.056107</td>\n",
       "      <td>0.174075</td>\n",
       "      <td>0.120035</td>\n",
       "      <td>0.774951</td>\n",
       "      <td>00:23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>62</td>\n",
       "      <td>0.037335</td>\n",
       "      <td>0.054129</td>\n",
       "      <td>0.184372</td>\n",
       "      <td>0.117542</td>\n",
       "      <td>0.785618</td>\n",
       "      <td>00:23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>63</td>\n",
       "      <td>0.036163</td>\n",
       "      <td>0.047353</td>\n",
       "      <td>0.198508</td>\n",
       "      <td>0.138676</td>\n",
       "      <td>0.763043</td>\n",
       "      <td>00:23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>64</td>\n",
       "      <td>0.034735</td>\n",
       "      <td>0.046458</td>\n",
       "      <td>0.190982</td>\n",
       "      <td>0.143608</td>\n",
       "      <td>0.761123</td>\n",
       "      <td>00:23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>65</td>\n",
       "      <td>0.033902</td>\n",
       "      <td>0.049226</td>\n",
       "      <td>0.179551</td>\n",
       "      <td>0.130521</td>\n",
       "      <td>0.776976</td>\n",
       "      <td>00:23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>66</td>\n",
       "      <td>0.032606</td>\n",
       "      <td>0.056705</td>\n",
       "      <td>0.158106</td>\n",
       "      <td>0.115910</td>\n",
       "      <td>0.798491</td>\n",
       "      <td>00:23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>67</td>\n",
       "      <td>0.031539</td>\n",
       "      <td>0.061155</td>\n",
       "      <td>0.154101</td>\n",
       "      <td>0.112835</td>\n",
       "      <td>0.802528</td>\n",
       "      <td>00:23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>68</td>\n",
       "      <td>0.030285</td>\n",
       "      <td>0.055897</td>\n",
       "      <td>0.156826</td>\n",
       "      <td>0.115764</td>\n",
       "      <td>0.798782</td>\n",
       "      <td>00:23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>69</td>\n",
       "      <td>0.029454</td>\n",
       "      <td>0.053476</td>\n",
       "      <td>0.161315</td>\n",
       "      <td>0.114367</td>\n",
       "      <td>0.799947</td>\n",
       "      <td>00:23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>0.028327</td>\n",
       "      <td>0.052014</td>\n",
       "      <td>0.157311</td>\n",
       "      <td>0.113094</td>\n",
       "      <td>0.803050</td>\n",
       "      <td>00:23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>71</td>\n",
       "      <td>0.027826</td>\n",
       "      <td>0.057263</td>\n",
       "      <td>0.150941</td>\n",
       "      <td>0.106464</td>\n",
       "      <td>0.814632</td>\n",
       "      <td>00:23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>72</td>\n",
       "      <td>0.027229</td>\n",
       "      <td>0.057723</td>\n",
       "      <td>0.149038</td>\n",
       "      <td>0.113473</td>\n",
       "      <td>0.810200</td>\n",
       "      <td>00:23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>73</td>\n",
       "      <td>0.026493</td>\n",
       "      <td>0.055454</td>\n",
       "      <td>0.143746</td>\n",
       "      <td>0.106333</td>\n",
       "      <td>0.808833</td>\n",
       "      <td>00:23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>74</td>\n",
       "      <td>0.025786</td>\n",
       "      <td>0.058468</td>\n",
       "      <td>0.139078</td>\n",
       "      <td>0.106131</td>\n",
       "      <td>0.816674</td>\n",
       "      <td>00:23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>75</td>\n",
       "      <td>0.025568</td>\n",
       "      <td>0.060042</td>\n",
       "      <td>0.140358</td>\n",
       "      <td>0.105272</td>\n",
       "      <td>0.821661</td>\n",
       "      <td>00:23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>76</td>\n",
       "      <td>0.024766</td>\n",
       "      <td>0.062551</td>\n",
       "      <td>0.134123</td>\n",
       "      <td>0.100743</td>\n",
       "      <td>0.829671</td>\n",
       "      <td>00:23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>77</td>\n",
       "      <td>0.024354</td>\n",
       "      <td>0.054497</td>\n",
       "      <td>0.138369</td>\n",
       "      <td>0.102572</td>\n",
       "      <td>0.824001</td>\n",
       "      <td>00:23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>78</td>\n",
       "      <td>0.023782</td>\n",
       "      <td>0.061997</td>\n",
       "      <td>0.146582</td>\n",
       "      <td>0.110302</td>\n",
       "      <td>0.815295</td>\n",
       "      <td>00:23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>79</td>\n",
       "      <td>0.023284</td>\n",
       "      <td>0.059094</td>\n",
       "      <td>0.141187</td>\n",
       "      <td>0.105334</td>\n",
       "      <td>0.818704</td>\n",
       "      <td>00:23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>0.023139</td>\n",
       "      <td>0.053292</td>\n",
       "      <td>0.145445</td>\n",
       "      <td>0.113753</td>\n",
       "      <td>0.808104</td>\n",
       "      <td>00:23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>81</td>\n",
       "      <td>0.022538</td>\n",
       "      <td>0.059692</td>\n",
       "      <td>0.128392</td>\n",
       "      <td>0.102078</td>\n",
       "      <td>0.823202</td>\n",
       "      <td>00:23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>82</td>\n",
       "      <td>0.022175</td>\n",
       "      <td>0.062826</td>\n",
       "      <td>0.123755</td>\n",
       "      <td>0.098431</td>\n",
       "      <td>0.832106</td>\n",
       "      <td>00:23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>83</td>\n",
       "      <td>0.021728</td>\n",
       "      <td>0.057410</td>\n",
       "      <td>0.125164</td>\n",
       "      <td>0.101708</td>\n",
       "      <td>0.823445</td>\n",
       "      <td>00:23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>84</td>\n",
       "      <td>0.021344</td>\n",
       "      <td>0.059985</td>\n",
       "      <td>0.129699</td>\n",
       "      <td>0.100402</td>\n",
       "      <td>0.826929</td>\n",
       "      <td>00:23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>85</td>\n",
       "      <td>0.020931</td>\n",
       "      <td>0.065919</td>\n",
       "      <td>0.121850</td>\n",
       "      <td>0.096984</td>\n",
       "      <td>0.838057</td>\n",
       "      <td>00:23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>86</td>\n",
       "      <td>0.020917</td>\n",
       "      <td>0.055039</td>\n",
       "      <td>0.130958</td>\n",
       "      <td>0.106886</td>\n",
       "      <td>0.820888</td>\n",
       "      <td>00:23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>87</td>\n",
       "      <td>0.020654</td>\n",
       "      <td>0.067449</td>\n",
       "      <td>0.123848</td>\n",
       "      <td>0.102610</td>\n",
       "      <td>0.828409</td>\n",
       "      <td>00:23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>88</td>\n",
       "      <td>0.020266</td>\n",
       "      <td>0.081347</td>\n",
       "      <td>0.117873</td>\n",
       "      <td>0.101632</td>\n",
       "      <td>0.837449</td>\n",
       "      <td>00:23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>89</td>\n",
       "      <td>0.019598</td>\n",
       "      <td>0.075273</td>\n",
       "      <td>0.116439</td>\n",
       "      <td>0.104919</td>\n",
       "      <td>0.837146</td>\n",
       "      <td>00:23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>0.019440</td>\n",
       "      <td>0.063989</td>\n",
       "      <td>0.115690</td>\n",
       "      <td>0.095766</td>\n",
       "      <td>0.838793</td>\n",
       "      <td>00:23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>91</td>\n",
       "      <td>0.021182</td>\n",
       "      <td>0.068795</td>\n",
       "      <td>0.109881</td>\n",
       "      <td>0.091446</td>\n",
       "      <td>0.847294</td>\n",
       "      <td>00:23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>92</td>\n",
       "      <td>0.021443</td>\n",
       "      <td>0.050884</td>\n",
       "      <td>0.147189</td>\n",
       "      <td>0.101823</td>\n",
       "      <td>0.825720</td>\n",
       "      <td>00:23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>93</td>\n",
       "      <td>0.020753</td>\n",
       "      <td>0.076128</td>\n",
       "      <td>0.126494</td>\n",
       "      <td>0.100614</td>\n",
       "      <td>0.842968</td>\n",
       "      <td>00:23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>94</td>\n",
       "      <td>0.020285</td>\n",
       "      <td>0.069119</td>\n",
       "      <td>0.115311</td>\n",
       "      <td>0.099621</td>\n",
       "      <td>0.842734</td>\n",
       "      <td>00:23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>95</td>\n",
       "      <td>0.019702</td>\n",
       "      <td>0.056293</td>\n",
       "      <td>0.119566</td>\n",
       "      <td>0.095240</td>\n",
       "      <td>0.836648</td>\n",
       "      <td>00:23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>96</td>\n",
       "      <td>0.019165</td>\n",
       "      <td>0.062686</td>\n",
       "      <td>0.113943</td>\n",
       "      <td>0.090080</td>\n",
       "      <td>0.849650</td>\n",
       "      <td>00:23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>97</td>\n",
       "      <td>0.018830</td>\n",
       "      <td>0.060117</td>\n",
       "      <td>0.118105</td>\n",
       "      <td>0.090823</td>\n",
       "      <td>0.847918</td>\n",
       "      <td>00:23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>98</td>\n",
       "      <td>0.018514</td>\n",
       "      <td>0.065441</td>\n",
       "      <td>0.107506</td>\n",
       "      <td>0.095074</td>\n",
       "      <td>0.846921</td>\n",
       "      <td>00:23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>99</td>\n",
       "      <td>0.018020</td>\n",
       "      <td>0.059186</td>\n",
       "      <td>0.110110</td>\n",
       "      <td>0.093296</td>\n",
       "      <td>0.845599</td>\n",
       "      <td>00:23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.017722</td>\n",
       "      <td>0.065556</td>\n",
       "      <td>0.107980</td>\n",
       "      <td>0.094256</td>\n",
       "      <td>0.848370</td>\n",
       "      <td>00:23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>101</td>\n",
       "      <td>0.017552</td>\n",
       "      <td>0.069570</td>\n",
       "      <td>0.102578</td>\n",
       "      <td>0.091344</td>\n",
       "      <td>0.852766</td>\n",
       "      <td>00:23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>102</td>\n",
       "      <td>0.017258</td>\n",
       "      <td>0.072053</td>\n",
       "      <td>0.103246</td>\n",
       "      <td>0.091981</td>\n",
       "      <td>0.852611</td>\n",
       "      <td>00:23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>103</td>\n",
       "      <td>0.017044</td>\n",
       "      <td>0.068664</td>\n",
       "      <td>0.103495</td>\n",
       "      <td>0.094841</td>\n",
       "      <td>0.849159</td>\n",
       "      <td>00:23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>104</td>\n",
       "      <td>0.017128</td>\n",
       "      <td>0.071225</td>\n",
       "      <td>0.101028</td>\n",
       "      <td>0.093994</td>\n",
       "      <td>0.851159</td>\n",
       "      <td>00:23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>105</td>\n",
       "      <td>0.017196</td>\n",
       "      <td>0.064732</td>\n",
       "      <td>0.106599</td>\n",
       "      <td>0.093859</td>\n",
       "      <td>0.850664</td>\n",
       "      <td>00:23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>106</td>\n",
       "      <td>0.016856</td>\n",
       "      <td>0.068138</td>\n",
       "      <td>0.105268</td>\n",
       "      <td>0.091556</td>\n",
       "      <td>0.853616</td>\n",
       "      <td>00:23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>107</td>\n",
       "      <td>0.016719</td>\n",
       "      <td>0.063936</td>\n",
       "      <td>0.109142</td>\n",
       "      <td>0.094972</td>\n",
       "      <td>0.847415</td>\n",
       "      <td>00:23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>108</td>\n",
       "      <td>0.016491</td>\n",
       "      <td>0.063809</td>\n",
       "      <td>0.105557</td>\n",
       "      <td>0.093286</td>\n",
       "      <td>0.851336</td>\n",
       "      <td>00:23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>109</td>\n",
       "      <td>0.016393</td>\n",
       "      <td>0.068034</td>\n",
       "      <td>0.103666</td>\n",
       "      <td>0.092310</td>\n",
       "      <td>0.853268</td>\n",
       "      <td>00:23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>110</td>\n",
       "      <td>0.016274</td>\n",
       "      <td>0.068848</td>\n",
       "      <td>0.102290</td>\n",
       "      <td>0.092385</td>\n",
       "      <td>0.851036</td>\n",
       "      <td>00:23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>111</td>\n",
       "      <td>0.016276</td>\n",
       "      <td>0.076591</td>\n",
       "      <td>0.098907</td>\n",
       "      <td>0.094293</td>\n",
       "      <td>0.852032</td>\n",
       "      <td>00:23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>112</td>\n",
       "      <td>0.016153</td>\n",
       "      <td>0.069390</td>\n",
       "      <td>0.099987</td>\n",
       "      <td>0.093419</td>\n",
       "      <td>0.850184</td>\n",
       "      <td>00:23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>113</td>\n",
       "      <td>0.016108</td>\n",
       "      <td>0.070149</td>\n",
       "      <td>0.101665</td>\n",
       "      <td>0.093015</td>\n",
       "      <td>0.850762</td>\n",
       "      <td>00:23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>114</td>\n",
       "      <td>0.016262</td>\n",
       "      <td>0.064064</td>\n",
       "      <td>0.107310</td>\n",
       "      <td>0.099534</td>\n",
       "      <td>0.844173</td>\n",
       "      <td>00:23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>115</td>\n",
       "      <td>0.016189</td>\n",
       "      <td>0.062726</td>\n",
       "      <td>0.110322</td>\n",
       "      <td>0.096978</td>\n",
       "      <td>0.844934</td>\n",
       "      <td>00:23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>116</td>\n",
       "      <td>0.016112</td>\n",
       "      <td>0.063002</td>\n",
       "      <td>0.109015</td>\n",
       "      <td>0.093009</td>\n",
       "      <td>0.848349</td>\n",
       "      <td>00:23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>117</td>\n",
       "      <td>0.016117</td>\n",
       "      <td>0.070594</td>\n",
       "      <td>0.105370</td>\n",
       "      <td>0.088123</td>\n",
       "      <td>0.857627</td>\n",
       "      <td>00:23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>118</td>\n",
       "      <td>0.016158</td>\n",
       "      <td>0.072495</td>\n",
       "      <td>0.104748</td>\n",
       "      <td>0.093219</td>\n",
       "      <td>0.853189</td>\n",
       "      <td>00:23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>119</td>\n",
       "      <td>0.016380</td>\n",
       "      <td>0.065013</td>\n",
       "      <td>0.108147</td>\n",
       "      <td>0.094152</td>\n",
       "      <td>0.849902</td>\n",
       "      <td>00:23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120</td>\n",
       "      <td>0.016201</td>\n",
       "      <td>0.067494</td>\n",
       "      <td>0.105242</td>\n",
       "      <td>0.090799</td>\n",
       "      <td>0.856156</td>\n",
       "      <td>00:23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>121</td>\n",
       "      <td>0.016336</td>\n",
       "      <td>0.071756</td>\n",
       "      <td>0.099682</td>\n",
       "      <td>0.091743</td>\n",
       "      <td>0.856158</td>\n",
       "      <td>00:23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>122</td>\n",
       "      <td>0.016041</td>\n",
       "      <td>0.080861</td>\n",
       "      <td>0.095042</td>\n",
       "      <td>0.096073</td>\n",
       "      <td>0.856657</td>\n",
       "      <td>00:23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>123</td>\n",
       "      <td>0.016067</td>\n",
       "      <td>0.074196</td>\n",
       "      <td>0.100390</td>\n",
       "      <td>0.093074</td>\n",
       "      <td>0.855920</td>\n",
       "      <td>00:23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>124</td>\n",
       "      <td>0.015881</td>\n",
       "      <td>0.079409</td>\n",
       "      <td>0.097890</td>\n",
       "      <td>0.097061</td>\n",
       "      <td>0.855269</td>\n",
       "      <td>00:23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>125</td>\n",
       "      <td>0.015812</td>\n",
       "      <td>0.077063</td>\n",
       "      <td>0.098546</td>\n",
       "      <td>0.096700</td>\n",
       "      <td>0.853784</td>\n",
       "      <td>00:23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>126</td>\n",
       "      <td>0.015622</td>\n",
       "      <td>0.074394</td>\n",
       "      <td>0.096925</td>\n",
       "      <td>0.091710</td>\n",
       "      <td>0.857167</td>\n",
       "      <td>00:23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>127</td>\n",
       "      <td>0.015471</td>\n",
       "      <td>0.071919</td>\n",
       "      <td>0.098896</td>\n",
       "      <td>0.093877</td>\n",
       "      <td>0.852973</td>\n",
       "      <td>00:23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>128</td>\n",
       "      <td>0.015434</td>\n",
       "      <td>0.075140</td>\n",
       "      <td>0.096485</td>\n",
       "      <td>0.091292</td>\n",
       "      <td>0.858280</td>\n",
       "      <td>00:23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>129</td>\n",
       "      <td>0.015255</td>\n",
       "      <td>0.083291</td>\n",
       "      <td>0.093991</td>\n",
       "      <td>0.094697</td>\n",
       "      <td>0.859767</td>\n",
       "      <td>00:23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>130</td>\n",
       "      <td>0.015090</td>\n",
       "      <td>0.073228</td>\n",
       "      <td>0.100925</td>\n",
       "      <td>0.095888</td>\n",
       "      <td>0.852452</td>\n",
       "      <td>00:23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>131</td>\n",
       "      <td>0.015207</td>\n",
       "      <td>0.070718</td>\n",
       "      <td>0.103303</td>\n",
       "      <td>0.086298</td>\n",
       "      <td>0.858922</td>\n",
       "      <td>00:23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>132</td>\n",
       "      <td>0.015172</td>\n",
       "      <td>0.069351</td>\n",
       "      <td>0.103405</td>\n",
       "      <td>0.088065</td>\n",
       "      <td>0.858206</td>\n",
       "      <td>00:23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>133</td>\n",
       "      <td>0.015540</td>\n",
       "      <td>0.073809</td>\n",
       "      <td>0.102150</td>\n",
       "      <td>0.093484</td>\n",
       "      <td>0.856970</td>\n",
       "      <td>00:23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>134</td>\n",
       "      <td>0.015541</td>\n",
       "      <td>0.074253</td>\n",
       "      <td>0.097514</td>\n",
       "      <td>0.091743</td>\n",
       "      <td>0.856601</td>\n",
       "      <td>00:23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>135</td>\n",
       "      <td>0.015435</td>\n",
       "      <td>0.080829</td>\n",
       "      <td>0.095213</td>\n",
       "      <td>0.093178</td>\n",
       "      <td>0.859210</td>\n",
       "      <td>00:23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>136</td>\n",
       "      <td>0.015679</td>\n",
       "      <td>0.071818</td>\n",
       "      <td>0.103567</td>\n",
       "      <td>0.092754</td>\n",
       "      <td>0.855240</td>\n",
       "      <td>00:23</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>\n",
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "                background: #F44336;\n",
       "            }\n",
       "        </style>\n",
       "      <progress value='3' class='' max='3', style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      100.00% [3/3 00:02<00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Better model found at epoch 0 with valid_loss value: 1.0762139558792114.\n",
      "Better model found at epoch 1 with valid_loss value: 0.8218368291854858.\n",
      "Better model found at epoch 2 with valid_loss value: 0.6719263792037964.\n",
      "Better model found at epoch 3 with valid_loss value: 0.4137875437736511.\n",
      "Better model found at epoch 4 with valid_loss value: 0.14868196845054626.\n",
      "Better model found at epoch 5 with valid_loss value: 0.09471604973077774.\n",
      "Better model found at epoch 7 with valid_loss value: 0.07564861327409744.\n",
      "Better model found at epoch 12 with valid_loss value: 0.06260214000940323.\n",
      "Better model found at epoch 16 with valid_loss value: 0.06159978359937668.\n",
      "Better model found at epoch 23 with valid_loss value: 0.05364485830068588.\n",
      "Better model found at epoch 29 with valid_loss value: 0.05122267082333565.\n",
      "Better model found at epoch 30 with valid_loss value: 0.0489858016371727.\n",
      "Better model found at epoch 34 with valid_loss value: 0.0456542894244194.\n",
      "on end of epoch#40: start annealing from 0.001 to 0.0001\n",
      "on end of epoch#95: start annealing from 0.0001 to 1e-05\n",
      "on end of epoch#135: start annealing from 1e-05 to 1.0000000000000002e-06\n"
     ]
    }
   ],
   "source": [
    "lrs = [1e-3]\n",
    "checkpoints = [None]\n",
    "opts = [optf_adam]\n",
    "\n",
    "nb_train_script_logger.multi_train(get_learn=getlearn, \n",
    "            epoch_len=1e9, epochs=500,\n",
    "            opts=opts, lrs=lrs, checkpoints=checkpoints,\n",
    "            tb_log_root='./tb_log/',\n",
    "            autoSave=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "backup_train_logs(tag)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### balance_ratio=10比1的时候差了很多(虽然现在是一个allres一个vanlia)，mask_iou只有0.855"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 200数据集，无transform，resnet18，vanila,balance_bce,adam,balance_ratio=0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "tag = '__'.join(['resnet18', 'vanila', 'balance_bce', 'dataset200','adam', 'balance_ratio_0.1'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "getlearn = partial(get_learn, data = data_200, model_name = 'resnet18'\n",
    "                , loss_func_name = 'balance_bce', allres = False, balance_ratio = 0.1, tag = tag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Learner(data=ImageDataBunch;\n",
       "\n",
       "Train: LabelList (160 items)\n",
       "x: SegmentationItemList\n",
       "Image (3, 512, 512),Image (3, 512, 512),Image (3, 512, 512),Image (3, 512, 512),Image (3, 512, 512)\n",
       "y: SegmentationLabelList\n",
       "ImageSegment (1, 512, 512),ImageSegment (1, 512, 512),ImageSegment (1, 512, 512),ImageSegment (1, 512, 512),ImageSegment (1, 512, 512)\n",
       "Path: data/dataset_20200715/image;\n",
       "\n",
       "Valid: LabelList (40 items)\n",
       "x: SegmentationItemList\n",
       "Image (3, 512, 512),Image (3, 512, 512),Image (3, 512, 512),Image (3, 512, 512),Image (3, 512, 512)\n",
       "y: SegmentationLabelList\n",
       "ImageSegment (1, 512, 512),ImageSegment (1, 512, 512),ImageSegment (1, 512, 512),ImageSegment (1, 512, 512),ImageSegment (1, 512, 512)\n",
       "Path: data/dataset_20200715/image;\n",
       "\n",
       "Test: None, model=DataParallel(\n",
       "  (module): Resnet_UNet(\n",
       "    (down_blocks): ModuleList(\n",
       "      (0): Sequential(\n",
       "        (0): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "        (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU(inplace=True)\n",
       "      )\n",
       "      (1): Sequential(\n",
       "        (0): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "        (1): Sequential(\n",
       "          (0): BasicBlock(\n",
       "            (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "            (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (relu): ReLU(inplace=True)\n",
       "            (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "            (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "          (1): BasicBlock(\n",
       "            (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "            (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (relu): ReLU(inplace=True)\n",
       "            (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "            (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (2): Sequential(\n",
       "        (0): BasicBlock(\n",
       "          (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (downsample): Sequential(\n",
       "            (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "            (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (1): BasicBlock(\n",
       "          (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (3): Sequential(\n",
       "        (0): BasicBlock(\n",
       "          (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (downsample): Sequential(\n",
       "            (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "            (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (1): BasicBlock(\n",
       "          (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (4): Sequential(\n",
       "        (0): BasicBlock(\n",
       "          (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (downsample): Sequential(\n",
       "            (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "            (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (1): BasicBlock(\n",
       "          (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (bridge): Bridge(\n",
       "      (conv1): Conv_Bn_ReLu(\n",
       "        (conv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (bn): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (conv2): Conv_Bn_ReLu(\n",
       "        (conv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (bn): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (up_blocks): ModuleList(\n",
       "      (0): UpBlock(\n",
       "        (upsample): Sequential(\n",
       "          (0): ConvTranspose2d(512, 256, kernel_size=(2, 2), stride=(2, 2), bias=False)\n",
       "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (conv1): Conv_Bn_ReLu(\n",
       "          (conv): Conv2d(512, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (conv2): Conv_Bn_ReLu(\n",
       "          (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): UpBlock(\n",
       "        (upsample): Sequential(\n",
       "          (0): ConvTranspose2d(256, 128, kernel_size=(2, 2), stride=(2, 2), bias=False)\n",
       "          (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (conv1): Conv_Bn_ReLu(\n",
       "          (conv): Conv2d(256, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (conv2): Conv_Bn_ReLu(\n",
       "          (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (2): UpBlock(\n",
       "        (upsample): Sequential(\n",
       "          (0): ConvTranspose2d(128, 128, kernel_size=(2, 2), stride=(2, 2), bias=False)\n",
       "          (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (conv1): Conv_Bn_ReLu(\n",
       "          (conv): Conv2d(192, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (conv2): Conv_Bn_ReLu(\n",
       "          (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (3): UpBlock(\n",
       "        (upsample): Sequential(\n",
       "          (0): ConvTranspose2d(128, 128, kernel_size=(2, 2), stride=(2, 2), bias=False)\n",
       "          (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (conv1): Conv_Bn_ReLu(\n",
       "          (conv): Conv2d(192, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (conv2): Conv_Bn_ReLu(\n",
       "          (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (4): UpBlock(\n",
       "        (upsample): Sequential(\n",
       "          (0): ConvTranspose2d(128, 128, kernel_size=(2, 2), stride=(2, 2), bias=False)\n",
       "          (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (conv1): Conv_Bn_ReLu(\n",
       "          (conv): Conv2d(131, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (conv2): Conv_Bn_ReLu(\n",
       "          (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (head): Conv2d(128, 1, kernel_size=(1, 1), stride=(1, 1))\n",
       "  )\n",
       "), opt_func=functools.partial(<class 'torch.optim.adam.Adam'>, betas=(0.9, 0.99)), loss_func=functools.partial(<function balance_bce at 0x7f90e485f840>, balance_ratio=0.1), metrics=[<function dice_loss at 0x7f90e485f730>, functools.partial(<function balance_bce at 0x7f90e485f840>, balance_ratio=1), <function mask_iou at 0x7f90e485f950>], true_wd=True, bn_wd=True, wd=0.01, train_bn=True, path=PosixPath('data/dataset_20200715/image'), model_dir='models', callback_fns=[functools.partial(<class 'fastai.basic_train.Recorder'>, add_time=True, silent=False)], callbacks=[], layer_groups=[ModuleList(\n",
       "  (0): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "  (1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "  (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "  (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "  (4): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "  (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "  (6): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "  (7): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "  (8): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "  (9): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "  (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "  (11): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "  (12): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "  (13): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "  (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "  (15): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "  (16): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "  (17): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "  (18): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "  (19): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "), ModuleList(\n",
       "  (0): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (5): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (6): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (7): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (8): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (9): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (10): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (11): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (12): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (13): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (14): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (15): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (16): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (17): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (18): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (19): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (20): Bridge(\n",
       "    (conv1): Conv_Bn_ReLu(\n",
       "      (conv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (bn): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (conv2): Conv_Bn_ReLu(\n",
       "      (conv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (bn): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (21): ModuleList(\n",
       "    (0): UpBlock(\n",
       "      (upsample): Sequential(\n",
       "        (0): ConvTranspose2d(512, 256, kernel_size=(2, 2), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (conv1): Conv_Bn_ReLu(\n",
       "        (conv): Conv2d(512, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (conv2): Conv_Bn_ReLu(\n",
       "        (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): UpBlock(\n",
       "      (upsample): Sequential(\n",
       "        (0): ConvTranspose2d(256, 128, kernel_size=(2, 2), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (conv1): Conv_Bn_ReLu(\n",
       "        (conv): Conv2d(256, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (conv2): Conv_Bn_ReLu(\n",
       "        (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (2): UpBlock(\n",
       "      (upsample): Sequential(\n",
       "        (0): ConvTranspose2d(128, 128, kernel_size=(2, 2), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (conv1): Conv_Bn_ReLu(\n",
       "        (conv): Conv2d(192, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (conv2): Conv_Bn_ReLu(\n",
       "        (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (3): UpBlock(\n",
       "      (upsample): Sequential(\n",
       "        (0): ConvTranspose2d(128, 128, kernel_size=(2, 2), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (conv1): Conv_Bn_ReLu(\n",
       "        (conv): Conv2d(192, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (conv2): Conv_Bn_ReLu(\n",
       "        (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (4): UpBlock(\n",
       "      (upsample): Sequential(\n",
       "        (0): ConvTranspose2d(128, 128, kernel_size=(2, 2), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (conv1): Conv_Bn_ReLu(\n",
       "        (conv): Conv2d(131, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (conv2): Conv_Bn_ReLu(\n",
       "        (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (22): Conv2d(128, 1, kernel_size=(1, 1), stride=(1, 1))\n",
       ")], add_time=True, silent=False)"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "getlearn()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ALLERT: You are using CumtomEpochLength, please make sure that your training dataloader is using random sampler, or this may cause problem.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "                background: #F44336;\n",
       "            }\n",
       "        </style>\n",
       "      <progress value='153' class='' max='500', style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      30.60% [153/500 1:00:23<2:16:59]\n",
       "    </div>\n",
       "    \n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>dice_loss</th>\n",
       "      <th>balance_bce</th>\n",
       "      <th>mask_iou</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.365479</td>\n",
       "      <td>0.391350</td>\n",
       "      <td>0.811735</td>\n",
       "      <td>0.830813</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>00:24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.252936</td>\n",
       "      <td>0.426894</td>\n",
       "      <td>0.802151</td>\n",
       "      <td>0.787218</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>00:23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.195103</td>\n",
       "      <td>0.494709</td>\n",
       "      <td>0.787298</td>\n",
       "      <td>0.730750</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>00:23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.161218</td>\n",
       "      <td>0.363366</td>\n",
       "      <td>0.735809</td>\n",
       "      <td>0.643231</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>00:23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.141486</td>\n",
       "      <td>0.170961</td>\n",
       "      <td>0.577545</td>\n",
       "      <td>0.574355</td>\n",
       "      <td>0.243317</td>\n",
       "      <td>00:23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.119871</td>\n",
       "      <td>0.098694</td>\n",
       "      <td>0.344217</td>\n",
       "      <td>0.466001</td>\n",
       "      <td>0.594101</td>\n",
       "      <td>00:23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.106421</td>\n",
       "      <td>0.053720</td>\n",
       "      <td>0.154652</td>\n",
       "      <td>0.200183</td>\n",
       "      <td>0.816914</td>\n",
       "      <td>00:23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.095234</td>\n",
       "      <td>0.060293</td>\n",
       "      <td>0.189680</td>\n",
       "      <td>0.239017</td>\n",
       "      <td>0.796879</td>\n",
       "      <td>00:23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.087838</td>\n",
       "      <td>0.109010</td>\n",
       "      <td>0.325130</td>\n",
       "      <td>0.514044</td>\n",
       "      <td>0.578363</td>\n",
       "      <td>00:23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.089138</td>\n",
       "      <td>0.559830</td>\n",
       "      <td>0.367759</td>\n",
       "      <td>0.323072</td>\n",
       "      <td>0.489625</td>\n",
       "      <td>00:23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.088000</td>\n",
       "      <td>0.122552</td>\n",
       "      <td>0.168645</td>\n",
       "      <td>0.169888</td>\n",
       "      <td>0.765461</td>\n",
       "      <td>00:23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.084007</td>\n",
       "      <td>0.103436</td>\n",
       "      <td>0.228817</td>\n",
       "      <td>0.429614</td>\n",
       "      <td>0.687672</td>\n",
       "      <td>00:23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.079240</td>\n",
       "      <td>0.073306</td>\n",
       "      <td>0.167560</td>\n",
       "      <td>0.155055</td>\n",
       "      <td>0.810420</td>\n",
       "      <td>00:23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.073185</td>\n",
       "      <td>0.069565</td>\n",
       "      <td>0.217580</td>\n",
       "      <td>0.341924</td>\n",
       "      <td>0.723916</td>\n",
       "      <td>00:23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.067596</td>\n",
       "      <td>0.049166</td>\n",
       "      <td>0.117098</td>\n",
       "      <td>0.173789</td>\n",
       "      <td>0.838172</td>\n",
       "      <td>00:23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.067677</td>\n",
       "      <td>0.238619</td>\n",
       "      <td>0.259176</td>\n",
       "      <td>0.185365</td>\n",
       "      <td>0.613148</td>\n",
       "      <td>00:23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.064769</td>\n",
       "      <td>0.060050</td>\n",
       "      <td>0.156953</td>\n",
       "      <td>0.240367</td>\n",
       "      <td>0.792110</td>\n",
       "      <td>00:23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.060842</td>\n",
       "      <td>0.081787</td>\n",
       "      <td>0.170086</td>\n",
       "      <td>0.244270</td>\n",
       "      <td>0.757339</td>\n",
       "      <td>00:23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.057043</td>\n",
       "      <td>0.076516</td>\n",
       "      <td>0.268970</td>\n",
       "      <td>0.356291</td>\n",
       "      <td>0.692467</td>\n",
       "      <td>00:23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.053622</td>\n",
       "      <td>0.067958</td>\n",
       "      <td>0.201289</td>\n",
       "      <td>0.346636</td>\n",
       "      <td>0.722842</td>\n",
       "      <td>00:23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.052869</td>\n",
       "      <td>0.063844</td>\n",
       "      <td>0.159069</td>\n",
       "      <td>0.271726</td>\n",
       "      <td>0.787609</td>\n",
       "      <td>00:23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>0.051834</td>\n",
       "      <td>0.054351</td>\n",
       "      <td>0.142651</td>\n",
       "      <td>0.199143</td>\n",
       "      <td>0.808589</td>\n",
       "      <td>00:23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>0.052563</td>\n",
       "      <td>0.145109</td>\n",
       "      <td>0.438270</td>\n",
       "      <td>0.755005</td>\n",
       "      <td>0.405347</td>\n",
       "      <td>00:23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>0.054173</td>\n",
       "      <td>0.082755</td>\n",
       "      <td>0.225690</td>\n",
       "      <td>0.395303</td>\n",
       "      <td>0.707332</td>\n",
       "      <td>00:23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>0.053583</td>\n",
       "      <td>0.054376</td>\n",
       "      <td>0.136240</td>\n",
       "      <td>0.199659</td>\n",
       "      <td>0.841089</td>\n",
       "      <td>00:23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>0.052765</td>\n",
       "      <td>0.066977</td>\n",
       "      <td>0.172188</td>\n",
       "      <td>0.325992</td>\n",
       "      <td>0.759779</td>\n",
       "      <td>00:23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>0.050504</td>\n",
       "      <td>0.082565</td>\n",
       "      <td>0.129749</td>\n",
       "      <td>0.196066</td>\n",
       "      <td>0.800699</td>\n",
       "      <td>00:23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>0.047562</td>\n",
       "      <td>0.054700</td>\n",
       "      <td>0.161513</td>\n",
       "      <td>0.201894</td>\n",
       "      <td>0.806943</td>\n",
       "      <td>00:23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28</td>\n",
       "      <td>0.045607</td>\n",
       "      <td>0.046038</td>\n",
       "      <td>0.142586</td>\n",
       "      <td>0.175454</td>\n",
       "      <td>0.851505</td>\n",
       "      <td>00:23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29</td>\n",
       "      <td>0.043400</td>\n",
       "      <td>0.055585</td>\n",
       "      <td>0.144975</td>\n",
       "      <td>0.271914</td>\n",
       "      <td>0.786290</td>\n",
       "      <td>00:23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>0.041554</td>\n",
       "      <td>0.059977</td>\n",
       "      <td>0.111856</td>\n",
       "      <td>0.138295</td>\n",
       "      <td>0.845185</td>\n",
       "      <td>00:23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31</td>\n",
       "      <td>0.040298</td>\n",
       "      <td>0.055423</td>\n",
       "      <td>0.130410</td>\n",
       "      <td>0.255517</td>\n",
       "      <td>0.809976</td>\n",
       "      <td>00:23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32</td>\n",
       "      <td>0.039658</td>\n",
       "      <td>0.046084</td>\n",
       "      <td>0.087199</td>\n",
       "      <td>0.127797</td>\n",
       "      <td>0.869471</td>\n",
       "      <td>00:23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33</td>\n",
       "      <td>0.037661</td>\n",
       "      <td>0.035597</td>\n",
       "      <td>0.090514</td>\n",
       "      <td>0.135042</td>\n",
       "      <td>0.879297</td>\n",
       "      <td>00:23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34</td>\n",
       "      <td>0.035828</td>\n",
       "      <td>0.040620</td>\n",
       "      <td>0.098547</td>\n",
       "      <td>0.108708</td>\n",
       "      <td>0.879115</td>\n",
       "      <td>00:23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35</td>\n",
       "      <td>0.033959</td>\n",
       "      <td>0.042708</td>\n",
       "      <td>0.116827</td>\n",
       "      <td>0.185090</td>\n",
       "      <td>0.846913</td>\n",
       "      <td>00:23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36</td>\n",
       "      <td>0.032243</td>\n",
       "      <td>0.037981</td>\n",
       "      <td>0.085480</td>\n",
       "      <td>0.122355</td>\n",
       "      <td>0.884870</td>\n",
       "      <td>00:23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>37</td>\n",
       "      <td>0.030524</td>\n",
       "      <td>0.039507</td>\n",
       "      <td>0.103658</td>\n",
       "      <td>0.165366</td>\n",
       "      <td>0.861234</td>\n",
       "      <td>00:23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>38</td>\n",
       "      <td>0.029259</td>\n",
       "      <td>0.051224</td>\n",
       "      <td>0.099702</td>\n",
       "      <td>0.141880</td>\n",
       "      <td>0.852122</td>\n",
       "      <td>00:23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>39</td>\n",
       "      <td>0.028018</td>\n",
       "      <td>0.043266</td>\n",
       "      <td>0.078097</td>\n",
       "      <td>0.136221</td>\n",
       "      <td>0.876837</td>\n",
       "      <td>00:23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>0.026725</td>\n",
       "      <td>0.042242</td>\n",
       "      <td>0.090847</td>\n",
       "      <td>0.171111</td>\n",
       "      <td>0.860314</td>\n",
       "      <td>00:23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>41</td>\n",
       "      <td>0.025462</td>\n",
       "      <td>0.038703</td>\n",
       "      <td>0.080926</td>\n",
       "      <td>0.167219</td>\n",
       "      <td>0.875324</td>\n",
       "      <td>00:23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>42</td>\n",
       "      <td>0.024718</td>\n",
       "      <td>0.043530</td>\n",
       "      <td>0.076013</td>\n",
       "      <td>0.188207</td>\n",
       "      <td>0.883486</td>\n",
       "      <td>00:23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>43</td>\n",
       "      <td>0.023938</td>\n",
       "      <td>0.043923</td>\n",
       "      <td>0.072827</td>\n",
       "      <td>0.123495</td>\n",
       "      <td>0.882571</td>\n",
       "      <td>00:23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>44</td>\n",
       "      <td>0.023097</td>\n",
       "      <td>0.035173</td>\n",
       "      <td>0.073416</td>\n",
       "      <td>0.138801</td>\n",
       "      <td>0.888211</td>\n",
       "      <td>00:23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>45</td>\n",
       "      <td>0.023317</td>\n",
       "      <td>0.075214</td>\n",
       "      <td>0.148146</td>\n",
       "      <td>0.339224</td>\n",
       "      <td>0.808484</td>\n",
       "      <td>00:23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>46</td>\n",
       "      <td>0.026002</td>\n",
       "      <td>0.085242</td>\n",
       "      <td>0.134248</td>\n",
       "      <td>0.366017</td>\n",
       "      <td>0.796603</td>\n",
       "      <td>00:23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>47</td>\n",
       "      <td>0.029289</td>\n",
       "      <td>0.084972</td>\n",
       "      <td>0.153052</td>\n",
       "      <td>0.278223</td>\n",
       "      <td>0.775156</td>\n",
       "      <td>00:23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>48</td>\n",
       "      <td>0.031357</td>\n",
       "      <td>0.195494</td>\n",
       "      <td>0.228994</td>\n",
       "      <td>0.147763</td>\n",
       "      <td>0.698456</td>\n",
       "      <td>00:23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>49</td>\n",
       "      <td>0.032680</td>\n",
       "      <td>0.489806</td>\n",
       "      <td>0.820385</td>\n",
       "      <td>2.686735</td>\n",
       "      <td>0.070217</td>\n",
       "      <td>00:23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.033316</td>\n",
       "      <td>0.041439</td>\n",
       "      <td>0.104392</td>\n",
       "      <td>0.148020</td>\n",
       "      <td>0.862212</td>\n",
       "      <td>00:23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>51</td>\n",
       "      <td>0.032213</td>\n",
       "      <td>0.038436</td>\n",
       "      <td>0.093154</td>\n",
       "      <td>0.128495</td>\n",
       "      <td>0.876140</td>\n",
       "      <td>00:23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>52</td>\n",
       "      <td>0.032037</td>\n",
       "      <td>0.036807</td>\n",
       "      <td>0.094719</td>\n",
       "      <td>0.131169</td>\n",
       "      <td>0.877706</td>\n",
       "      <td>00:23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>53</td>\n",
       "      <td>0.030152</td>\n",
       "      <td>0.036744</td>\n",
       "      <td>0.095661</td>\n",
       "      <td>0.140692</td>\n",
       "      <td>0.875584</td>\n",
       "      <td>00:23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>54</td>\n",
       "      <td>0.029411</td>\n",
       "      <td>0.037625</td>\n",
       "      <td>0.093967</td>\n",
       "      <td>0.139462</td>\n",
       "      <td>0.874000</td>\n",
       "      <td>00:23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>55</td>\n",
       "      <td>0.027970</td>\n",
       "      <td>0.037449</td>\n",
       "      <td>0.092814</td>\n",
       "      <td>0.135925</td>\n",
       "      <td>0.876277</td>\n",
       "      <td>00:23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>56</td>\n",
       "      <td>0.026557</td>\n",
       "      <td>0.035459</td>\n",
       "      <td>0.084472</td>\n",
       "      <td>0.124621</td>\n",
       "      <td>0.885120</td>\n",
       "      <td>00:23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>57</td>\n",
       "      <td>0.025347</td>\n",
       "      <td>0.033821</td>\n",
       "      <td>0.081881</td>\n",
       "      <td>0.122164</td>\n",
       "      <td>0.889199</td>\n",
       "      <td>00:23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>58</td>\n",
       "      <td>0.024243</td>\n",
       "      <td>0.034455</td>\n",
       "      <td>0.081695</td>\n",
       "      <td>0.118349</td>\n",
       "      <td>0.888309</td>\n",
       "      <td>00:23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>59</td>\n",
       "      <td>0.023348</td>\n",
       "      <td>0.034954</td>\n",
       "      <td>0.080630</td>\n",
       "      <td>0.125900</td>\n",
       "      <td>0.885035</td>\n",
       "      <td>00:24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>0.022430</td>\n",
       "      <td>0.035751</td>\n",
       "      <td>0.081969</td>\n",
       "      <td>0.132373</td>\n",
       "      <td>0.881275</td>\n",
       "      <td>00:24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>61</td>\n",
       "      <td>0.021786</td>\n",
       "      <td>0.035778</td>\n",
       "      <td>0.083083</td>\n",
       "      <td>0.127898</td>\n",
       "      <td>0.881439</td>\n",
       "      <td>00:23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>62</td>\n",
       "      <td>0.021156</td>\n",
       "      <td>0.035050</td>\n",
       "      <td>0.080161</td>\n",
       "      <td>0.128819</td>\n",
       "      <td>0.883555</td>\n",
       "      <td>00:23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>63</td>\n",
       "      <td>0.020525</td>\n",
       "      <td>0.034089</td>\n",
       "      <td>0.077812</td>\n",
       "      <td>0.124131</td>\n",
       "      <td>0.887361</td>\n",
       "      <td>00:23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>64</td>\n",
       "      <td>0.019997</td>\n",
       "      <td>0.034841</td>\n",
       "      <td>0.077214</td>\n",
       "      <td>0.130057</td>\n",
       "      <td>0.884941</td>\n",
       "      <td>00:23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>65</td>\n",
       "      <td>0.019441</td>\n",
       "      <td>0.034869</td>\n",
       "      <td>0.076118</td>\n",
       "      <td>0.132985</td>\n",
       "      <td>0.885138</td>\n",
       "      <td>00:23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>66</td>\n",
       "      <td>0.020107</td>\n",
       "      <td>0.034603</td>\n",
       "      <td>0.080822</td>\n",
       "      <td>0.130187</td>\n",
       "      <td>0.884466</td>\n",
       "      <td>00:23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>67</td>\n",
       "      <td>0.019790</td>\n",
       "      <td>0.035540</td>\n",
       "      <td>0.084789</td>\n",
       "      <td>0.144461</td>\n",
       "      <td>0.879778</td>\n",
       "      <td>00:23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>68</td>\n",
       "      <td>0.019274</td>\n",
       "      <td>0.036098</td>\n",
       "      <td>0.085696</td>\n",
       "      <td>0.148847</td>\n",
       "      <td>0.877248</td>\n",
       "      <td>00:23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>69</td>\n",
       "      <td>0.018756</td>\n",
       "      <td>0.034137</td>\n",
       "      <td>0.072707</td>\n",
       "      <td>0.120816</td>\n",
       "      <td>0.893543</td>\n",
       "      <td>00:23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>0.018289</td>\n",
       "      <td>0.034355</td>\n",
       "      <td>0.069567</td>\n",
       "      <td>0.114251</td>\n",
       "      <td>0.896409</td>\n",
       "      <td>00:23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>71</td>\n",
       "      <td>0.017814</td>\n",
       "      <td>0.035936</td>\n",
       "      <td>0.072884</td>\n",
       "      <td>0.123897</td>\n",
       "      <td>0.890322</td>\n",
       "      <td>00:23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>72</td>\n",
       "      <td>0.017499</td>\n",
       "      <td>0.034769</td>\n",
       "      <td>0.072071</td>\n",
       "      <td>0.117734</td>\n",
       "      <td>0.893469</td>\n",
       "      <td>00:23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>73</td>\n",
       "      <td>0.017175</td>\n",
       "      <td>0.035288</td>\n",
       "      <td>0.075008</td>\n",
       "      <td>0.141811</td>\n",
       "      <td>0.884371</td>\n",
       "      <td>00:23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>74</td>\n",
       "      <td>0.016807</td>\n",
       "      <td>0.034169</td>\n",
       "      <td>0.071730</td>\n",
       "      <td>0.122467</td>\n",
       "      <td>0.892843</td>\n",
       "      <td>00:23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>75</td>\n",
       "      <td>0.016569</td>\n",
       "      <td>0.034232</td>\n",
       "      <td>0.071339</td>\n",
       "      <td>0.119697</td>\n",
       "      <td>0.893727</td>\n",
       "      <td>00:23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>76</td>\n",
       "      <td>0.016347</td>\n",
       "      <td>0.036145</td>\n",
       "      <td>0.074330</td>\n",
       "      <td>0.137238</td>\n",
       "      <td>0.885837</td>\n",
       "      <td>00:23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>77</td>\n",
       "      <td>0.016136</td>\n",
       "      <td>0.034683</td>\n",
       "      <td>0.070330</td>\n",
       "      <td>0.130363</td>\n",
       "      <td>0.891561</td>\n",
       "      <td>00:23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>78</td>\n",
       "      <td>0.015915</td>\n",
       "      <td>0.034103</td>\n",
       "      <td>0.069979</td>\n",
       "      <td>0.125973</td>\n",
       "      <td>0.893439</td>\n",
       "      <td>00:23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>79</td>\n",
       "      <td>0.016102</td>\n",
       "      <td>0.034934</td>\n",
       "      <td>0.072181</td>\n",
       "      <td>0.139475</td>\n",
       "      <td>0.888824</td>\n",
       "      <td>00:23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>0.015957</td>\n",
       "      <td>0.034187</td>\n",
       "      <td>0.069514</td>\n",
       "      <td>0.128566</td>\n",
       "      <td>0.895371</td>\n",
       "      <td>00:23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>81</td>\n",
       "      <td>0.015771</td>\n",
       "      <td>0.034244</td>\n",
       "      <td>0.067922</td>\n",
       "      <td>0.120411</td>\n",
       "      <td>0.896963</td>\n",
       "      <td>00:23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>82</td>\n",
       "      <td>0.015528</td>\n",
       "      <td>0.035715</td>\n",
       "      <td>0.069314</td>\n",
       "      <td>0.129583</td>\n",
       "      <td>0.892674</td>\n",
       "      <td>00:23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>83</td>\n",
       "      <td>0.015256</td>\n",
       "      <td>0.034406</td>\n",
       "      <td>0.070590</td>\n",
       "      <td>0.114664</td>\n",
       "      <td>0.896311</td>\n",
       "      <td>00:23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>84</td>\n",
       "      <td>0.015100</td>\n",
       "      <td>0.035058</td>\n",
       "      <td>0.070852</td>\n",
       "      <td>0.134012</td>\n",
       "      <td>0.890957</td>\n",
       "      <td>00:24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>85</td>\n",
       "      <td>0.014878</td>\n",
       "      <td>0.035311</td>\n",
       "      <td>0.068674</td>\n",
       "      <td>0.126624</td>\n",
       "      <td>0.894204</td>\n",
       "      <td>00:23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>86</td>\n",
       "      <td>0.014732</td>\n",
       "      <td>0.033812</td>\n",
       "      <td>0.069490</td>\n",
       "      <td>0.120220</td>\n",
       "      <td>0.896936</td>\n",
       "      <td>00:23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>87</td>\n",
       "      <td>0.014605</td>\n",
       "      <td>0.033877</td>\n",
       "      <td>0.069548</td>\n",
       "      <td>0.128717</td>\n",
       "      <td>0.894455</td>\n",
       "      <td>00:23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>88</td>\n",
       "      <td>0.014325</td>\n",
       "      <td>0.034931</td>\n",
       "      <td>0.067351</td>\n",
       "      <td>0.122818</td>\n",
       "      <td>0.895541</td>\n",
       "      <td>00:23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>89</td>\n",
       "      <td>0.014223</td>\n",
       "      <td>0.034797</td>\n",
       "      <td>0.064724</td>\n",
       "      <td>0.131603</td>\n",
       "      <td>0.897877</td>\n",
       "      <td>00:23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>0.014743</td>\n",
       "      <td>0.042832</td>\n",
       "      <td>0.080919</td>\n",
       "      <td>0.199669</td>\n",
       "      <td>0.870985</td>\n",
       "      <td>00:23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>91</td>\n",
       "      <td>0.014672</td>\n",
       "      <td>0.040850</td>\n",
       "      <td>0.079093</td>\n",
       "      <td>0.176955</td>\n",
       "      <td>0.875443</td>\n",
       "      <td>00:23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>92</td>\n",
       "      <td>0.014481</td>\n",
       "      <td>0.038397</td>\n",
       "      <td>0.074865</td>\n",
       "      <td>0.145013</td>\n",
       "      <td>0.883254</td>\n",
       "      <td>00:23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>93</td>\n",
       "      <td>0.014459</td>\n",
       "      <td>0.037606</td>\n",
       "      <td>0.072090</td>\n",
       "      <td>0.133797</td>\n",
       "      <td>0.886614</td>\n",
       "      <td>00:23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>94</td>\n",
       "      <td>0.014313</td>\n",
       "      <td>0.037875</td>\n",
       "      <td>0.072003</td>\n",
       "      <td>0.140662</td>\n",
       "      <td>0.885114</td>\n",
       "      <td>00:24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>95</td>\n",
       "      <td>0.014073</td>\n",
       "      <td>0.037579</td>\n",
       "      <td>0.066693</td>\n",
       "      <td>0.133051</td>\n",
       "      <td>0.892642</td>\n",
       "      <td>00:23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>96</td>\n",
       "      <td>0.013921</td>\n",
       "      <td>0.037823</td>\n",
       "      <td>0.066112</td>\n",
       "      <td>0.127028</td>\n",
       "      <td>0.892800</td>\n",
       "      <td>00:23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>97</td>\n",
       "      <td>0.013806</td>\n",
       "      <td>0.037523</td>\n",
       "      <td>0.068041</td>\n",
       "      <td>0.131099</td>\n",
       "      <td>0.891191</td>\n",
       "      <td>00:23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>98</td>\n",
       "      <td>0.013668</td>\n",
       "      <td>0.037207</td>\n",
       "      <td>0.068615</td>\n",
       "      <td>0.129152</td>\n",
       "      <td>0.891333</td>\n",
       "      <td>00:23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>99</td>\n",
       "      <td>0.013512</td>\n",
       "      <td>0.038681</td>\n",
       "      <td>0.073507</td>\n",
       "      <td>0.128740</td>\n",
       "      <td>0.884720</td>\n",
       "      <td>00:23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.013399</td>\n",
       "      <td>0.038788</td>\n",
       "      <td>0.070930</td>\n",
       "      <td>0.126515</td>\n",
       "      <td>0.887900</td>\n",
       "      <td>00:23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>101</td>\n",
       "      <td>0.013303</td>\n",
       "      <td>0.039864</td>\n",
       "      <td>0.072665</td>\n",
       "      <td>0.137615</td>\n",
       "      <td>0.884303</td>\n",
       "      <td>00:23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>102</td>\n",
       "      <td>0.013393</td>\n",
       "      <td>0.037174</td>\n",
       "      <td>0.065408</td>\n",
       "      <td>0.124686</td>\n",
       "      <td>0.893500</td>\n",
       "      <td>00:23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>103</td>\n",
       "      <td>0.013249</td>\n",
       "      <td>0.037187</td>\n",
       "      <td>0.067116</td>\n",
       "      <td>0.120805</td>\n",
       "      <td>0.893552</td>\n",
       "      <td>00:23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>104</td>\n",
       "      <td>0.013241</td>\n",
       "      <td>0.036520</td>\n",
       "      <td>0.070663</td>\n",
       "      <td>0.129813</td>\n",
       "      <td>0.888740</td>\n",
       "      <td>00:23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>105</td>\n",
       "      <td>0.013210</td>\n",
       "      <td>0.037246</td>\n",
       "      <td>0.068439</td>\n",
       "      <td>0.128401</td>\n",
       "      <td>0.891078</td>\n",
       "      <td>00:23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>106</td>\n",
       "      <td>0.013146</td>\n",
       "      <td>0.037256</td>\n",
       "      <td>0.067925</td>\n",
       "      <td>0.122856</td>\n",
       "      <td>0.892344</td>\n",
       "      <td>00:23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>107</td>\n",
       "      <td>0.013085</td>\n",
       "      <td>0.037497</td>\n",
       "      <td>0.066665</td>\n",
       "      <td>0.134018</td>\n",
       "      <td>0.891187</td>\n",
       "      <td>00:23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>108</td>\n",
       "      <td>0.013000</td>\n",
       "      <td>0.037335</td>\n",
       "      <td>0.065822</td>\n",
       "      <td>0.130561</td>\n",
       "      <td>0.892715</td>\n",
       "      <td>00:23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>109</td>\n",
       "      <td>0.012933</td>\n",
       "      <td>0.037719</td>\n",
       "      <td>0.066955</td>\n",
       "      <td>0.132127</td>\n",
       "      <td>0.890979</td>\n",
       "      <td>00:23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>110</td>\n",
       "      <td>0.012887</td>\n",
       "      <td>0.037853</td>\n",
       "      <td>0.067209</td>\n",
       "      <td>0.140685</td>\n",
       "      <td>0.890173</td>\n",
       "      <td>00:23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>111</td>\n",
       "      <td>0.012839</td>\n",
       "      <td>0.038050</td>\n",
       "      <td>0.065906</td>\n",
       "      <td>0.139322</td>\n",
       "      <td>0.891669</td>\n",
       "      <td>00:23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>112</td>\n",
       "      <td>0.012904</td>\n",
       "      <td>0.038328</td>\n",
       "      <td>0.065184</td>\n",
       "      <td>0.125917</td>\n",
       "      <td>0.893195</td>\n",
       "      <td>00:23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>113</td>\n",
       "      <td>0.012863</td>\n",
       "      <td>0.037479</td>\n",
       "      <td>0.067729</td>\n",
       "      <td>0.127918</td>\n",
       "      <td>0.891055</td>\n",
       "      <td>00:23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>114</td>\n",
       "      <td>0.012895</td>\n",
       "      <td>0.037078</td>\n",
       "      <td>0.068125</td>\n",
       "      <td>0.130995</td>\n",
       "      <td>0.889928</td>\n",
       "      <td>00:23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>115</td>\n",
       "      <td>0.012959</td>\n",
       "      <td>0.038150</td>\n",
       "      <td>0.070056</td>\n",
       "      <td>0.140609</td>\n",
       "      <td>0.886919</td>\n",
       "      <td>00:23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>116</td>\n",
       "      <td>0.012870</td>\n",
       "      <td>0.037493</td>\n",
       "      <td>0.069274</td>\n",
       "      <td>0.128607</td>\n",
       "      <td>0.889727</td>\n",
       "      <td>00:23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>117</td>\n",
       "      <td>0.012770</td>\n",
       "      <td>0.037041</td>\n",
       "      <td>0.068886</td>\n",
       "      <td>0.123702</td>\n",
       "      <td>0.891551</td>\n",
       "      <td>00:23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>118</td>\n",
       "      <td>0.012704</td>\n",
       "      <td>0.037498</td>\n",
       "      <td>0.069996</td>\n",
       "      <td>0.133575</td>\n",
       "      <td>0.888937</td>\n",
       "      <td>00:23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>119</td>\n",
       "      <td>0.012751</td>\n",
       "      <td>0.036177</td>\n",
       "      <td>0.069294</td>\n",
       "      <td>0.135458</td>\n",
       "      <td>0.888937</td>\n",
       "      <td>00:23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120</td>\n",
       "      <td>0.012694</td>\n",
       "      <td>0.038476</td>\n",
       "      <td>0.071136</td>\n",
       "      <td>0.149819</td>\n",
       "      <td>0.884699</td>\n",
       "      <td>00:23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>121</td>\n",
       "      <td>0.012664</td>\n",
       "      <td>0.037628</td>\n",
       "      <td>0.067921</td>\n",
       "      <td>0.133419</td>\n",
       "      <td>0.889558</td>\n",
       "      <td>00:23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>122</td>\n",
       "      <td>0.012632</td>\n",
       "      <td>0.039224</td>\n",
       "      <td>0.070800</td>\n",
       "      <td>0.144187</td>\n",
       "      <td>0.884921</td>\n",
       "      <td>00:23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>123</td>\n",
       "      <td>0.012591</td>\n",
       "      <td>0.039290</td>\n",
       "      <td>0.067338</td>\n",
       "      <td>0.133910</td>\n",
       "      <td>0.889875</td>\n",
       "      <td>00:23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>124</td>\n",
       "      <td>0.012550</td>\n",
       "      <td>0.037897</td>\n",
       "      <td>0.069659</td>\n",
       "      <td>0.138331</td>\n",
       "      <td>0.887761</td>\n",
       "      <td>00:23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>125</td>\n",
       "      <td>0.012571</td>\n",
       "      <td>0.036925</td>\n",
       "      <td>0.069918</td>\n",
       "      <td>0.134658</td>\n",
       "      <td>0.888945</td>\n",
       "      <td>00:23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>126</td>\n",
       "      <td>0.012583</td>\n",
       "      <td>0.037726</td>\n",
       "      <td>0.069311</td>\n",
       "      <td>0.132693</td>\n",
       "      <td>0.888814</td>\n",
       "      <td>00:23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>127</td>\n",
       "      <td>0.012565</td>\n",
       "      <td>0.038028</td>\n",
       "      <td>0.070777</td>\n",
       "      <td>0.123046</td>\n",
       "      <td>0.888850</td>\n",
       "      <td>00:23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>128</td>\n",
       "      <td>0.012570</td>\n",
       "      <td>0.035977</td>\n",
       "      <td>0.065866</td>\n",
       "      <td>0.130489</td>\n",
       "      <td>0.893913</td>\n",
       "      <td>00:23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>129</td>\n",
       "      <td>0.012541</td>\n",
       "      <td>0.037934</td>\n",
       "      <td>0.068728</td>\n",
       "      <td>0.129201</td>\n",
       "      <td>0.889970</td>\n",
       "      <td>00:23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>130</td>\n",
       "      <td>0.012527</td>\n",
       "      <td>0.037039</td>\n",
       "      <td>0.066037</td>\n",
       "      <td>0.128873</td>\n",
       "      <td>0.893424</td>\n",
       "      <td>00:23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>131</td>\n",
       "      <td>0.012523</td>\n",
       "      <td>0.036811</td>\n",
       "      <td>0.067602</td>\n",
       "      <td>0.129557</td>\n",
       "      <td>0.891784</td>\n",
       "      <td>00:23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>132</td>\n",
       "      <td>0.012536</td>\n",
       "      <td>0.038289</td>\n",
       "      <td>0.068922</td>\n",
       "      <td>0.129618</td>\n",
       "      <td>0.890112</td>\n",
       "      <td>00:23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>133</td>\n",
       "      <td>0.012692</td>\n",
       "      <td>0.037500</td>\n",
       "      <td>0.069053</td>\n",
       "      <td>0.130727</td>\n",
       "      <td>0.890086</td>\n",
       "      <td>00:23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>134</td>\n",
       "      <td>0.012587</td>\n",
       "      <td>0.036890</td>\n",
       "      <td>0.066698</td>\n",
       "      <td>0.131087</td>\n",
       "      <td>0.892937</td>\n",
       "      <td>00:23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>135</td>\n",
       "      <td>0.012513</td>\n",
       "      <td>0.037647</td>\n",
       "      <td>0.066619</td>\n",
       "      <td>0.126842</td>\n",
       "      <td>0.891893</td>\n",
       "      <td>00:23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>136</td>\n",
       "      <td>0.012470</td>\n",
       "      <td>0.036916</td>\n",
       "      <td>0.065755</td>\n",
       "      <td>0.121062</td>\n",
       "      <td>0.894869</td>\n",
       "      <td>00:23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>137</td>\n",
       "      <td>0.012504</td>\n",
       "      <td>0.037653</td>\n",
       "      <td>0.067274</td>\n",
       "      <td>0.129020</td>\n",
       "      <td>0.891712</td>\n",
       "      <td>00:23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>138</td>\n",
       "      <td>0.012455</td>\n",
       "      <td>0.037451</td>\n",
       "      <td>0.068158</td>\n",
       "      <td>0.133271</td>\n",
       "      <td>0.889695</td>\n",
       "      <td>00:23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>139</td>\n",
       "      <td>0.012402</td>\n",
       "      <td>0.038752</td>\n",
       "      <td>0.066726</td>\n",
       "      <td>0.130598</td>\n",
       "      <td>0.891046</td>\n",
       "      <td>00:23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>140</td>\n",
       "      <td>0.012356</td>\n",
       "      <td>0.037366</td>\n",
       "      <td>0.066421</td>\n",
       "      <td>0.131625</td>\n",
       "      <td>0.891779</td>\n",
       "      <td>00:23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>141</td>\n",
       "      <td>0.012318</td>\n",
       "      <td>0.037324</td>\n",
       "      <td>0.066393</td>\n",
       "      <td>0.129466</td>\n",
       "      <td>0.891668</td>\n",
       "      <td>00:23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>142</td>\n",
       "      <td>0.012293</td>\n",
       "      <td>0.037283</td>\n",
       "      <td>0.067656</td>\n",
       "      <td>0.139923</td>\n",
       "      <td>0.889809</td>\n",
       "      <td>00:23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>143</td>\n",
       "      <td>0.012362</td>\n",
       "      <td>0.036822</td>\n",
       "      <td>0.066007</td>\n",
       "      <td>0.132437</td>\n",
       "      <td>0.892386</td>\n",
       "      <td>00:23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>144</td>\n",
       "      <td>0.012322</td>\n",
       "      <td>0.037372</td>\n",
       "      <td>0.063612</td>\n",
       "      <td>0.128454</td>\n",
       "      <td>0.895384</td>\n",
       "      <td>00:23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>145</td>\n",
       "      <td>0.012296</td>\n",
       "      <td>0.037750</td>\n",
       "      <td>0.063091</td>\n",
       "      <td>0.123759</td>\n",
       "      <td>0.896901</td>\n",
       "      <td>00:23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>146</td>\n",
       "      <td>0.012240</td>\n",
       "      <td>0.037329</td>\n",
       "      <td>0.063644</td>\n",
       "      <td>0.128595</td>\n",
       "      <td>0.895498</td>\n",
       "      <td>00:23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>147</td>\n",
       "      <td>0.012349</td>\n",
       "      <td>0.038064</td>\n",
       "      <td>0.063737</td>\n",
       "      <td>0.124377</td>\n",
       "      <td>0.894973</td>\n",
       "      <td>00:23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>148</td>\n",
       "      <td>0.012343</td>\n",
       "      <td>0.037885</td>\n",
       "      <td>0.066262</td>\n",
       "      <td>0.130410</td>\n",
       "      <td>0.892483</td>\n",
       "      <td>00:23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>149</td>\n",
       "      <td>0.012320</td>\n",
       "      <td>0.038120</td>\n",
       "      <td>0.067639</td>\n",
       "      <td>0.138144</td>\n",
       "      <td>0.889486</td>\n",
       "      <td>00:23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>0.012341</td>\n",
       "      <td>0.038194</td>\n",
       "      <td>0.066264</td>\n",
       "      <td>0.131719</td>\n",
       "      <td>0.891207</td>\n",
       "      <td>00:23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>151</td>\n",
       "      <td>0.012278</td>\n",
       "      <td>0.037699</td>\n",
       "      <td>0.069054</td>\n",
       "      <td>0.133211</td>\n",
       "      <td>0.889830</td>\n",
       "      <td>00:23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>152</td>\n",
       "      <td>0.012246</td>\n",
       "      <td>0.038255</td>\n",
       "      <td>0.067085</td>\n",
       "      <td>0.130736</td>\n",
       "      <td>0.891839</td>\n",
       "      <td>00:23</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>\n",
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "                background: #F44336;\n",
       "            }\n",
       "        </style>\n",
       "      <progress value='3' class='' max='3', style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      100.00% [3/3 00:02<00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Better model found at epoch 0 with valid_loss value: 0.3913504183292389.\n",
      "Better model found at epoch 3 with valid_loss value: 0.36336639523506165.\n",
      "Better model found at epoch 4 with valid_loss value: 0.1709609478712082.\n",
      "Better model found at epoch 5 with valid_loss value: 0.09869351983070374.\n",
      "Better model found at epoch 6 with valid_loss value: 0.05372047424316406.\n",
      "Better model found at epoch 14 with valid_loss value: 0.049166422337293625.\n",
      "Better model found at epoch 28 with valid_loss value: 0.046037591993808746.\n",
      "Better model found at epoch 33 with valid_loss value: 0.03559652715921402.\n",
      "Better model found at epoch 44 with valid_loss value: 0.03517285734415054.\n",
      "on end of epoch#49: start annealing from 0.001 to 0.0001\n",
      "Better model found at epoch 57 with valid_loss value: 0.03382079303264618.\n",
      "Better model found at epoch 86 with valid_loss value: 0.03381175547838211.\n",
      "on end of epoch#94: start annealing from 0.0001 to 1e-05\n",
      "on end of epoch#151: start annealing from 1e-05 to 1.0000000000000002e-06\n"
     ]
    }
   ],
   "source": [
    "lrs = [1e-3]\n",
    "checkpoints = [None]\n",
    "opts = [optf_adam]\n",
    "\n",
    "nb_train_script_logger.multi_train(get_learn=getlearn, \n",
    "            epoch_len=1e9, epochs=500,\n",
    "            opts=opts, lrs=lrs, checkpoints=checkpoints,\n",
    "            tb_log_root='./tb_log/',\n",
    "            autoSave=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "backup_train_logs(tag)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### banlance_bce总体低于dice，balance_ratio=1的时候最好"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted train_eval.ipynb to exp/nb_train_eval.py\r\n"
     ]
    }
   ],
   "source": [
    "!python notebook2script.py --fname 'train_eval.ipynb' --outputDir './exp/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
