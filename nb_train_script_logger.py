
#################################################
### THIS FILE WAS AUTOGENERATED! DO NOT EDIT! ###
#################################################
# file to edit: dev_nb/train_script_logger.ipynb

#================================================
from fastai.basic_train import LearnerCallback


#================================================
from fastai.callbacks.general_sched import *
from fastai.callback import *


#================================================
from fastai.core import *


#================================================
from IPython.core import debugger as idb


#================================================
from exp import nb_resnet_unet
from exp import nb_loss_metrics
from exp import nb_optimizer
from exp import nb_tensorboard_callback
from exp import nb_scheduling_train
from exp import nb_databunch


#================================================
import torch


#================================================
from torch import tensor


#================================================
from torch import nn


#================================================
from torch.nn import Sequential, ModuleList


#================================================
from fastai.basic_train import Learner


#================================================
from fastai.torch_core import bn_types,bias_types


#================================================
from functools import partial


#================================================
import os,shutil


#================================================
from fastai.callbacks import CSVLogger


#================================================
from fastai.callbacks.tracker import SaveModelCallback


#================================================
def txt_write(fh, i, opt, lr, path, csv_fname):
    fh.write('===================================\n')
    fh.write(f'run_{i}\n')
    fh.write('-----------------------------------\n')
    fh.write(f'--opt_func: {opt}\n')
    fh.write(f'--lr: {lr}\n')
    fh.write(f'--csv_log: {path}/{csv_fname}.csv\n')
    fh.write(f'--best model: {path}/models/run_{i}.pth\n')


#================================================
def multi_train(get_learn, epoch_len, epochs, opts, lrs, checkpoints, tb_log_root,autoSave=True):
    '''
    可以从checkpoint继续训练，为了保证训练连续性，需要手动设置lr与checkpoint保存时一致。
    '''
    # 清理tensorboard log dir
    if os.path.exists(tb_log_root): shutil.rmtree(tb_log_root)
    os.mkdir(tb_log_root)

    if not os.path.exists('./run_log/'): os.mkdir('./run_log/')
    txtlog = open('./run_log/log.txt',mode='w')
    for i,(opt,lr,checkpoint) in enumerate(zip(opts,lrs,checkpoints)):
        # create a learner
        learn = get_learn()

        # set optimizer
        learn.opt_func = opt

        # load checkpoint
        if checkpoint is not None:
            with open(checkpoint,'rb') as f:
                learn.load(f)

        # 在txt log中记录
        csv_log_dir = f'csv_log/'
        if not os.path.exists(learn.path/csv_log_dir): os.mkdir(learn.path/csv_log_dir)
        csv_fname = csv_log_dir+f'run_{i}'
        txt_write(txtlog,i,opt,lr,learn.path,csv_fname)

        callbacks = []
        # get csvlogger callback
        csvLog = CSVLogger(learn,filename=csv_fname)
        callbacks += [csvLog]

        if autoSave:
            # savemodel callback
            autoSave = SaveModelCallback(learn,monitor='valid_loss',mode='min',every='improvement',name=f'run_{i}')
            callbacks += [autoSave]

        # get tensorboard callback
        tbCb = get_tbCb(learn,tb_log_root+f'run_{i}')
        callbacks += [tbCb]

        # train
        fit(learn=learn, epoch_len=epoch_len, epochs=epochs, lr=lr, callbacks=callbacks)

    txtlog.close()


#================================================
def split_model(model):
    group0 = ModuleList()
    group1 = ModuleList()

    pretrained_layers = model.down_blocks
    noPretrain_layers = Sequential(model.bridge, model.side_layers, model.up_blocks, model.head)

    #把pretrained layers分作batchnorm部分（放在group1），和非batchnorm部分（放在group0）
    for m in pretrained_layers.modules():
        if isinstance(m,bn_types): group1.append(m)
        elif isinstance(m,bias_types): group0.append(m)

    #把非pretrain的层放到group1
    for m in noPretrain_layers.children():
        group1.append(m)

    return [group0, group1]


#================================================
def get_learn(data):
    # create model
    model = nb_resnet_unet.get_unet_res18(1,True)
    model.load_state_dict(torch.load('./models/unet_res18_allres_init.pth'));

    # create learner
    learn = Learner(data,model)

    # split model
    learn.layer_groups = split_model(learn.model)

    # set multi-gpu
    if data.device.type=='cuda':
        learn.model = torch.nn.DataParallel(learn.model,device_ids=[0,1,2,3])

    # set loss func
#     learn.loss_func = partial(nb_loss_metrics.combo_loss, balance_ratio=1)
#     learn.loss_func = nb_loss_metrics.dice_loss
    learn.loss_func = partial(nb_loss_metrics.balance_bce, balance_ratio=1)

    # 添加metrics
    learn.metrics += [nb_loss_metrics.dice_loss]
    learn.metrics += [partial(nb_loss_metrics.balance_bce,balance_ratio=1)]
    learn.metrics += [nb_loss_metrics.mask_iou]

    return learn


#================================================
def get_tbCb(learn,log_dir):
    tbCb = nb_tensorboard_callback.TensorBoardCallback(
                                   learn=learn,
                                   log_dir=log_dir,
                                   plot_net=False,
                                   plot_loss=True,
                                   metric_plots=['mask_iou'],
                                   hyper_plots=['lr'],
                                   hist_plots=['down_blocks.2.0.conv1.weight',
                                               'up_blocks.2.conv1.conv.weight'],
                                   hist_iters=50)
    return tbCb


#================================================
def fit(learn,epoch_len,epochs,lr,callbacks):
    nb_scheduling_train.fit_with_warmup_multiAnnealPlat(learn,
                                    epoch_len=epoch_len,
                                    num_epoch=epochs,

                                    lr_start=lr/10,
                                    lr_constant=lr,
                                    warmup_iter=10,

                                    monitor='train_smooth',
                                    worseN_thres=5,
                                    annealRate=10,
                                    duration_thres=30,
                                    annealIte=10,
                                    phaseMaxN=3,
                                    finetune_stop=1,
                                    callbacks=callbacks)


#================================================
# 设置device
device = torch.device('cuda')


#================================================
ds = './data/dataset_20200708'
data = nb_databunch.get_databunch(ds, bs=16, device=device)


#================================================
opts = [partial(nb_optimizer.Adam, betas=(0.9,0.99))]

lrs = [1e-3]

checkpoints = [None]


#================================================
multi_train(get_learn=partial(get_learn,data=data),
            epoch_len=1e9, epochs=500,
            opts=opts, lrs=lrs, checkpoints=checkpoints,
            tb_log_root='./tb_log/',
            autoSave=True)
